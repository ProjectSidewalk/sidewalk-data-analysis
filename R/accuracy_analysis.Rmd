---
title: 'Accuracy Summary'
author: 'Mikey Saugstad'
date: 'October 10, 2017'
output:
  html_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(magrittr)
library(ggplot2)
library(gridExtra)
library(reshape2)
library(lattice)
library(dplyr)
library(knitr)
library(tidyr)
library(rlang)
library(data.table)

theme.histogram <- theme_bw() + theme(panel.grid.major.x = element_blank())
```

## Preliminary Notes

* For all graphs relating to accuracy, each data point is a single user (or a set of clustered users) over a single condition (i.e., 2-3 routes, or 2000/4000ft).
* In the case of 5 meter and 10 meter segment granularity, raw accuracy and specificity get a huge boost from a large number of true negatives.
* For NoSidewalk issues, these should only be looked at in the street-level, binary case. Both because there is not an established way to label, and because that is the level of granularity that users would actually care about.
* When clustering, we choose the "average" of the points (using haversine distance, etc, etc), and when we want to represent the cluster on a panorama, we show the label that is closest to this "average".

```{r reading-cleaning-data-combined, include=FALSE, cache=FALSE}
# read in data
classes <- c(replicate(6, 'factor'), 'numeric', 'numeric', 'logical', 'logical',
             replicate(3, 'factor'), replicate(8, 'numeric'))
volunteer.data <- read.csv('../data/accuracies-volunteer.csv',
                           colClasses = classes,
                           na.strings = c('null')) %>%
  mutate(is.turker = FALSE, single.user.clust = TRUE)
turker.data <- read.csv('../data/accuracies-turker.csv',
                        colClasses = classes,
                        na.strings = c('null')) %>%
  mutate(is.turker = TRUE, single.user.clust = TRUE)
volunteer.data.old <- read.csv('../data/accuracies-volunteer-pre_single_user_clustering.csv',
                           colClasses = classes,
                           na.strings = c('null')) %>%
  mutate(is.turker = FALSE, single.user.clust = FALSE)
turker.data.old <- read.csv('../data/accuracies-turker-pre_single_user_clustering.csv',
                        colClasses = classes,
                        na.strings = c('null')) %>%
  mutate(is.turker = TRUE, single.user.clust = FALSE)

# Combine datasets
accuracy.data <- rbind(volunteer.data, volunteer.data.old, turker.data, turker.data.old)

# Remove occlusion label type
accuracy.data <- subset(accuracy.data, label.type != 'Occlusion')
accuracy.data$label.type <- droplevels(accuracy.data$label.type)

# Rename SurfaceProblem label type as SurfaceProb for easier visualization
# possibly redo using https://stackoverflow.com/questions/28190435/changing-factor-levels-with-dplyr-mutate
label.type.levs <- levels(accuracy.data$label.type)
label.type.levs[6] <- c('SurfaceProb')
levels(accuracy.data$label.type) <- label.type.levs

# Reorder label type levels.
label.type.levs <- c('CurbRamp','NoCurbRamp','Obstacle','SurfaceProb','NoSidewalk','Problem')
accuracy.data$label.type <- factor(accuracy.data$label.type, label.type.levs)

# More setup: remove binary analysis (except street level), add is.anon.route column, add
#             raw.accuracy column, add worker.type column, give granularity an ordering.
# Note that we convert condition.id from factor -> character -> numeric, b/c converting directly to
# numeric from factor starts from 0 instead of using the contents of the string to get the number.
get.worker.type <- Vectorize(
  function(n.workers, worker.thresh, is.turker, is.anon.route) {
    if (n.workers == 5 & worker.thresh == 3) 'maj'
    else if (is.turker == TRUE) 'turk'
    else if (is.anon.route == TRUE) 'anon'
    else 'reg'
  }
)
# Copies the current granularity, but adds separate granularity for street&5m level binary.
get.granularity <- Vectorize(
  function(gran, binary) {
    if (binary == TRUE & gran == 'street') 'street.binary'
    else if (binary == TRUE & gran == '5_meter') '5_meter.binary'
    else as.character(gran)
  }
)
new.gran.names <- c('5_meter', '5_meter.binary', '10_meter', 'street', 'street.binary')
data.with.raw.accuracy <-
  accuracy.data %>%
  filter(binary == FALSE | granularity != '10_meter') %>%
  mutate(granularity = ordered(get.granularity(granularity, binary), levels = new.gran.names)) %>%
  mutate(is.anon.route = as.numeric(as.character(condition.id)) > 121) %>%
  mutate(worker.type = factor(get.worker.type(n.workers, worker.thresh, is.turker, is.anon.route),
                              levels = c('anon', 'reg', 'turk', 'maj'))) %>%
  mutate(raw.accuracy = (true.pos + true.neg) / (true.pos + true.neg + false.pos + false.neg))
# volunteer.summary <-
#   volunteer.with.raw.accuracy %>%
#   group_by(label.type, granularity) %>%
#   summarise_at(c('specificity', 'recall', 'f.measure', 'precision', 'raw.accuracy'), mean, na.rm = TRUE)

# TODO Figure out wtf is wrong here. I included counts that I got from a SQL query temporarily.
label.counts <-
  data_frame(
    count = c(2784, 77, 242, 772, 256, 1129),
    label.type = c('CurbRamp','NoCurbRamp','Obstacle','SurfaceProb','NoSidewalk','Problem')
    )
  # accuracy.data %>%
  # filter(binary == FALSE, is.turker == FALSE, granularity == 'street', remove.low.severity == FALSE) %>%
  # group_by(label.type) %>%
  # summarize(count = sum(true.pos, false.neg))
gt.label.count.lookup <- setNames(label.counts$count, label.counts$label.type)

label.count.labeller <- Vectorize(
  function(curr.type) {
    paste0(curr.type, ', GT=', gt.label.count.lookup[curr.type])
  }
)
```

```{r reading-cleaning-data-turker, include=FALSE, cache=FALSE}
# add column for vote threshold
get.threshold.factor <- Vectorize(
    function(workers, threshold) {
    if (threshold == 1) 'at.least.one'
    else if (workers == threshold) 'consensus'
    else 'majority'
  }
)
with.vote <-
  data.with.raw.accuracy %>%
  filter(is.turker == TRUE) %>%
  filter(remove.low.severity == FALSE) %>%
  mutate(vote.threshold = ordered(get.threshold.factor(n.workers, worker.thresh),
                                  levels = c('at.least.one', 'majority', 'consensus')))

# summary((with.vote %>% slice(rep(worker.thresh == n.workers, each=3)) )$vote.threshold)
set1 <- with.vote %>% filter(n.workers == 1) %>% mutate(vote.threshold = factor('consensus'))
set2 <- with.vote %>% filter(n.workers == 1) %>% mutate(vote.threshold = factor('majority'))
expanded.vote <- with.vote %>% rbind(set1, set2)
```

```{r reading-cleaning-data-volunteer, include=FALSE, cache=FALSE}
volunteer.with.raw.accuracy <-
  data.with.raw.accuracy %>%
  filter(single.user.clust == TRUE) %>%
  filter(is.turker == FALSE) %>%
  filter(remove.low.severity == FALSE)
```


## Comparing Anon vs. Registered Volunteers vs. Turkers

This section has a series of boxplots that compare performance of anonymous volunteers, registered volunteers, a single turker, and 5 turkers with majority vote. Here, a the single turker is the first turker to complete that set of routes.

For each granularity and label type, an ANOVA test was run, with the p-value being reported in the top-right corner of each boxplot. I appended a `**` if the p-value was less than 0.01, and just a `*` if the p-value was less than 0.05, to make finding significant results easier.

To make room for the p-value on each boxplot, I expanded the y-limit past 1.0, and then added a dotted line at the 1.0 mark so that we still have that reference point. However, it makes the graphs look a bit ugly, so LMK if you want to remove it.

*Takeaways*: 

* Missing curb ramp labels precision is very low for all worker types, indicating that verification would be extremely useful for this label type, in particular.

```{r worker-types-setup, echo=FALSE, warning=FALSE, cache=FALSE}
worker.type.accuracy <-
  data.with.raw.accuracy %>%
  filter(single.user.clust == TRUE) %>%
  filter(remove.low.severity == FALSE) %>%
  filter(n.workers == 1 | (n.workers == 5 & worker.thresh == 3))

# Takes a p-value, returns a * if it is below 0.05, and ** if below 0.01; an empty string o/w.
sig.sym <- Vectorize(
  function(significance) {
  if (significance < 0.01) '**'
  else if (significance < 0.05) '*'
  else ''
  }
)

# Defines the function that produces the boxplots that compare different worker types. It takes in
# the accuracy measure you are plotting as a string. It also runs an ANOVA test and puts the p-val
# as a label in the top-left corner of each boxplot.
worker.type.trellis.boxplot <- function(accuracy.type) {
  
  # Run ANOVA, and format p-value appropriately for visualizing on the plot (in the form 'x.xxx**')
  pval <-
    worker.type.accuracy %>%
    group_by(granularity, label.type) %>%
    do(model = aov(as.formula(paste0(accuracy.type, ' ~ worker.type')), data = .)) %>%
    mutate(p = summary(model)[[1]][["Pr(>F)"]][1]) %>%
    mutate(p.str = paste0("p = ", sprintf("%.3f", p), sig.sym(p)))
  
  # Create the trellis boxplots.
  ggplot(data = worker.type.accuracy, aes_string(x = 'worker.type', y = accuracy.type)) +
    geom_boxplot() +
    scale_y_continuous(breaks = c(0, 0.25, 0.5, 0.75, 1.0), limits = c(0, 1.15)) +
    geom_hline(aes(yintercept = 1.0), linetype = 'dotted') +
    geom_label(data = pval, aes(x = -Inf, y = 1.13, hjust = 'inward', label = p.str),
               size = 3.0, label.padding = unit(0.15, "lines")) +
    facet_grid(granularity ~ label.type, labeller = labeller(label.type = label.count.labeller)) +
    theme_bw() +
    theme(legend.position = 'top')
}
```

#### Raw Accuracy
Defined as $\frac{TP + TN}{TP + TN + FP + FN}$. Just the percentage of things they got correct.

```{r worker-types-raw-accuracy, echo=FALSE, fig.height=7, fig.width=10, warning=FALSE, cache=FALSE}
worker.type.trellis.boxplot('raw.accuracy')
```

#### Recall
Defined as $\frac{TP}{TP + FN}$. High recall means that they found most of the issues/features.

```{r worker-types-recall, echo=FALSE, fig.height=7, fig.width=10, warning=FALSE, cache=FALSE}
worker.type.trellis.boxplot('recall')
```

#### Precision
Defined as $\frac{TP}{TP + FP}$. High precision means that they rarely placed a label when they shouldn't have.

```{r worker-types-precision, echo=FALSE, fig.height=7, fig.width=10, warning=FALSE, cache=FALSE}
worker.type.trellis.boxplot('precision')
```

#### F-measure
Defined as $2 * \frac{precision * recall}{precision + recall}$. It is essentially a balance between recall and precision.

```{r worker-types-f-measure, echo=FALSE, fig.height=7, fig.width=10, warning=FALSE, cache=FALSE}
worker.type.trellis.boxplot('f.measure')
```

#### Specificity
Defined as $\frac{TN}{TN + FP}$. Similar to precision, high specificity means that they rarely placed a label when they shouldn't have, but specificity gives more weight to true _negatives_, while precision gives more weight to true _positives_.

```{r worker-types-specificity, echo=FALSE, fig.height=7, fig.width=10, warning=FALSE, cache=FALSE}
worker.type.trellis.boxplot('specificity')
```

## Volunteer Data
In this section, there are a series of histograms that help to visualize the distribution of volunteers' accuracy. For each accuracy measure, there is a grid of histograms split by label type and granularity (street, 5 meter, 10 meter).

Note that these histograms have lines representing the _mean_ of each group (not the _median_; lmk if you want to see median instead).

```{r volunteer-histograms-setup, echo=FALSE, warning=FALSE, cache=FALSE}
volunteer.grouped <- volunteer.with.raw.accuracy %>% group_by(granularity, label.type)

# Defines the function that produces the boxplots that compare different worker types. It takes in
# the accuracy measure you are plotting as a string.
volunteer.trellis.histogram <- function(accuracy.type) {
  ggplot(data =
           volunteer.grouped %>%
           mutate(mean = mean(!!sym(accuracy.type), na.rm = TRUE),
                  median = median(!!sym(accuracy.type), na.rm = TRUE),
                  sd = sd(!!sym(accuracy.type), na.rm = TRUE)),
         aes_string(accuracy.type)) +
    geom_histogram(binwidth = 0.1) +
    geom_vline(aes(xintercept = mean)) +
    geom_label(
      aes(x = -Inf, y = Inf, hjust = 'inward', vjust = 'inward',
          label = paste0("mn=", sprintf('%.2f', mean), "\n",
                         "md=", sprintf('%.2f', median), '\n',
                         "sd=", sprintf('%.2f', sd))),
      size = 2.2, label.padding = unit(0.15, "lines")) +
    facet_grid(granularity ~ label.type,
               labeller = labeller(label.type = label.count.labeller)) +
    theme_bw() +
    scale_y_continuous(expand = c(0, 0)) # removes weird white space at bottom of plot
}
```

#### Raw accuracy
Defined as $\frac{TP + TN}{TP + TN + FP + FN}$. Just the percentage of things they got correct.

```{r volunteer-raw-accuracy, echo=FALSE, fig.height=7, fig.width=10, warning=FALSE, cache=FALSE}
volunteer.trellis.histogram('raw.accuracy')
```

#### Recall
Defined as $\frac{TP}{TP + FN}$. High recall means that they found most of the issues/features.

```{r volunteer-recall, echo=FALSE, fig.height=7, fig.width=10, warning=FALSE, cache=FALSE}
volunteer.trellis.histogram('recall')
```

#### Precision
Defined as $\frac{TP}{TP + FP}$. High precision means that they rarely placed a label when they shouldn't have.

*Note*: Very little confidence should be given to precision for the NoSidewalk label, since GT labelers only placed the label at intersections and at places where a sidewalk ends.

```{r volunteer-precision, echo=FALSE, fig.height=7, fig.width=10, warning=FALSE, cache=FALSE}
volunteer.trellis.histogram('precision')
```

#### F-measure
Defined as $2 * \frac{precision * recall}{precision + recall}$. It is essentially a balance between recall and precision.

```{r volunteer-f-measure, echo=FALSE, fig.height=7, fig.width=10, warning=FALSE, cache=FALSE}
volunteer.trellis.histogram('f.measure')
```

#### Specificity
Defined as $\frac{TN}{TN + FP}$. Similar to precision, high specificity means that they rarely placed a label when they shouldn't have, but specificity gives more weight to true _negatives_, while precision gives more weight to true _positives_.

```{r volunteer-specificity, echo=FALSE, fig.height=7, fig.width=10, warning=FALSE, cache=FALSE}
volunteer.trellis.histogram('specificity')
```


## Turker Data

### Comparing effects of number of turkers and vote type on accuracy
In this section, there are a series of line graphs to help visualize how the number of turkers used and the method of voting affects the various accuracy measures. For each accuracy measure, there is a grid of line graphs, split by label type and granularity (same as above). However, each graph also has a line for each of the voting methods. You will notice that all voting methods are equivalent when looking at only one turker.

```{r vote-type-setup, echo=FALSE, warning=FALSE, cache=FALSE}
# Defines the function that produces the line charts that compare different voting thresholds. It
# takes in the accuracy measure you are plotting as a string.
vote.type.trellis.linechart <- function(accuracy.type) {
  ggplot(data =
           expanded.vote %>%
           filter(single.user.clust == TRUE) %>%
           drop_na(accuracy.type) %>%
           group_by(label.type, n.workers, vote.threshold, granularity) %>%
           summarise_at(c(accuracy.type), mean),
         aes(n.workers)) +
    geom_line(aes_string(y = accuracy.type, colour = 'vote.threshold')) +
    geom_point(aes_string(y = accuracy.type, colour = 'vote.threshold'), size = 1) +
    facet_grid(granularity ~ label.type,
               labeller = labeller(label.type = label.count.labeller)) +
    ylim(0, 1) +
    theme_bw() +
    theme(legend.position = 'top')
}
```

#### Raw Accuracy
Defined as $\frac{TP + TN}{TP + TN + FP + FN}$. Just the percentage of things they got correct.

```{r vote-type-raw-accuracy, echo=FALSE, fig.height=7, fig.width=10}
vote.type.trellis.linechart('raw.accuracy')
```

#### Recall
Defined as $\frac{TP}{TP + FN}$. High recall means that they found most of the issues/features.

```{r vote-type-recall, echo=FALSE, fig.height=7, fig.width=10}
vote.type.trellis.linechart('recall')
```

#### Precision
Defined as $\frac{TP}{TP + FP}$. High precision means that they rarely placed a label when they shouldn't have.

```{r vote-type-precision, echo=FALSE, fig.height=7, fig.width=10}
vote.type.trellis.linechart('precision')
```

#### F-measure
Defined as $2 * \frac{precision * recall}{precision + recall}$. It is essentially a balance between recall and precision.

```{r vote-type-f-measure, echo=FALSE, fig.height=7, fig.width=10}
vote.type.trellis.linechart('f.measure')
```

#### Specificity
Defined as $\frac{TN}{TN + FP}$. Similar to precision, high specificity means that they rarely placed a label when they shouldn't have, but specificity gives more weight to true _negatives_, while precision gives more weight to true _positives_.

```{r vote-type-specificity, echo=FALSE, fig.height=7, fig.width=10}
vote.type.trellis.linechart('specificity')
```


## Removing Low Severity
In this section, we are looking at how accuracy is effected when we remove low severity labels from the GT set. The idea is that higher severity problems (as defined by the GT labelers) will be easier for crowd workers to find, resulting in higher recall. This would be a nice result to have, to say that crowd workers can at least find the most severe problems. The two expected outcomes here: recall goes up, and for precision to go down. I've included all accuracy types for now in case we see anything interesting.

_Note_: Low severity labels were removed from the _ground truth data only_. This is why we expect precision to go down, as legitimate problems have been removed from the ground truth, that crowd workers may have seen. Removing only crowd worker labels, but not GT labels, would likely result in higher precision and lower recall. Removing labels from both GT and crowd workers would probably result in lower precision _and_ recall, due to the variability in how people label severity. Discrepancy between GT and crowd worker severity ratings will be addressed in a separate section.

```{r low-severity-setup, echo=FALSE, warning=FALSE, cache=FALSE}
# Gives a more informative name to low.severity.thresh factor, also fills NAs
name.thresh <- Vectorize(
  function(thresh) {
  if (is.na(thresh)) 'baseline'
  else paste0('>=', as.character(thresh))
  }
)

# Removes label types that don't have severity (plus CurbRamp).
low.severity.check.data <-
  data.with.raw.accuracy %>%
  filter(single.user.clust == TRUE) %>%
  filter(n.workers == 1 | (n.workers == 5 & worker.thresh == 3)) %>%
  filter(label.type %in% c('NoCurbRamp', 'Obstacle', 'SurfaceProb')) %>%
  droplevels %>%
  mutate(low.severity.thresh = ordered(name.thresh(low.severity.thresh),
                                       levels = c('baseline', '>=3', '>=4')))

# Constructs a trellis of line charts that show accuracy, faceted by label type and granularity.
# It compares removing low severity and different thresholds on the x-axis, and has a different
# line colour for each worker type (anon, reg, turker, turker w/ majority vote).
low.severity.trellis.linechart <- function(accuracy.type) {
  ggplot(data =
           low.severity.check.data %>%
           drop_na(accuracy.type) %>%
           group_by(label.type, worker.type, low.severity.thresh, granularity) %>%
           summarise_at(c(accuracy.type), mean),
         aes(low.severity.thresh)) +
    geom_line(aes_string(y = accuracy.type, colour = 'worker.type', group = 'worker.type')) +
    geom_point(aes_string(y = accuracy.type, colour = 'worker.type'), size = 1) +
    facet_grid(granularity ~ label.type,
               labeller = labeller(label.type = label.count.labeller)) +
    ylim(0, 1) +
    theme_bw() +
    theme(legend.position = 'top')
}
```

### How many low severity problems are there?
Below is a graph that shows the number of conditions (sets of routes) containing at least one label (split by label type), and how that is affected by removing low severity labels. On that first graph, the horizontal line shows the number of conditions that is in the current data set (some still need to be added once I fix some bugs and such).

The second graph shows the GT label counts, and how they are affected by removing low severity labels.

```{r low-severity-label-counts, echo=FALSE, fig.height=3, fig.width=8}
total.conditions <- length(levels(accuracy.data$condition.id))
label.count.summary <- 
  low.severity.check.data %>%
           filter(granularity == 'street') %>%
           filter(is.turker == FALSE) %>%
           mutate(num.labs = true.pos + false.neg) %>%
           group_by(low.severity.thresh, label.type) %>%
           summarise(gt.labels = sum(num.labs), conditions.with.labels = sum(num.labs > 0))
ggplot(data = label.count.summary, aes(low.severity.thresh)) +
  geom_line(aes(y = conditions.with.labels, group = 1)) +
  geom_point(aes(y = conditions.with.labels), size = 1) +
  geom_hline(aes(yintercept = total.conditions)) +
  facet_grid(. ~ label.type) +
  theme_bw() +
  scale_y_continuous(limits = c(0, 60), expand = c(0, 0))
ggplot(data = label.count.summary, aes(low.severity.thresh)) +
  geom_line(aes(y = gt.labels, group = 1)) +
  geom_point(aes(y = gt.labels), size = 1) +
  facet_grid(. ~ label.type) +
  theme_bw() +
  scale_y_continuous(limits = c(0, 800), expand = c(0, 0))

# count.table <- matrix(c('>= 1', '77', '100%', '242', '100%', '772', '100%',
#                         '>= 3', '66', '86%', '85', '35%', '130', '17%',
#                         '>= 4', '38', '49%', '45', '19%', '47', '6%'),
#                       ncol = 7, byrow = TRUE)
# colnames(count.table) <- c('Severity', 'NoRamp.cnt', 'NoRamp.pct', 'Obs.cnt', 'Obs.pct', 'SurfaceProb.cnt', 'SurfaceProb.pct')
# kable(count.table)
```


#### Raw Accuracy
Defined as $\frac{TP + TN}{TP + TN + FP + FN}$. Just the percentage of things they got correct.

```{r low-severity-raw-accuracy, echo=FALSE, fig.height=7, fig.width=8}
low.severity.trellis.linechart('raw.accuracy')
```

#### Recall
Defined as $\frac{TP}{TP + FN}$. High recall means that they found most of the issues/features.

```{r low-severity-recall, echo=FALSE, fig.height=7, fig.width=8}
low.severity.trellis.linechart('recall')
```

#### Precision
Defined as $\frac{TP}{TP + FP}$. High precision means that they rarely placed a label when they shouldn't have.

```{r low-severity-precision, echo=FALSE, fig.height=7, fig.width=8}
low.severity.trellis.linechart('precision')
```

#### F-measure
Defined as $2 * \frac{precision * recall}{precision + recall}$. It is essentially a balance between recall and precision.

```{r low-severity-f-measure, echo=FALSE, fig.height=7, fig.width=8}
low.severity.trellis.linechart('f.measure')
```

#### Specificity
Defined as $\frac{TN}{TN + FP}$. Similar to precision, high specificity means that they rarely placed a label when they shouldn't have, but specificity gives more weight to true _negatives_, while precision gives more weight to true _positives_.

```{r low-severity-specificity, echo=FALSE, fig.height=7, fig.width=8}
low.severity.trellis.linechart('specificity')
```


## Incorporating single-user clustering

```{r single-user-clustering-setup, echo=FALSE, fig.height=7, fig.width=8}
worker.type.single.user.clust <-
  data.with.raw.accuracy %>%
  filter(granularity == '5_meter') %>%
  filter(!(label.type %in% c('NoSidewalk', 'Problem'))) %>%
  filter(remove.low.severity == FALSE) %>%
  mutate(single.user.clust = factor(single.user.clust, labels = c('without', 'with'))) %>%
  # filter(worker.type %in% c('turk', 'maj')) %>%
  # filter(n.workers == 1 | (n.workers == 5 & worker.thresh == 3))
  filter(worker.type != 'maj', n.workers == 1)

# Defines the function that produces the boxplots that compare different worker types. It takes in
# the accuracy measure you are plotting as a string. It also runs an ANOVA test and puts the p-val
# as a label in the top-left corner of each boxplot.
worker.type.single.user.clust.trellis.boxplot <- function(accuracy.type) {
  
  # Run ANOVA, and format p-value appropriately for visualizing on the plot (in the form 'x.xxx**')
  models <-
    worker.type.single.user.clust %>%
    group_by(worker.type, label.type) %>%
    do(model = aov(as.formula(paste0(accuracy.type, ' ~ single.user.clust')), data = .)) %>%
    mutate(p = summary(model)[[1]][["Pr(>F)"]][1])

  n.obs <-
    worker.type.single.user.clust %>%
    group_by(worker.type, label.type) %>%
    summarise(n1 = sum(single.user.clust == 'without' & !is.na(!!sym(accuracy.type))),
              n2 = sum(single.user.clust == 'with' & !is.na(!!sym(accuracy.type))))

  pval <-
    inner_join(models, n.obs, by = c('worker.type', 'label.type')) %>%
      mutate(p.str = paste0("p=", sprintf("%.3f", p), sig.sym(p),
                            ', n=(', n1, ',', n2, ')'))
  
  # Create the trellis boxplots.
  ggplot(data = worker.type.single.user.clust, aes_string(x = 'single.user.clust', y = accuracy.type)) +
    # geom_boxplot() +
    geom_violin(draw_quantiles = 0.5, na.rm = TRUE) +
    scale_y_continuous(breaks = c(0, 0.25, 0.5, 0.75, 1.0), limits = c(0, 1.15)) +
    geom_hline(aes(yintercept = 1.0), linetype = 'dotted') +
    geom_label(data = pval, aes(x = -Inf, y = 1.12, hjust = 'inward', label = p.str),
               size = 3.0, label.padding = unit(0.15, "lines")) +
    facet_grid(worker.type ~ label.type, labeller = labeller(label.type = label.count.labeller)) +
    theme.histogram +
    theme(legend.position = 'top')
}
```

#### Label counts

_Expected_: There will be _significantly fewer_ ramp labels, and _slightly_ fewer labels from the other label types: since ramp labels often come in pairs, although a much lower distance threshold is being used for the clustering of ramp labels, we can expect a fair number of legitimate ramp labels to be excluded.

_Observed_: It is interesting that a higher proportion of NoSidewalk labels were removed than any other label type. I just used the 7.5 meter distance threshold that we used for Obstacle and SurfaceProblem for the NoSidewalk. This certainly indicates that there are quite a few users (of all user groups) who placed the NoSidewalk label frequently (possibly in the same way that Jon places NoSidewalk labels).

For turkers, after single-user clustering the percentage of ramp labels removed is actually much lower than that of other the label types, and the percentage of NoCurbRamp labels removed is the lowest for each user group.

What I find most interesting about comparing user groups is the difference in this graph between registered users and turkers (I talk specifically about them, since the group of anon users is quite small -- though it is interesting that it anonymous users seem to match up with turkers). I don't really have a guess right now for why 22% of registered users CurbRamp labels are gone, but only 7% of turker CurbRamps are.

```{r single-user-clustering-label-counts-1, echo=FALSE, fig.height=3, fig.width=8}
single.user.clust.ramp.counts <-
  data.with.raw.accuracy %>%
  filter(granularity == 'street', # any non-binary granularity gives same label counts
         remove.low.severity == FALSE,
         worker.type != 'maj',
         n.workers == 1,
         label.type != 'Problem') %>%
  group_by(single.user.clust, label.type, worker.type) %>%
  summarize(n.labels = sum(true.pos, false.pos)) %>%
  group_by(label.type, worker.type) %>%
  mutate(percentage.labels.removed = 100 - 100 * n.labels / max(n.labels)) %>%
  # mutate(n.labels.removed = max(n.labels) - n.labels) %>%
  ungroup() %>%
  filter(single.user.clust == TRUE)

count.labs <-
  data.with.raw.accuracy %>%
  filter(granularity == 'street', # any non-binary granularity gives same label counts
         remove.low.severity == FALSE,
         worker.type != 'maj',
         n.workers == 1,
         label.type != 'Problem',
         single.user.clust == TRUE) %>%
  group_by(worker.type) %>%
  summarize(workers = n() / 5, labels = sum(true.pos, false.pos)) %>%
  mutate(txt = paste0('# workers = ', workers, '\n# labels = ', labels))

ggplot(data = single.user.clust.ramp.counts, aes(x = label.type, y = percentage.labels.removed)) +
  geom_col(aes(fill = label.type)) +
  scale_fill_manual(values = c('green', 'red', 'blue', 'orange', 'grey')) +
  theme.histogram +
  # scale_y_continuous(expand = c(0, 0), limits = c(0, 100)) + # removes weird white space at bottom of plot
  scale_y_continuous(expand = c(0, 0), limits = c(0, 1.05 * max(single.user.clust.ramp.counts$percentage.labels.removed))) +
  geom_label(data = count.labs, aes(x = 1.6, y = 30, label = txt),
             size = 3.0, label.padding = unit(0.15, "lines")) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1.05, vjust = 1.05)) +
  facet_grid(. ~ worker.type)
```

*SKIP THE REST OF THIS SECTION*

_Expected_: For higher vote thresholds, _reduction in labels might be more pronounced_, 

_Expected_: For higher vote thresholds, and for larger numbers of turkers being clustered, the _reduction in labels might be more pronounced_. The rationale is that in both those situations, there are more turkers involved, and thus there is a higher probability that one of the labels in the cluster is removed after single-user clustering... Increasing the probability that said cluster will no longer make the vote threshold.

_Observed_: In the histograms below, we see that both of the above turned out to be correct. More clusters no longer met the vote threshold for larger numbers of turkers, and for higher vote thresholds.

```{r single-user-clustering-label-counts-2, echo=FALSE, fig.height=5, fig.width=7}
single.user.clust.turk.ramp.counts <-
  expanded.vote %>%
  filter(granularity == 'street',
         remove.low.severity == FALSE,
         # n.workers > 1,
         label.type != 'Problem') %>%
  group_by(single.user.clust, label.type, n.workers, vote.threshold) %>%
  summarize(n.labels = sum(true.pos, false.pos)) %>%
  group_by(label.type, n.workers, vote.threshold) %>%
  mutate(percentage.labels.removed = 100 - 100 * n.labels / max(n.labels)) %>%
  ungroup() %>%
  filter(single.user.clust == TRUE)
  
ggplot(data = single.user.clust.turk.ramp.counts,
       aes(x = as.factor(n.workers), y = percentage.labels.removed)) +
  geom_col() +
  theme.histogram +
  xlab('Number of Turkers') +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 100)) + # removes weird white space at bottom of plot
  facet_grid(vote.threshold ~ label.type)

ggplot(data = single.user.clust.turk.ramp.counts %>% filter(n.workers > 1),
       aes(x = vote.threshold, y = percentage.labels.removed)) +
  geom_col() +
  theme.histogram +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 100)) + # removes weird white space at bottom of plot
  # scale_y_continuous(expand = c(0, 0)) + # removes weird white space at bottom of plot
  theme(axis.text.x = element_text(angle = 45, hjust = 1.05, vjust = 1.05)) +
  facet_grid(as.factor(n.workers) ~ label.type)
```

#### Recall
_Expected_: Recall will be _slightly worse_ for all non-ramp label types: there are some cases where there are, in fact, multiple distinct issues in close proximity to one another. In such cases, clustering will remove one of the labels from the volunteer's data, thus lowering recall.

Recall with be _significantly worse_ for CurbRamp and NoCurbRamp labels: due to there being significantly fewer ramp labels.

_Observed_: At least for individual users, recall does decrease very slightly, and more so for ramp label types. But this is a very small difference; average recall decreased by about 0.01 for ramp label types, and by 0.005 for non ramp label types... This will likely be more pronounced once we look at clustered turkers.

```{r single-user-clustering-recall, echo=FALSE, fig.height=7, fig.width=8}
worker.type.single.user.clust.trellis.boxplot('recall')

# worker.type.single.user.clust %>%
#   group_by(label.type, single.user.clust) %>%
#   summarise_at(vars(recall), mean, na.rm = TRUE) %>%
#   group_by(label.type) %>%
#   summarise(recall.diff = 100 * (max(recall) - min(recall))) %>%
#   group_by(label.type) %>%
#   summarize(recall.ave.diff = mean(recall.diff))
```

#### Precision

_Expected_: Precision will _improve slightly_ for all label types: in situations where a user labeled the same problem/feature multiple times, clustering should remove the duplicate label, thus improving precision.

_Observed_: In almost all cases, we do see a slight improvement in precision. The outliers are CurbRamp labels, which have a sizable improvement in precision (especially for registered users); anonymous users, whose precision does not seem to have been affected; and SurfaceProblem labels for turker, where precision actually slightly decreased.

```{r single-user-clustering-precision, echo=FALSE, fig.height=7, fig.width=8}
worker.type.single.user.clust.trellis.boxplot('precision')

# worker.type.single.user.clust %>%
#   group_by(label.type, single.user.clust) %>%
#   summarise_at(vars(precision), mean, na.rm = TRUE) %>%
#   group_by(label.type) %>%
#   summarise(precision.diff = 100 * (max(precision) - min(precision))) %>%
#   group_by(label.type) %>%
#   summarize(precision.ave.diff = mean(precision.diff))
```


#### Specificity

_Expected_: Specificity will _improve slightly_ across the board: fewer labels => more true negatives and fewer false positives => higher specificity.

_Observed_: It is not exceptionally obvious from the graph, but specificity does improve slightly across the board. There is no way that specificity could get worse, since the formula includes only true negatives and false positives, which can only improve if we take away some labels. The largest improvement is with CurbRamps placed by registered users.

```{r single-user-clustering-specificity, echo=FALSE, fig.height=7, fig.width=8}
worker.type.single.user.clust.trellis.boxplot('specificity')

# worker.type.single.user.clust %>%
#   group_by(label.type, single.user.clust) %>%
#   summarise_at(vars(specificity), mean, na.rm = TRUE) %>%
#   group_by(label.type) %>%
#   summarise(specificity.diff = 100 * (max(specificity) - min(specificity))) %>%
#   group_by(label.type) %>%
#   summarize(specificity.ave.diff = mean(specificity.diff))
```

#### Variance in accuracy (NOT READY)

_Expected_: Variance will be _slightly larger_ for precision and f-measure, for turkers with vote thresholds greater than 1: fewer labels => fewer labels that pass majority vote => fewer true/false positives => more sets of turkers with no true or false positives (for a given label type) => more sets of turkers with null precision (since denominator is TP + FP), and f-measure (since precision is used to compute f-measure) => larger confidence intervals for those accuracy types (b/c smaller n).

_Observed_: TBD

```{r single-user-clustering-variance, echo=FALSE, fig.height=7, fig.width=8}
single.user.clust.variance.no.vote <-
  data.with.raw.accuracy %>%
  mutate(single.user.clust = factor(single.user.clust, labels = c('without', 'with'))) %>%
  filter(granularity == '5_meter',
         label.type != 'Problem',
         remove.low.severity == FALSE,
         n.workers == 1 || (n.workers == 5 & worker.thresh == 3)) %>%
  group_by(single.user.clust, label.type, worker.type) %>%
  summarize_at(vars(raw.accuracy, recall, precision, f.measure), var, na.rm = TRUE)
  # summarize(med.prec = median(precision, na.rm = TRUE), mean.prec = mean(precision, na.rm = TRUE))# %>%
  # summarize(n.labels = sum(true.pos, false.pos)) %>%
  # group_by(label.type, worker.type) %>%
  # mutate(n.labels = n.labels / max(n.labels)) %>%
  # ungroup()

single.user.clust.variance <-
  data.with.raw.accuracy %>%
  mutate(single.user.clust = factor(single.user.clust, labels = c('without', 'with'))) %>%
  filter(granularity == '5_meter',
         label.type != 'Problem',
         remove.low.severity == FALSE,
         n.workers == 5 & worker.thresh == 3) %>%
  group_by(single.user.clust, label.type) %>%
  summarize_at(vars(raw.accuracy, recall, precision, f.measure), var, na.rm = TRUE) %>%
  gather(accuracy.type, accuracy.value, raw.accuracy, recall, precision, f.measure)

ggplot(data = single.user.clust.variance, aes(x = single.user.clust, y = accuracy.value)) +
  geom_col() +
  theme.histogram +
  # scale_y_continuous(expand = c(0, 0), limits = c(0, 1.1 * max(single.user.clust.variance$accuracy.value))) +
  facet_grid(accuracy.type ~ label.type, scales = 'free_y')


  # expanded.vote %>%
  # filter(granularity == 'street',
  #        remove.low.severity == FALSE,
  #        # n.workers > 1,
  #        label.type != 'Problem') %>%
  # group_by(single.user.clust, label.type, n.workers, vote.threshold) %>%
  # summarize(n.labels = sum(true.pos, false.pos)) %>%
  # group_by(label.type, n.workers, vote.threshold) %>%
  # mutate(percentage.labels.removed = 100 - 100 * n.labels / max(n.labels)) %>%
  # ungroup() %>%
  # filter(single.user.clust == TRUE)
  

# ggplot(data = single.user.clust.variance, aes(x = single.user.clust, y = f.measure)) +
#   geom_col() +
#   theme.histogram +
#   scale_y_continuous(expand = c(0, 0), limits = c(0, 1.1 * max(single.user.clust.variance$f.measure))) +
#   facet_grid(worker.type ~ label.type)

```


```{r maybe-later, echo=FALSE, fig.height=3, fig.width=10, message=FALSE, cache=FALSE}
# Example of how to display the data in a table, not currently in use.
# volunteer.reg.vs.anon.summary <-
#   data.with.raw.accuracy %>%
#   group_by(label.type, granularity, is.anon.route) %>%
#   summarise_at(c('specificity', 'recall', 'f.measure', 'precision', 'raw.accuracy'), mean, na.rm = TRUE)

# volunteer.reg.vs.anon.summary %>%
#   kable(format = 'markdown')

# Example of how to display the data in a table, not currently in use.
# kable(summarized.raw.accuracy %>%
#         filter(vote.threshold == 'majority') %>%
#         select(-vote.threshold) %>%
#         spread(label.type, raw.accuracy),
#       format = 'markdown', digits = 2, align = 'l')


# turker.majority.vote <- subset(street.turker, ceiling(n.workers / 2) == worker.thresh)

# ggplot(data = turker.majority.vote, aes(recall)) +
#   geom_histogram(binwidth = 0.1, na.rm = TRUE) +
#   facet_grid(n.workers ~ label.type) +
#   theme_bw()
# 
# ggplot(data = turker.majority.vote, aes(recall)) +
#   stat_bin(aes(y = ..density..), binwidth = 0.1, na.rm = TRUE) +
#   facet_grid(label.type ~ n.workers) +
#   theme_bw()
```
