---
title: "Statistics for Paper"
author: "Mikey Saugstad"
date: "April 17, 2018"
output:
  github_document:
    toc: yes
  html_document:
    toc: yes
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(knitr.kable.NA = '-')

library(knitr)
library(RPostgreSQL)
library(multcomp) # experimenting with stuff (glht), might remove later
library(nlme)
library(lme4)
library(ggplot2) # may need to use devtools::install_github('tidyverse/ggplot2') if version < 2.2.2
library(tidyr)
library(dplyr)
library(forcats)
library(janitor)
library(Kendall)
library(stringr)

# Run the following in console when you want to generate github flavored markdown as well.
# library(rmarkdown)
# render('R/stats_for_paper.Rmd', c('html_document', 'github_document'))

# If true, queries local postges database for data (could take a very long time), then saves in
# CSVs, so you should set to FALSE after running with the actual queries once.
REFRESH_PUBLIC_DEPLOYMENT_DATA = FALSE
REFRESH_TURK_STUDY_DATA = FALSE

LABEL_TYPE_MAPPING <- c(
  '1' = 'CurbRamp',
  '2' = 'NoCurbRamp',
  '3' = 'Obstacle',
  '4' = 'SurfaceProblem',
  '5' = 'Other',
  '6' = 'Occlusion',
  '7' = 'NoSidewalk'
)
TOTAL_STREET_DIST_METERS <- 1730179
TOTAL_STREET_DIST_MILES <- TOTAL_STREET_DIST_METERS / 1609.34

LABEL_TYPES_TO_ANALYZE = c('All', 'Problem', 'CurbRamp', 'NoCurbRamp', 'Obstacle', 'SurfaceProb')
NON_AGGREGATE_TYPES = c('CurbRamp', 'NoCurbRamp', 'Obstacle', 'SurfaceProb')

# Variables commonly used in plotting
my.theme.discrete.x <- theme_bw() + theme(panel.grid.major.x = element_blank())
ACCURACY_BREAKS <- c(0, 0.25, 0.5, 0.75, 1.0)

# Helpful functions
'%not-in%' <- function(x,y)!('%in%'(x,y))
print.p.value <- function(p) {
  if_else(p < 0.001, 'p < 0.001', paste0('p = ', format(round(p, digits = 3), scientific = FALSE)))
}
p.num.to.p.str.with.sig.code <- function(p.value) {
  sig.code <- case_when(
    p.value < 0.001 ~ '***',
    p.value < 0.01  ~ '**',
    p.value < 0.05  ~ '*',
    TRUE            ~ '')
  formatted.p.value <- if_else(
    p.value < 0.001,
    '< 0.001',
    format(round(p.value, digits = 3), scientific = FALSE)
  )
  paste(formatted.p.value, sig.code)
}
extract_label_type <- Vectorize(
  function(note.str) {
  str_extract_all(note.str, '[:alpha:]+')[[1]][2]
  }
)
print.lrt.results <- function(test, model, fixed.effect) {
  if ('Pr(Chi)' %in% names(test)) p.col <- 'Pr(Chi)' else p.col <- 'Pr(>Chi)'
  paste0('likelihood ratio = ', format(test[fixed.effect,'LRT'], digits = 5),
         ', df = ', test[fixed.effect,'Df'],
         ', n = ', nobs(model),
         ', ', print.p.value(test[fixed.effect, p.col]))
}
format.tukey.table <- function(tukey, outcome.var, fixed.var, test.data, ordering = NULL,
                               descending = FALSE) {
  ordering.sign <- if_else(descending == TRUE, '<', '>')

  outcome.by.type <-
    test.data %>%
    mutate(!!fixed.var := as.character(!!sym(fixed.var))) %>%
    group_by(!!sym(fixed.var)) %>%
    dplyr::summarize_at(vars(outcome.var), mean)

  if (is.null(ordering)) {
    ordering <-
      if (descending == TRUE) {
        outcome.by.type %>% arrange(desc(!!sym(outcome.var))) %>% pull(fixed.var)
      } else {
        outcome.by.type %>% arrange(!!sym(outcome.var)) %>% pull(fixed.var)
      }
  }

  tukey.with.vars.split <-
    data.frame(comparison = names(tukey$test$pvalues),
               p = unname(tukey$test$pvalues),
               z.value = unname(tukey$test$tstat)) %>%
    mutate(comparison = if_else((descending == TRUE & z.value > 0)
                                | (descending == FALSE & z.value < 0),
                                gsub("([[:alnum:]]+) - ([[:alnum:]]+)", "\\2 - \\1", comparison),
                                as.character(comparison)),
           first.type = gsub("([[:alnum:]]+) - ([[:alnum:]]+)", "\\1", comparison),
           second.type = gsub("([[:alnum:]]+) - ([[:alnum:]]+)", "\\2", comparison),
           first.index = match(first.type, ordering),
           second.index = match(second.type, ordering),
           test = paste(ordering.sign, second.type),
           p.value = p.num.to.p.str.with.sig.code(p),
           z.value = format(abs(z.value), digits = 4, scientific = FALSE))

  indices.to.start <-
    tukey.with.vars.split %>%
    filter(p < 0.05 | second.index == 1) %>%
    group_by(first.index) %>%
    dplyr::summarise(start.second.index = max(second.index))

  inner_join(tukey.with.vars.split, indices.to.start, by = 'first.index') %>%
    filter(second.index >= start.second.index) %>%
    right_join(outcome.by.type, by = c('first.type' = fixed.var)) %>%
    mutate(!!fixed.var := factor(first.type, levels = ordering)) %>%
    arrange(!!sym(fixed.var), desc(second.index)) %>%
    dplyr::select(fixed.var, test, p.value, z.value, outcome.var)
}


summarise_vars <- function(data, group, var.names = NULL, col.start = NULL, col.end = NULL,
                           funcs = funs(md = median, mn = mean, sd = sd), only.numeric = TRUE,
                           include.n = FALSE) {

  if (!is.null(col.start) & !is.null(col.end) & is.null(var.names)) {
    var.names <- names(data)[match(col.start, names(data)):match(col.end, names(data))]
  }

  if (is.null(var.names) & only.numeric) {
    summarised <- data %>% group_by(!!sym(group)) %>% summarise_if(is.numeric, funcs, na.rm = TRUE)
  } else if (is.null(var.names)) {
    summarised <- data %>% group_by(!!sym(group)) %>% summarise_all(funcs, na.rm = TRUE)
  } else {
    summarised <- data %>% group_by(!!sym(group)) %>% summarise_at(vars(var.names), funcs, na.rm = TRUE)
  }

  if (include.n) {
    data %>%
      group_by(!!sym(group)) %>%
      dplyr::summarise(n.users = n()) %>%
      inner_join(summarised, by = group) %>%
      select(group, n.users, 2 + order(colnames(.)[colnames(.) %not-in% c(group, 'n.users')]))
  } else {
    summarised %>%
      select(group, 1 + order(colnames(.)[colnames(.) != group]))
  }
}
```

```{r connect, echo=FALSE}
if (REFRESH_PUBLIC_DEPLOYMENT_DATA | REFRESH_TURK_STUDY_DATA) drv <- dbDriver("PostgreSQL")
if (REFRESH_PUBLIC_DEPLOYMENT_DATA) {
  db.connection.public.deployment <- dbConnect(drv, dbname = "sidewalk",
                   host = "localhost", port = 5432,
                   user = "sidewalk", password = 'sidewalk')
}
if (REFRESH_TURK_STUDY_DATA) {
  db.connection.turk.study <- dbConnect(drv, dbname = "sidewalkturk",
                   host = "localhost", port = 5432,
                   user = "sidewalk", password = 'sidewalk')
  db.connection.turk.study.production <- dbConnect(drv, dbname = "production-gt",
                   host = "localhost", port = 5432,
                   user = "sidewalk", password = 'sidewalk')
}
```


```{r reading.data, echo=FALSE, include=FALSE}
audits.path <- '../data/stats_for_paper_audits.csv'
times.path <- '../data/stats_for_paper_times.csv'
labels.path <- '../data/stats_for_paper_labels.csv'
attributes.path <- '../data/stats_for_paper_attributes.csv'
sessions.path <- '../data/stats_for_paper_sessions.csv'
missions.path <- '../data/stats_for_paper_reg_missions.csv'
if (REFRESH_PUBLIC_DEPLOYMENT_DATA) {
  reg.labels <-
    dbGetQuery(
      db.connection.public.deployment,
      'SELECT label_id, label_type_id, audit_task.audit_task_id, user_role.user_id, role.role,
              gsv_panorama_id IN (SELECT gsv_panorama_id FROM gsv_onboarding_pano) AS tutorial
      FROM role
      INNER JOIN user_role ON role.role_id = user_role.role_id
      INNER JOIN audit_task ON user_role.user_id = audit_task.user_id
      INNER JOIN street_edge ON audit_task.street_edge_id = street_edge.street_edge_id
      INNER JOIN label ON audit_task.audit_task_id = label.audit_task_id
      WHERE label.deleted = FALSE
          AND street_edge.deleted = FALSE
          AND user_role.user_id <> \'97760883-8ef0-4309-9a5e-0c086ef27573\''
      )
  anon.labels <-
    dbGetQuery(
      db.connection.public.deployment,
      'SELECT DISTINCT label_id,
                       label_type_id,
                       audit_task.audit_task_id,
                       ip_address AS user_id,
                       \'Anonymous\' AS role,
                       gsv_panorama_id IN
                       (
                           SELECT gsv_panorama_id FROM gsv_onboarding_pano
                       ) AS tutorial
      FROM audit_task
      INNER JOIN street_edge
          ON audit_task.street_edge_id = street_edge.street_edge_id
      INNER JOIN label
          ON audit_task.audit_task_id = label.audit_task_id
      INNER JOIN audit_task_environment
          ON audit_task.audit_task_id = audit_task_environment.audit_task_id
      WHERE label.deleted = FALSE
          AND street_edge.deleted = FALSE
          AND user_id = \'97760883-8ef0-4309-9a5e-0c086ef27573\''
      )
  reg.audits <-
    dbGetQuery(
      db.connection.public.deployment,
      'SELECT audit_task.user_id, role.role, audit_task.task_start, audit_task.completed,
              audit_task.audit_task_id, street_edge.street_edge_id,
              ST_LENGTH(ST_TRANSFORM(geom,26918)) * 3.28084 AS feet_audited
      FROM street_edge
      INNER JOIN audit_task
          ON street_edge.street_edge_id = audit_task.street_edge_id
      INNER JOIN user_role
          ON audit_task.user_id = user_role.user_id
      INNER JOIN role
          ON user_role.role_id = role.role_id
      WHERE street_edge.deleted = FALSE
          AND audit_task.user_id <> \'97760883-8ef0-4309-9a5e-0c086ef27573\''
      )
  anon.audits <-
    dbGetQuery(
      db.connection.public.deployment,
      'SELECT DISTINCT ip_address AS user_id, \'Anonymous\' AS role, audit_task.task_start,
              audit_task.completed, audit_task.audit_task_id, street_edge.street_edge_id,
              ST_LENGTH(ST_TRANSFORM(geom,26918)) * 3.28084 AS feet_audited
      FROM street_edge
      INNER JOIN audit_task
          ON street_edge.street_edge_id = audit_task.street_edge_id
      INNER JOIN audit_task_environment
          ON audit_task.audit_task_id = audit_task_environment.audit_task_id
      WHERE street_edge.deleted = FALSE
          AND user_id = \'97760883-8ef0-4309-9a5e-0c086ef27573\''
      )
  reg.times <-
    dbGetQuery(
      db.connection.public.deployment,
      'SELECT user_audit_times.user_id,
             CAST(extract( second from SUM(diff) ) / 60 +
                  extract( minute from SUM(diff) ) +
                  extract( hour from SUM(diff) ) * 60 AS decimal(10,2)) AS minutes_audited
      FROM
      (
          SELECT audit_task.user_id,
                 (timestamp - LAG(timestamp, 1) OVER(PARTITION BY user_id ORDER BY timestamp)) AS diff
          FROM audit_task_interaction
          INNER JOIN audit_task ON audit_task.audit_task_id = audit_task_interaction.audit_task_id
          WHERE action IN (\'ViewControl_MouseDown\', \'LabelingCanvas_MouseDown\')
              AND audit_task.user_id <> \'97760883-8ef0-4309-9a5e-0c086ef27573\'
      ) user_audit_times
      WHERE diff < \'00:05:00.000\' AND diff > \'00:00:00.000\'
      GROUP BY user_audit_times.user_id;'
      )
  anon.times <-
    dbGetQuery(
      db.connection.public.deployment,
      'SELECT user_audit_times.ip_address AS user_id,
             CAST(extract( second from SUM(diff) ) / 60 +
                  extract( minute from SUM(diff) ) +
                  extract( hour from SUM(diff) ) * 60 AS decimal(10,2)) AS minutes_audited
      FROM
      (
          SELECT  user_id, ip_address,
                  (
                      timestamp - Lag(timestamp, 1)
                      OVER(PARTITION BY ip_address ORDER BY timestamp)
                  ) AS diff
          FROM audit_task_interaction
          INNER JOIN audit_task ON audit_task.audit_task_id = audit_task_interaction.audit_task_id
          INNER JOIN audit_task_environment
              ON audit_task.audit_task_id = audit_task_environment.audit_task_id
          WHERE action IN (\'ViewControl_MouseDown\', \'LabelingCanvas_MouseDown\')
          AND audit_task.user_id = \'97760883-8ef0-4309-9a5e-0c086ef27573\'
          AND ip_address IN
          (
              SELECT ip_address
              FROM audit_task_environment
              INNER JOIN audit_task
                  ON audit_task.audit_task_id = audit_task_environment.audit_task_id
              WHERE completed = true
          )
      ) user_audit_times
      WHERE diff < \'00:05:00.000\' AND diff > \'00:00:00.000\'
      GROUP BY ip_address;'
      )
  reg.sessions <-
    dbGetQuery(
      db.connection.public.deployment,
      'SELECT "user".user_id, 1 + COALESCE(n_sessions_minus_one, 0) AS n_sessions
      FROM
      (
          SELECT user_audit_times.user_id, COUNT(diff) AS n_sessions_minus_one
          FROM
          (
              SELECT  audit_task.user_id,
                      (
                          timestamp - LAG(timestamp, 1)
                          OVER(PARTITION BY user_id ORDER BY timestamp)
                      ) AS diff
              FROM audit_task_interaction
              INNER JOIN audit_task
                  ON audit_task.audit_task_id = audit_task_interaction.audit_task_id
              WHERE action IN (\'ViewControl_MouseDown\', \'LabelingCanvas_MouseDown\')
                  AND audit_task.user_id <> \'97760883-8ef0-4309-9a5e-0c086ef27573\'
          ) user_audit_times
          WHERE diff > \'01:00:00.000\'
          GROUP BY user_audit_times.user_id
      ) sess_counts
      RIGHT JOIN "user" ON sess_counts.user_id = "user".user_id;'
      )
  anon.sessions <-
    dbGetQuery(
      db.connection.public.deployment,
      'SELECT ips.ip_address AS user_id, 1 + COALESCE(n_sessions_minus_one, 0) AS n_sessions
      FROM
      (
          SELECT DISTINCT(ip_address) FROM audit_task_environment
      ) ips
      LEFT JOIN
      (
          SELECT user_audit_times.ip_address, COUNT(diff) AS n_sessions_minus_one
          FROM
          (
              SELECT  user_id,
                      ip_address,
                      (
                          timestamp - Lag(timestamp, 1)
                          OVER(PARTITION BY user_id ORDER BY timestamp)
                      ) AS diff
              FROM audit_task_interaction
              INNER JOIN audit_task
                  ON audit_task.audit_task_id = audit_task_interaction.audit_task_id
              INNER JOIN audit_task_environment
                  ON audit_task.audit_task_id = audit_task_environment.audit_task_id
              WHERE action IN (\'ViewControl_MouseDown\', \'LabelingCanvas_MouseDown\')
                  AND audit_task.user_id = \'97760883-8ef0-4309-9a5e-0c086ef27573\'
                  AND ip_address IN
              (
                  SELECT ip_address
                  FROM audit_task_environment
                  INNER JOIN audit_task
                      ON audit_task.audit_task_id = audit_task_environment.audit_task_id
                  WHERE completed = true
              )
          ) user_audit_times
          WHERE diff > \'01:00:00.000\'
          GROUP BY ip_address
      ) sess_counts
          ON ips.ip_address = sess_counts.ip_address;'
      )
  reg.missions <-
    dbGetQuery(
      db.connection.public.deployment,
      'SELECT "user".user_id, COALESCE(count, 0) AS mission_count
      FROM "user"
      LEFT JOIN
      (
          SELECT "user".user_id, COUNT(mission_user_id)
          FROM "user"
          INNER JOIN mission_user ON "user".user_id = mission_user.user_id
          GROUP BY "user".user_id
      ) nonzero_counts
          ON nonzero_counts.user_id = "user".user_id
      WHERE "user".user_id <> \'97760883-8ef0-4309-9a5e-0c086ef27573\''
      )
  attributes <-
    dbGetQuery(
      db.connection.public.deployment,
      'SELECT label_type.label_type, COUNT(*) AS count
      FROM global_attribute
      INNER JOIN label_type ON global_attribute.label_type_id = label_type.label_type_id
      GROUP BY label_type.label_type'
      )
  dbDisconnect(db.connection.public.deployment)
  
  # Combine registered and anonymous user results.
  audits <- bind_rows(reg.audits, anon.audits)
  times <- bind_rows(reg.times, anon.times)
  labels <- bind_rows(reg.labels, anon.labels)
  sessions <- bind_rows(reg.sessions, anon.sessions)

  # Copy the old data files over with timestamp in case we want to look back at them.
  now <- gsub(' ', '_', Sys.time())
  file.copy(audits.path, paste('../data/old/stats_for_paper_audits', now, sep = '_'))
  file.copy(times.path, paste('../data/old/stats_for_paper_times.csv', now, sep = '_'))
  file.copy(labels.path, paste('../data/old/stats_for_paper_labels.csv', now, sep = '_'))
  file.copy(attributes.path, paste('../data/old/stats_for_paper_attributes.csv', now, sep = '_'))
  file.copy(sessions.path, paste('../data/old/stats_for_paper_sessions.csv', now, sep = '_'))
  file.copy(missions.path, paste('../data/old/stats_for_paper_reg_missions.csv', now, sep = '_'))

  # Overwrite old data files with new results of queries.
  write.csv(audits, audits.path, row.names = FALSE)
  write.csv(times, times.path, row.names = FALSE)
  write.csv(labels, labels.path, row.names = FALSE)
  write.csv(attributes, attributes.path, row.names = FALSE)
  write.csv(sessions, sessions.path, row.names = FALSE)
  write.csv(reg.missions, missions.path, row.names = FALSE)
} else {
  audits <- read.csv(audits.path,
                     colClasses = c('character', 'character', 'POSIXct',
                                    'logical', replicate(3, 'numeric')))
  times <- read.csv(times.path, colClasses = c('character', 'numeric'))
  labels <- read.csv(labels.path,
                     colClasses = c(replicate(3, 'numeric'), 'character', 'character', 'logical'))
  attributes <- read.csv(attributes.path, colClasses = c('character', 'numeric'))
  sessions <- read.csv(sessions.path, colClasses = c('character', 'numeric'))
  reg.missions <- read.csv(missions.path, colClasses = c('character', 'numeric'))
}

turk.times.path <- '../data/stats_for_paper_times-turk.csv'
turk.labels.path <- '../data/stats_for_paper_labels-turk.csv'
turk.labeling.times.path <- '../data/stats_for_paper_labeling_times-turk.csv'
route.region.path <- '../data/stats_for_paper_route_region.csv'
if (REFRESH_TURK_STUDY_DATA) {
  reg.labels.turk.study <-
    dbGetQuery(
      db.connection.turk.study.production,
      'SELECT label_id, user_id, label_type_id, audit_task.audit_task_id, amt_condition_id AS condition_id
      FROM amt_condition
      INNER JOIN amt_volunteer_route ON amt_condition.volunteer_id = amt_volunteer_route.volunteer_id
      INNER JOIN route_street ON amt_volunteer_route.route_id = route_street.route_id
      INNER JOIN audit_task ON  route_street.current_street_edge_id = audit_task.street_edge_id
                            AND amt_volunteer_route.volunteer_id = audit_task.user_id
      INNER JOIN label ON audit_task.audit_task_id = label.audit_task_id
      WHERE amt_condition.amt_condition_id NOT IN (71, 104, 123, 124, 138)
          AND label.deleted <> TRUE'
      )
  anon.labels.turk.study <-
    dbGetQuery(
      db.connection.turk.study.production,
      'SELECT DISTINCT label_id, audit_task_environment.ip_address AS user_id, label_type_id,
                       audit_task.audit_task_id, amt_condition_id AS condition_id
      FROM amt_condition
      INNER JOIN amt_volunteer_route
          ON amt_condition.volunteer_id = amt_volunteer_route.volunteer_id
      INNER JOIN route_street ON amt_volunteer_route.route_id = route_street.route_id
      INNER JOIN audit_task
          ON  route_street.current_street_edge_id = audit_task.street_edge_id
      INNER JOIN audit_task_environment
          ON  audit_task.audit_task_id = audit_task_environment.audit_task_id
          AND amt_volunteer_route.ip_address = audit_task_environment.ip_address
      INNER JOIN label ON audit_task.audit_task_id = label.audit_task_id
      WHERE amt_condition.amt_condition_id NOT IN (71, 104, 123, 124, 138)
          AND label.deleted <> TRUE'
      )
  turk.labels.turk.study <-
    dbGetQuery(
      db.connection.turk.study,
      'SELECT label_id, turker_id AS user_id, label_type_id, audit_task.audit_task_id, condition_id
      FROM amt_assignment
      INNER JOIN audit_task ON amt_assignment.amt_assignment_id = audit_task.amt_assignment_id
      INNER JOIN label ON audit_task.audit_task_id = label.audit_task_id
      WHERE amt_assignment.condition_id NOT IN (71, 104, 123, 124, 138)
          AND label.deleted <> TRUE'
      )
  reg.times.turk.study <-
    dbGetQuery(
      db.connection.turk.study.production,
      'SELECT user_audit_times.user_id,
             CAST(extract( second from SUM(diff) ) / 60 +
                  extract( minute from SUM(diff) ) +
                  extract( hour from SUM(diff) ) * 60 AS decimal(10,2)) AS minutes_audited
      FROM
      (
          SELECT audit_task.user_id,
                 (
                     timestamp - LAG(timestamp, 1)
                     OVER(PARTITION BY user_id ORDER BY timestamp)
                 ) AS diff
          FROM amt_condition
          INNER JOIN amt_volunteer_route
              ON amt_condition.volunteer_id = amt_volunteer_route.volunteer_id
          INNER JOIN route_street ON amt_volunteer_route.route_id = route_street.route_id
          INNER JOIN audit_task
              ON route_street.current_street_edge_id = audit_task.street_edge_id
              AND amt_volunteer_route.volunteer_id = audit_task.user_id
          INNER JOIN audit_task_interaction
              ON audit_task.audit_task_id = audit_task_interaction.audit_task_id
          WHERE action IN (\'ViewControl_MouseDown\', \'LabelingCanvas_MouseDown\')
              AND amt_condition_id NOT IN (71, 104, 123, 124, 138)
          ) user_audit_times
      WHERE diff < \'00:05:00.000\' AND diff > \'00:00:00.000\'
      GROUP BY user_audit_times.user_id;'
      )
  anon.times.turk.study <-
    dbGetQuery(
      db.connection.turk.study.production,
      'SELECT user_audit_times.user_id,
             CAST(extract( second from SUM(diff) ) / 60 +
                  extract( minute from SUM(diff) ) +
                  extract( hour from SUM(diff) ) * 60 AS decimal(10,2)) AS minutes_audited
      FROM
      (
          SELECT amt_volunteer_route.ip_address AS user_id,
                 (
                     timestamp - LAG(timestamp, 1)
                     OVER(PARTITION BY amt_volunteer_route.ip_address ORDER BY timestamp)
                 ) AS diff
          FROM amt_condition
          INNER JOIN amt_volunteer_route
              ON amt_condition.volunteer_id = amt_volunteer_route.volunteer_id
          INNER JOIN route_street ON amt_volunteer_route.route_id = route_street.route_id
          INNER JOIN audit_task ON route_street.current_street_edge_id = audit_task.street_edge_id
          INNER JOIN audit_task_environment
              ON  audit_task.audit_task_id = audit_task_environment.audit_task_id
              AND amt_volunteer_route.ip_address = audit_task_environment.ip_address
          INNER JOIN audit_task_interaction
              ON audit_task.audit_task_id = audit_task_interaction.audit_task_id
          WHERE action IN (\'ViewControl_MouseDown\', \'LabelingCanvas_MouseDown\')
              AND amt_condition_id NOT IN (71, 104, 123, 124, 138)
          ) user_audit_times
      WHERE diff < \'00:05:00.000\' AND diff > \'00:00:00.000\'
      GROUP BY user_audit_times.user_id;'
      )
  turk.times.turk.study <-
    dbGetQuery(
      db.connection.turk.study,
      'SELECT user_audit_times.user_id,
             CAST(extract( second from SUM(diff) ) / 60 +
                  extract( minute from SUM(diff) ) +
                  extract( hour from SUM(diff) ) * 60 AS decimal(10,2)) AS minutes_audited
      FROM
      (
          SELECT turker_id AS user_id,
                 (timestamp - LAG(timestamp, 1) OVER(PARTITION BY turker_id ORDER BY timestamp)) AS diff
          FROM amt_assignment
          INNER JOIN audit_task ON amt_assignment.amt_assignment_id = audit_task.amt_assignment_id
          INNER JOIN audit_task_interaction ON audit_task.audit_task_id = audit_task_interaction.audit_task_id
          WHERE action IN (\'ViewControl_MouseDown\', \'LabelingCanvas_MouseDown\')
              AND condition_id NOT IN (71, 104, 123, 124, 138)
          ) user_audit_times
      WHERE diff < \'00:05:00.000\' AND diff > \'00:00:00.000\'
      GROUP BY user_audit_times.user_id;'
      )
  reg.labeling.times.turk.study <-
    dbGetQuery(
      db.connection.turk.study.production,
      'SELECT user_id, note,
              EXTRACT
              (
                  EPOCH FROM
                  (
                      pano_time - LAG(pano_time, 1, \'00:00:00.000\')
                          OVER (PARTITION BY user_id, gsv_panorama_id ORDER BY timestamp)
                  )
              ) AS seconds_to_label
      FROM
      (
          SELECT user_id, gsv_panorama_id, note, action, timestamp,
                 SUM(timediff)
                     OVER (PARTITION BY user_id, gsv_panorama_id ORDER BY timestamp) AS pano_time
          FROM
          (
              SELECT user_id, panos.gsv_panorama_id, timestamp, note, action,
                      timestamp - LAG(timestamp) OVER
                          (
                              PARTITION BY user_id, panos.gsv_panorama_id ORDER BY timestamp
                          ) AS timediff
              FROM
              (
                  SELECT DISTINCT audit_task.user_id,
                                  gsv_panorama_id,
                                  audit_task_environment.audit_task_id
                  FROM amt_condition
                  INNER JOIN amt_volunteer_route
                      ON amt_condition.volunteer_id = amt_volunteer_route.volunteer_id
                  INNER JOIN route_street
                      ON amt_volunteer_route.route_id = route_street.route_id
                  INNER JOIN audit_task
                      ON  route_street.current_street_edge_id = audit_task.street_edge_id
                      AND amt_volunteer_route.volunteer_id = audit_task.user_id
                  INNER JOIN audit_task_interaction
                      ON audit_task.audit_task_id = audit_task_interaction.audit_task_id
                  INNER JOIN audit_task_environment
                      ON audit_task_interaction.audit_task_id = audit_task_environment.audit_task_id
                  WHERE action = \'LabelingCanvas_FinishLabeling\'
                      AND amt_condition_id NOT IN (71, 104, 123, 124, 138)
              ) panos
              INNER JOIN audit_task_interaction
                  ON panos.audit_task_id = audit_task_interaction.audit_task_id
                  AND panos.gsv_panorama_id = audit_task_interaction.gsv_panorama_id
          ) interactions_with_time_diffs
          WHERE timediff < \'00:01:00.000\'
      ) interactions_with_time_since_entering_pano
      WHERE action = \'LabelingCanvas_FinishLabeling\';'
    )
  anon.labeling.times.turk.study <-
    dbGetQuery(
      db.connection.turk.study.production,
      'SELECT user_id, note,
              EXTRACT
              (
                  EPOCH FROM
                  (
                      pano_time - LAG(pano_time, 1, \'00:00:00.000\')
                          OVER (PARTITION BY user_id, gsv_panorama_id ORDER BY timestamp)
                  )
              ) AS seconds_to_label
      FROM
      (
          SELECT user_id, gsv_panorama_id, note, action, timestamp,
                 SUM(timediff)
                     OVER (PARTITION BY user_id, gsv_panorama_id ORDER BY timestamp) AS pano_time
          FROM
          (
              SELECT user_id, panos.gsv_panorama_id, timestamp, note, action,
                     timestamp - LAG(timestamp) OVER
                         (
                             PARTITION BY user_id, panos.gsv_panorama_id ORDER BY timestamp
                         ) AS timediff
              FROM
              (
                  SELECT DISTINCT amt_volunteer_route.ip_address AS user_id,
                                  gsv_panorama_id,
                                  audit_task_environment.audit_task_id
                  FROM amt_condition
                  INNER JOIN amt_volunteer_route
                      ON amt_condition.volunteer_id = amt_volunteer_route.volunteer_id
                  INNER JOIN route_street
                      ON amt_volunteer_route.route_id = route_street.route_id
                  INNER JOIN audit_task
                      ON  route_street.current_street_edge_id = audit_task.street_edge_id
                  INNER JOIN audit_task_environment
                      ON audit_task.audit_task_id = audit_task_environment.audit_task_id
                      AND amt_volunteer_route.ip_address = audit_task_environment.ip_address
                  INNER JOIN audit_task_interaction
                      ON audit_task_environment.audit_task_id = audit_task_interaction.audit_task_id
                  WHERE action = \'LabelingCanvas_FinishLabeling\'
                      AND amt_condition_id NOT IN (71, 104, 123, 124, 138)
              ) panos
              INNER JOIN audit_task_interaction
                  ON panos.audit_task_id = audit_task_interaction.audit_task_id
                  AND panos.gsv_panorama_id = audit_task_interaction.gsv_panorama_id
          ) interactions_with_time_diffs
          WHERE timediff < \'00:01:00.000\'
      ) interactions_with_time_since_entering_pano
      WHERE action = \'LabelingCanvas_FinishLabeling\';'
    )
  turk.labeling.times.turk.study <-
    dbGetQuery(
      db.connection.turk.study,
      'SELECT user_id, note,
              EXTRACT
              (
                  EPOCH FROM
                  (
                      pano_time - LAG(pano_time, 1, \'00:00:00.000\')
                          OVER (PARTITION BY user_id, gsv_panorama_id ORDER BY timestamp)
                  )
              ) AS seconds_to_label
      FROM
      (
          SELECT user_id, gsv_panorama_id, note, action, timestamp,
                 SUM(timediff)
                     OVER (PARTITION BY user_id, gsv_panorama_id ORDER BY timestamp) AS pano_time
          FROM
          (
              SELECT user_id, panos.gsv_panorama_id, timestamp, note, action,
                     timestamp - LAG(timestamp) OVER
                         (
                             PARTITION BY user_id, panos.gsv_panorama_id ORDER BY timestamp
                         ) AS timediff
              FROM
              (
                  SELECT DISTINCT amt_assignment.turker_id AS user_id,
                                  gsv_panorama_id,
                                  audit_task_environment.audit_task_id
                  FROM amt_assignment
                  INNER JOIN
                  (
                      SELECT turker_id, COUNT(DISTINCT(route_id))
                      FROM amt_assignment
                      WHERE completed = TRUE
                      GROUP BY turker_id
                  ) completed_route_counts
                      ON amt_assignment.turker_id = completed_route_counts.turker_id
                  INNER JOIN audit_task
                      ON amt_assignment.amt_assignment_id = audit_task.amt_assignment_id
                  INNER JOIN audit_task_interaction
                      ON audit_task.audit_task_id = audit_task_interaction.audit_task_id
                  INNER JOIN audit_task_environment
                      ON audit_task_interaction.audit_task_id = audit_task_environment.audit_task_id
                  WHERE ((count > 1 AND condition_id >= 122) OR (count > 2 AND condition_id < 122))
                      AND action = \'LabelingCanvas_FinishLabeling\'
                      AND condition_id NOT IN (71, 104, 123, 124, 138)
              ) panos
              INNER JOIN audit_task_interaction
                  ON panos.audit_task_id = audit_task_interaction.audit_task_id
                  AND panos.gsv_panorama_id = audit_task_interaction.gsv_panorama_id
          ) interactions_with_time_diffs
          WHERE timediff < \'00:01:00.000\'
      ) interactions_with_time_since_entering_pano
      WHERE action = \'LabelingCanvas_FinishLabeling\';'
      )
  route.regions <-
    dbGetQuery(
      db.connection.turk.study,
      'SELECT DISTINCT amt_condition_id AS condition_id, region_id
      FROM amt_condition
      INNER JOIN amt_volunteer_route
          ON amt_condition.volunteer_id = amt_volunteer_route.volunteer_id
      INNER JOIN route
          ON amt_volunteer_route.route_id = route.route_id
      WHERE amt_condition_id NOT IN (71, 104, 123, 124, 138)'
    )
  dbDisconnect(db.connection.turk.study.production)
  dbDisconnect(db.connection.turk.study)
  
  # Combine results of registerd volunteers, anon volunteers, and turkers.
  times.turk.study <- bind_rows(reg.times.turk.study, anon.times.turk.study, turk.times.turk.study)
  labels.turk.study <- bind_rows(reg.labels.turk.study, anon.labels.turk.study, turk.labels.turk.study)
  labeling.times.turk.study <- bind_rows(reg.labeling.times.turk.study,
                                         anon.labeling.times.turk.study,
                                         turk.labeling.times.turk.study)

  # Copy the old data files over with timestamp in case we want to look back at them.
  now <- gsub(' ', '_', Sys.time())
  file.copy(turk.times.path, paste('../data/old/stats_for_paper_times-turk.csv', now, sep = '_'))
  file.copy(turk.labels.path, paste('../data/old/stats_for_paper_labels-turk.csv', now, sep = '_'))
  file.copy(turk.labeling.times.path,
            paste('../data/old/stats_for_paper_labeling_times-turk.csv', now, sep = '_'))
  file.copy(route.region.path, paste('../data/old/stats_for_paper_route_region.csv', now, sep = '_'))

  # Overwrite old data files with new results of queries.
  write.csv(times.turk.study, turk.times.path, row.names = FALSE)
  write.csv(labels.turk.study, turk.labels.path, row.names = FALSE)
  write.csv(labeling.times.turk.study, turk.labeling.times.path, row.names = FALSE)
  write.csv(route.regions, route.region.path, row.names = FALSE)
} else {
  times.turk.study <- read.csv(turk.times.path, colClasses = c('character', 'numeric'))
  labels.turk.study <- read.csv(turk.labels.path,
                                colClasses = c('numeric', 'character', replicate(3, 'numeric')))
  labeling.times.turk.study <- read.csv(turk.labeling.times.path,
                                        colClasses = c('character', 'character', 'numeric'))
  route.regions <- read.csv(route.region.path, colClasses = c('numeric', 'numeric'))
}

zoning.classes <- c('numeric', 'numeric', 'numeric', 'factor', 'factor', 'logical')
zoning.data <- read.csv('../data/street_zoning_info.csv', colClasses = zoning.classes) %>%
  rename(condition.id = condition_id)
```

```{r transforming.data, echo=FALSE, warning=FALSE}
# Change the _ column names to . separated.
names(labeling.times.turk.study) <- gsub("\\_", ".", names(labeling.times.turk.study))

# Compute median labeling time by label type for each user.
individual.search.times <-
  labeling.times.turk.study %>%
  mutate(label.type = factor(extract_label_type(note))) %>%
  mutate(label.type = fct_recode(label.type, 'SurfaceProb' = 'SurfaceProblem')) %>%
  filter(label.type %in% LABEL_TYPES_TO_ANALYZE) %>%
  dplyr::select(-note)
individual.search.times.prob <-
  individual.search.times %>%
  filter(label.type %in% c('NoCurbRamp', 'Obstacle', 'SurfaceProb')) %>%
  mutate(label.type = factor('Problem'))
individual.search.times.all <-
  individual.search.times %>%
  mutate(label.type = factor('All'))

user.search.times <-
  rbind(individual.search.times.all, individual.search.times.prob, individual.search.times) %>%
  group_by(user.id, label.type) %>%
  summarize(seconds.to.label = median(seconds.to.label)) %>%
  ungroup() %>%
  droplevels()

# Takes a distance in feet and approximates the number of missions the user completed. The
# approximation assumes that the first missions are 500, 500, 1000, 2000, and 1280 feet, and all the
# remaining missions are half a mile (2640 feet).
approximate.missions.completed <- Vectorize(
  function(dist.in.feet) {
    if (dist.in.feet > 7920) 5 + floor((dist.in.feet - 5280) / 2640)
    else if (dist.in.feet > 5280) 5
    else if (dist.in.feet > 4000) 4
    else if (dist.in.feet > 2000) 3
    else if (dist.in.feet > 1000) 2
    else if (dist.in.feet > 500) 1
    else 0
  }
)

# Computes minutes_per_mission as either minutes_audited / missions_completed if at least one
# mission was completed, other wise 500 * minutes_audited / feet_audited (how long the first mission
# would take, given the auditing speed for what they had done)
approx.min.per.mission <- Vectorize(
  function(time.audited, missions.completed, dist.in.feet) {
    if (missions.completed > 0) time.audited / missions.completed
    else 500 * time.audited / dist.in.feet
  }
)

tutorial.labels <- labels %>% filter(tutorial == TRUE) %>% select(-tutorial)
non.tutorial.labels <- labels %>% filter(tutorial == FALSE) %>% select(-tutorial)

n.tutorial.labels <- n_distinct(tutorial.labels$label_id)
n.non.tutorial.labels <- n_distinct(non.tutorial.labels$label_id)

pre.100.labels <-
  inner_join(non.tutorial.labels, audits, by = c('user_id', 'role', 'audit_task_id')) %>%
  filter(task_start < '2017-12-10 07:44:54.288+00') %>%
  mutate(dataset = 'first_100%') %>%
  select(names(non.tutorial.labels), dataset)
label.counts <-
  non.tutorial.labels %>%
  mutate(dataset = 'all') %>%
  bind_rows(pre.100.labels) %>%
  group_by(user_id, role, audit_task_id, dataset) %>%
  dplyr::summarize(label_count = n())

# Selects distinct on audit_task_id, then sums dist audited.
pre.100.audits <-
  audits %>%
  filter(task_start < '2017-12-10 07:44:54.288+00') %>%
  mutate(dataset = 'first_100%')
audit.length <-
  audits %>%
  mutate(dataset = 'all') %>%
  bind_rows(pre.100.audits) %>%
  group_by(user_id, role, dataset) %>%
  distinct(audit_task_id, .keep_all = TRUE) %>%
  left_join(label.counts, by  = c('user_id', 'role', 'dataset', 'audit_task_id')) %>%
  replace_na(list(label_count = 0)) %>%
  dplyr::summarise(label_count = sum(label_count),
                   feet_audited = sum(feet_audited[completed == TRUE]),
                   audit_count = length(audit_task_id[completed == TRUE])) %>%
  replace_na(list(feet_audited = 0)) %>%
  ungroup() %>%
  mutate(miles_audited = feet_audited / 5280) %>%
  left_join(sessions %>% mutate(dataset = 'all'), by = c('user_id', 'dataset')) %>%
  mutate(dataset = factor(dataset),
         role = factor(role),
         role = recode(role, User = 'Registered', .default = levels(role))) %>%
  mutate(labels_per_100m = 328.084 * label_count / feet_audited)

user.list <-
  audit.length %>%
  filter(dataset == 'all', audit_count > 0, label_count > 0) %>%
  pull(user_id)
good.user.list <-
  audit.length %>%
  filter(dataset == 'all', audit_count > 0, labels_per_100m >= 3.75) %>%
  pull(user_id)

# Computes approximate missions completed for anon users.
all.mission.counts <-
  audit.length %>%
  filter(dataset == 'all',
         role == "Anonymous") %>%
  mutate(mission_count = approximate.missions.completed(feet_audited)) %>%
  select(user_id, mission_count) %>%
  bind_rows(reg.missions)

# Computes a few metrics based on distance audited, time auditing, and missions completed.
speeds <-
  audit.length %>%
  filter(dataset == 'all',
         user_id %in% user.list) %>%
  left_join(all.mission.counts, by = 'user_id') %>%
  left_join(times, by = 'user_id') %>%
  mutate(meters_audited = feet_audited / 3.28084,
         m_per_min = meters_audited / minutes_audited,
         mins_per_mission = approx.min.per.mission(minutes_audited, mission_count, feet_audited),
         hours_audited = minutes_audited / 60,
         minutes_per_session = minutes_audited / n_sessions)

filtered.speeds <-
  speeds %>%
  filter(role %in% c('Registered', 'Anonymous', 'Turker'),
         audit_count > 0,
         labels_per_100m >= 3.75)
```

# Public Deployment

NOTE: Public deployment data includes all data up through March 31st (and part of April 1st). This includes all data through the most recent deployment on mturk. We only consider someone a "user" if they have completed an audit of at least one street and have placed at least one label after the tutorial. Later on, we also filter out users with low labeling frequency.

## High-level results

### Top-line numbers (no filtering)

The following are the label counts (not attribute counts) by user group and label type. There are a total of `r n.tutorial.labels + n.non.tutorial.labels` labels, `r n.tutorial.labels` are tutorial labels and `r n.non.tutorial.labels` are non tutorial labels. We consider only non tutorial labels throughout this document.

```{r public.deployment.how.much.data, echo=FALSE, warning=FALSE}
ordered.label.type.levs <- c('CurbRamp', 'NoCurbRamp', 'Obstacle', 'SurfaceProblem', 'NoSidewalk',
                             'Occlusion', 'Other')
kable(
  non.tutorial.labels %>%
    distinct(label_id, .keep_all = TRUE) %>%
    mutate(label_type = LABEL_TYPE_MAPPING[label_type_id]) %>%
    mutate(label_type = factor(label_type, ordered.label.type.levs)) %>%
    mutate(role = recode_factor(role,
                                `Anonymous` = 'Anon',
                                `User` = 'Registered',
                                `Turker` = 'Turker',
                                `Administrator` = 'Researcher',
                                `Owner` = 'Researcher')) %>%
    select(label_type, role) %>%
    tabyl(label_type, role) %>%
    adorn_totals(where = c('row', 'col')) %>%
    adorn_percentages(denominator = 'all') %>%
    adorn_pct_formatting() %>%
    adorn_ns(position = 'front'),
  align = 'l'
)
```

### Attribute counts by type

Here are the counts of attributes by attribute type after single and multi user clustering.

```{r public.deployment.attributes, echo=FALSE, warning=FALSE}
ordered.attribute.type.levs <- c('CurbRamp', 'NoCurbRamp', 'Obstacle', 'SurfaceProblem',
                                 'NoSidewalk', 'Occlusion', 'Other', 'Problem', 'Total')
total.attributes <- attributes %>% filter(label_type != 'Problem') %>% pull(count) %>% sum
kable(
  attributes %>%
    rbind(data.frame(label_type = c('Total'), count = c(total.attributes))) %>%
    mutate(attribute.type = factor(label_type, ordered.attribute.type.levs)) %>%
    select(attribute.type, count) %>%
    mutate(percentage = if_else(attribute.type != 'Problem',
                                paste0(format(100 * count / total.attributes, digits = 1), '%'),
                                '-')) %>%
    arrange(attribute.type),
  align = 'l')
```

### Data characteristics

This is the start of filtering out users with low labeling frequency (also filtering out researchers).

```{r public.deployment.data.characteristics, echo=FALSE, warning=FALSE}
filtered.labels <- filter(non.tutorial.labels, user_id %in% filtered.speeds$user_id)
kable(
  filtered.labels %>%
    distinct(label_id, .keep_all = TRUE) %>%
    count(label_type_id) %>%
    mutate(label_type = LABEL_TYPE_MAPPING[label_type_id]) %>%
    select(label_type, n) %>%
    bind_rows(list(label_type = 'Total', n = n_distinct(filtered.labels$label_id))) %>%
    spread(label_type, n),
  align = 'l'
)

filtered.audits <- filter(audits, user_id %in% filtered.speeds$user_id, completed == TRUE)
n.audits <- nrow(filtered.audits)
n.streets <- n_distinct(filtered.audits$street_edge_id)

streets.with.mult.audits <-
  filtered.audits %>%
  group_by(street_edge_id) %>%
  dplyr::summarise(n.audits = n()) %>%
  filter(n.audits > 1)
```

There have been a total of `r n.audits` audits by our "good" users across `r n.streets` streets, averaging `r format(n.audits/n.streets, digits = 3)` audits per street. Of the `r nrow(streets.with.mult.audits)` streets that have been audited multiple times, there are an average of `r format(sum(streets.with.mult.audits$n.audits) / nrow(streets.with.mult.audits), digits = 3)` audits per street.


### Dataset 1st 100% vs full deployment

The first table describes the dataset at the point where we hit 100% completion. The second table describes the full dataset. Both include _all_ users, not just "good" users.

```{r public.deployment.first.100, echo=FALSE, warning=FALSE}
kable(
  audit.length %>%
    filter(dataset == 'first_100%') %>%
    mutate(role = fct_recode(role, 'Researcher' = 'Administrator', 'Researcher' = 'Owner')) %>%
    summarise_vars('role', c('miles_audited', 'label_count'), funcs = sum, include.n = TRUE),
  digits = 0,
  align = 'l'
)
kable(
  audit.length %>%
    filter(dataset == 'all') %>%
    mutate(role = fct_recode(role, 'Researcher' = 'Administrator', 'Researcher' = 'Owner')) %>%
    summarise_vars('role', c('miles_audited', 'label_count'), funcs = sum, include.n = TRUE),
  digits = 0,
  align = 'l'
)
```


### Data lost due to filtering

```{r public.deployment.data.lost, echo=FALSE, warning=FALSE}
researcher.ids <-
  speeds %>%
  filter(role %in% c('Researcher', 'Administrator', 'Owner')) %>%
  distinct(user_id) %>%
  pull()

bad.non.researchers.ids <-
  speeds %>%
  filter(role %in% c('Registered', 'Anonymous', 'Turker'),
         labels_per_100m < 3.75) %>%
  distinct(user_id) %>%
  pull()

n.users.pre.filter <- n_distinct(speeds$user_id)
n.researchers <- length(researcher.ids)
n.bad.non.researchers <- length(bad.non.researchers.ids)
n.filtered.users <- n.researchers + n.bad.non.researchers
n.users.remaining <- n_distinct(filtered.speeds$user_id)

percent.researchers <- format(100 * n.researchers / n.users.pre.filter, digits = 3)
percent.bad.non.researchers <- format(100 * n.bad.non.researchers / n.users.pre.filter, digits = 3)
percent.filtered.users <- format(100 * n.filtered.users / n.users.pre.filter, digits = 3)
percent.users.remaining <- format(100 * n.users.remaining / n.users.pre.filter, digits = 3)

n.labels.pre.filter <- n_distinct(non.tutorial.labels$label_id)
n.labels.researcher <- n_distinct(non.tutorial.labels[non.tutorial.labels$user_id %in% researcher.ids,'label_id'])
n.labels.bad.non.researcher <- n_distinct(non.tutorial.labels[non.tutorial.labels$user_id %in% bad.non.researchers.ids,'label_id'])
n.filtered.labels <- n.labels.researcher + n.labels.bad.non.researcher
n.labels.remaining <- n_distinct(filtered.labels$label_id)

percent.labels.researcher <- format(100 * n.labels.researcher / n.labels.pre.filter, digits = 3)
percent.labels.bad.non.researcher <- format(100 * n.labels.bad.non.researcher / n.labels.pre.filter, digits = 3)
percent.filtered.labels <- format(100 * n.filtered.labels / n.labels.pre.filter, digits = 3)
percent.labels.remaining <- format(100 * n.labels.remaining / n.labels.pre.filter, digits = 3)
```

There were `r n.users.pre.filter` users who placed `r n.labels.pre.filter` labels pre-filtering. Researchers accounted for `r n.researchers` of the users (`r percent.researchers`%) and `r n.labels.researcher` of the labels (`r percent.labels.researcher`%). Non-researchers with low labeling frequency accounted for `r n.bad.non.researchers` of the users (`r percent.bad.non.researchers`%) and `r n.labels.bad.non.researcher` of the labels (`r percent.labels.bad.non.researcher`%). This means that we filtered out a total of `r n.filtered.users` of the users (`r percent.filtered.users`%) and `r n.filtered.labels` of the labels (`r percent.filtered.labels`%), and are left with `r n.users.remaining` of the users (`r percent.users.remaining`%) and `r n.labels.remaining` of the labels (`r percent.labels.remaining`%).


### User stats and tool usage

TODO: Missions started vs missions completed (not sure we can do this; I expect it to be difficult, without much benefit).

Below are the means/medains/sds for a few metrics (followed by sums), split by user group. For all user groups, the minimum threshold to be included in this list was that they have completed at least one audit task and that their labeling threshold is above 3.75 labels per 100 meters.

NOTE: A "session" below is defined as a sequence of audit task interactions for a user where the minimum time between consecutive interactions is less than one hour.

```{r public.deployment.showing.stats, echo=FALSE, warning=FALSE}
kable(
  filtered.speeds %>%
    mutate(km = meters_audited / 1000) %>%
    select(-meters_audited, -feet_audited, -mins_per_mission, -hours_audited) %>%
    summarise_vars('role', include.n = TRUE),
  digits = 3,
  align = 'l'
  )
kable(
  filtered.speeds %>%
    mutate(km = meters_audited / 1000,
           mult_sess = as.numeric(n_sessions > 1)) %>%
    rename(miles = miles_audited,
           audits = audit_count,
           labels = label_count,
           missions = mission_count) %>%
    select(role, miles, km, missions, audits, hours_audited, labels, mult_sess) %>%
    summarise_vars('role', funcs = sum, include.n = TRUE) %>%
    mutate(coverage = paste0(format(100 * miles / TOTAL_STREET_DIST_MILES, digits = 2), '%'),
           '>1 sess' = paste0(format(100 * mult_sess / n.users, digits = 2), '%')) %>%
    select(-mult_sess),
  align = 'l',
  digits = 0
  )
```


## Possible Stories

### Data overlap and agreement between users

Among all the data collected in DC, how much of DC is labeled by multiple users and what is the disagreement among them? (see comment in Outline document for details on implementation)

```{r public.deployment.agreement.analysis, echo=FALSE, fig.width=7, fig.height=4}
percent.streets.with.multiple.audits <-
  filtered.audits %>%
  group_by(street_edge_id) %>%
  dplyr::summarise(n.audits = n()) %>%
  filter(n.audits > 1) %>%
  nrow() * 100 / n.streets
```

A total of `r format(percent.streets.with.multiple.audits, digits = 3)`% of streets were audited by multiple users. 


### Stickyness of tool: user dropoffs

We want a bar chart here showing, after a user clicks start mapping, what percentage finish the tutorial, what percentage finish a mission, etc.


# Turk Study

Update: This is now all of the data. There used to be 19 anonymous user routes, but three of them actually had no labels placed by the anonymous user (we had forgotten to check beforehand), thus we have only 16.

Even though 5 turkers did each route, the high level results for individual turkers looks only at the first turker to complete each set of routes. This makes aggregate stats more even, and a fairer comparison across user groups. (but maybe we should actually use all turkers when not aggregating, actually...)

## High level results

```{r turk.reading.cleaning.data, echo=FALSE, include=FALSE}
# Get IRR data
agreement.classes <- c('numeric', 'factor', 'factor', 'factor', 'numeric', 'numeric')
agreement.data <- read.csv('../data/irr_results-not_for_humans.csv', colClasses = agreement.classes)

label.type.levels.in.order <- c('CurbRamp','NoCurbRamp','Obstacle','SurfaceProb','Problem')
clean.agreement.data <-
  agreement.data %>%
  mutate(label.type = case_when(
            data.type %in% c('prob.binary', 'prob.ordinal') ~ 'Problem',
            label.type == 'SurfaceProblem'                  ~ 'SurfaceProb',
            TRUE                                            ~ as.character(label.type)
          ),
         data.type = if_else(data.type %in% c('prob.binary', 'binary'),
                             'binary',
                             'ordinal')) %>%
  filter(level == 'street',
         data.type == 'binary',
         label.type %in% c('CurbRamp', 'NoCurbRamp', 'Obstacle', 'SurfaceProb', 'Problem')) %>%
  mutate(data.type = factor(data.type),
         level = factor(level),
         label.type = factor(label.type, label.type.levels.in.order))


# Get accuracy data
classes <- c('numeric', replicate(5, 'character'), 'numeric', 'numeric', 'logical', 'character',
             'factor', 'character', replicate(8, 'numeric'))
volunteer.data <- read.csv('../data/accuracies-volunteer.csv',
                           colClasses = classes,
                           na.strings = c('null')) %>%
  mutate(is.turker = FALSE)
turker.data <- read.csv('../data/accuracies-turker.csv',
                        colClasses = classes,
                        na.strings = c('null')) %>%
  mutate(is.turker = TRUE)

# Combine datasets
accuracy.data <- rbind(volunteer.data, turker.data)

# Create data for summing true/false pos/neg over all label types, and all problem label types.
# First summing over curb ramp, missing curb ramp, obstacle, and surface problem...
all.types.data <-
  accuracy.data %>%
  filter(label.type %in% c('CurbRamp', 'NoCurbRamp', 'Obstacle', 'SurfaceProblem')) %>%
  group_by(condition.id, worker1, worker2, worker3, worker4, worker5, n.workers, worker.thresh,
           binary, included.severity, granularity, is.turker) %>%
  dplyr::summarize(label.type = 'All',
                   true.pos = sum(true.pos),
                   false.pos = sum(false.pos),
                   true.neg = sum(true.neg),
                   false.neg = sum(false.neg)) %>%
  ungroup()

# Then summing over just missing curb ramp, obstacle, and surface problem...
problem.types.data <-
  accuracy.data %>%
  filter(label.type %in% c('NoCurbRamp', 'Obstacle', 'SurfaceProblem')) %>%
  group_by(condition.id, worker1, worker2, worker3, worker4, worker5, n.workers, worker.thresh,
           binary, included.severity, granularity, is.turker) %>%
  dplyr::summarize(label.type = 'AllProb',
                   true.pos = sum(true.pos),
                   false.pos = sum(false.pos),
                   true.neg = sum(true.neg),
                   false.neg = sum(false.neg)) %>%
  ungroup()

# Now we combine them and compute the accuracy metrics for each row, and attach to main dataset.
data.with.agg.types <-
  all.types.data %>%
  bind_rows(problem.types.data) %>%
  mutate(precision = true.pos / (true.pos + false.pos),
         recall = true.pos / (true.pos + false.neg),
         specificity = true.neg / (true.neg + false.pos),
         f.measure = 2 * (precision * recall) / (precision + recall)) %>%
  bind_rows(accuracy.data)

# Remove occlusion and other label types, rename SurfaceProblem label type as SurfaceProb for
# easier visualization and set order of factor levels.
label.types <- c('All', 'Problem', 'CurbRamp', 'NoCurbRamp', 'Obstacle', 'SurfaceProb', 'NoSidewalk', 'AllProb')
data.filtered.label.type <-
  data.with.agg.types %>%
  mutate(label.type = if_else(label.type == 'SurfaceProblem', 'SurfaceProb', label.type)) %>%
  filter(label.type %in% label.types) %>%
  mutate(label.type = factor(label.type, levels = label.types, labels = label.types))

# More setup: remove binary analysis (except street level), add is.anon.route column, add
#             raw.accuracy column, add worker.type column, give granularity an ordering.
get.worker.type <- Vectorize(
  function(n.workers, worker.thresh, is.turker, is.anon.route) {
    if (n.workers == 5 & worker.thresh == 3) 'turk5'
    else if (n.workers == 3 & worker.thresh == 2) 'turk3'
    else if (n.workers == 1 & is.turker == TRUE) 'turk1'
    else if (is.turker == TRUE) 'turk0' # probably aren't analyzing any other turkers
    else if (is.anon.route == TRUE) 'anon'
    else 'reg'
  }
)
# The user.id column has the user id for reg and turk users, but for anon they are of the form
# '<anon.user.id>---<ip.address>'. Since the anon user id is the same for everyone, we just want the
# ip address. So this function splits on '---' (which only occurs for anon users), reverses the
# output of the split (meaning no ip address is now first in output list for anon users, and no
# change for other users, since the list is of length one), then takes first element of that list.
fix.anon.names <- Vectorize(
  function(user.id) {
    rev(strsplit(user.id, '---', fixed = TRUE)[[1]])[1]
  }
)
  
data.with.raw.accuracy <-
  data.filtered.label.type %>%
  filter(granularity != '10_meter') %>%
  mutate(is.anon.route = condition.id > 121) %>%
  mutate(worker.type = factor(get.worker.type(n.workers, worker.thresh, is.turker, is.anon.route),
                              levels = c('anon', 'reg', 'turk1', 'turk3', 'turk5', 'turk0'))) %>%
  mutate(worker1 = fix.anon.names(worker1)) %>%
  mutate(raw.accuracy = (true.pos + true.neg) / (true.pos + true.neg + false.pos + false.neg)) %>%
  mutate(granularity = factor(granularity,
                              levels = c('street', '5_meter'),
                              labels = c('street', '5_meter'))) %>%
  filter(label.type != 'NoSidewalk' | (granularity == 'street' & binary == TRUE))
```


### Ground truth label counts

```{r turk.high.level.results, echo=FALSE, include=FALSE}
unique.turkers <-
  data.with.raw.accuracy %>%
  filter(worker.type == 'turk5',
         binary == FALSE,
         included.severity == 'all',
         granularity == 'street',
         label.type == 'CurbRamp') %>%
  select(worker1, worker2, worker3, worker4, worker5) %>%
  gather(worker.index, turker.id) %>%
  select(turker.id)

n.turkers <- nrow(unique.turkers)
n.reg <- n_distinct(data.with.raw.accuracy %>% filter(!is.turker, !is.anon.route) %>% select(worker1))
n.anon <- n_distinct(data.with.raw.accuracy %>% filter(!is.turker, is.anon.route) %>% select(worker1))

aggregate.accuracy.data <-
  data.with.raw.accuracy %>%
  filter(n.workers == 1,
         binary == TRUE,
         included.severity == 'all',
         label.type %in% c('All', 'Problem')) %>%
  select_at(vars(label.type, granularity, recall, precision, f.measure)) %>%
  group_by(label.type, granularity) %>%
  summarize_at(vars(recall, precision, f.measure), mean, na.rm = TRUE) %>%
  ungroup()

# We filter down to the data we care about, then in the group_by we get down to 2 entries per group;
# one entry for the All type and one entry for the Problem type. We want to combine those into one
# row with columns all.recall, all.precision, prob.recall, prob.precision, etc. So while they are
# grouped, we create each of those new columns. We fill in the all.* columns in correctly in the
# "All" label type rows, putting NaN in the "Problem" rows. Vice versa for the prob.* columns. Then
# to actually combine these two rows we take the sum (where NA is treated as 0 in the sum). But b/c
# NA is treated as 0, this doesn't work when the value that was _supposed_ to be there is NA also!
# So we use an if_else to set to NA if both are NA, and take the some o/w.
turk.study.summary.stats <-
  data.with.raw.accuracy %>%
  filter(worker.type != 'turk0',
         binary == TRUE,
         included.severity == 'all',
         label.type %in% c('All', 'Problem')) %>%
  group_by(condition.id, worker1, is.anon.route, worker.type, granularity) %>%
  mutate(all.recall = if_else(label.type == 'All', recall, NaN),
         all.precision = if_else(label.type == 'All', precision, NaN),
         all.f.measure = if_else(label.type == 'All', f.measure, NaN),
         prob.recall = if_else(label.type == 'Problem', recall, NaN),
         prob.precision = if_else(label.type == 'Problem', precision, NaN),
         prob.f.measure = if_else(label.type == 'Problem', f.measure, NaN)) %>%
  summarize_at(vars(all.recall:prob.f.measure),
               function(x) { if_else(length(na.omit(x)) > 0, sum(x, na.rm = TRUE), NaN) }) %>%
  ungroup()

turk.study.summary.stats.with.labels <-
  turk.study.summary.stats %>%
  filter(worker.type %in% c('anon', 'reg', 'turk1')) %>%
  left_join(labels.turk.study, by = c('worker1' = 'user_id', 'condition.id' = 'condition_id')) %>%
  group_by_at(vars(worker1:prob.f.measure)) %>%
  dplyr::summarise(n.labels = n_distinct(label_id, na.rm = TRUE)) %>%
  left_join(times.turk.study, by = c('worker1' = 'user_id')) %>%
  left_join(user.search.times %>% filter(label.type == 'All'), by = c('worker1' = 'user.id')) %>%
  mutate(n.missions = if_else(is.anon.route, 2, 3),
         distance.feet = if_else(is.anon.route, 2000, 4000),
         distance.meters = distance.feet / 3.28084,
         labels.per.100m = 100 * n.labels / distance.meters,
         meters.per.min = distance.meters / minutes_audited,
         hours.audited = minutes_audited / 60)
```

Below is a table showing number of ground truth labels by user group and by label type.

```{r turk.showing.stats.gt, echo=FALSE}
# GT label counts by label type and route.
gt.count.by.route.and.label.type <-
  data.with.raw.accuracy %>%
  filter(worker.type %in% c('anon', 'reg'),
         binary == FALSE,
         included.severity == 'all',
         granularity == 'street') %>%
  group_by(condition.id, label.type) %>%
  summarize(gt.labels = sum(true.pos, false.neg)) %>%
  ungroup()

gt.count.by.label.type <-
  gt.count.by.route.and.label.type %>%
  group_by(label.type) %>%
  summarise_at(vars(gt.labels), sum)

gt.count.data <-
  data.with.raw.accuracy %>%
  filter(worker.type %in% c('anon', 'reg'),
         binary == FALSE,
         included.severity == 'all',
         granularity == 'street',
         label.type != 'AllProb') %>%
  group_by(worker.type, label.type) %>%
  summarize(gt.labels = sum(true.pos, false.neg)) %>%
  ungroup() %>%
  mutate(worker.type = as.character(worker.type))

gt.count.totals <-
  gt.count.data %>%
  group_by(label.type) %>%
  summarize(gt.labels = sum(gt.labels)) %>%
  mutate(worker.type = 'total')

n.gt <- gt.count.totals %>% filter(label.type == 'All') %>% select(gt.labels) %>% as.numeric()

gt.count.percentages <-
  gt.count.totals %>%
  mutate(gt.labels = paste0(format(100 * gt.labels / n.gt, digits = 2), '%'),
         worker.type = '% of total')
  
table.levels <- c('anon', 'reg', 'total', '% of total')
kable(bind_rows(gt.count.data %>% mutate(gt.labels = as.character(gt.labels)),
                gt.count.totals %>% mutate(gt.labels = as.character(gt.labels)),
                gt.count.percentages) %>%
        mutate(worker.type = factor(worker.type, levels = table.levels, labels = table.levels)) %>%
        spread(label.type, gt.labels),
  format = 'markdown',
  digits = 3,
  align = 'l'
  )
```

A total of `r n.turkers` turkers, `r n.reg` registered users, and `r n.anon` anonymous users were part of this study.


### Aggregate accuracy

Below are two tables (street level, then 5 meter level) showing mean accuracy across all users when aggregating over all label types, and for problem vs no problem. We see that the accuracies are comparable at the street level, but accuracy is much higher for curb ramps than problems at the 5 meter level.

NOTE: In these two tables, the data is binary (not ordinal), we are only considering single users auditing (i.e., no multi-user clustering or majority vote), and we only consider the first turker to audit each route.

Mean accuracy across all users - street level:

```{r turk.showing.stats.2, echo=FALSE}
kable(
  aggregate.accuracy.data %>%
    filter(granularity == 'street') %>%
    select(-granularity),
  format = 'markdown',
  digits = 3,
  align = 'l'
  )
```

Mean accuracy across all users - 5 meter level:

```{r turk.showing.stats.3, echo=FALSE}
kable(
  aggregate.accuracy.data %>%
    filter(granularity == '5_meter') %>%
    select(-granularity),
  digits = 3,
  align = 'l'
  )
```


### Voting: Improved recall when at least one turker marks

```{r turk.voting.method.setup, echo=FALSE}
voting.method.data <-
  data.with.raw.accuracy %>%
  filter(label.type %in% c('All', 'Problem'),
         n.workers == 5 | worker.type == 'reg',
         binary == TRUE,
         granularity == 'street',
         label.type == 'Problem',
         included.severity == 'all',
         worker.thresh %in% c(1,3)) %>%
  mutate(voting.method = factor(if_else(worker.thresh == 1,
                                        'at.least.one',
                                        'majority.vote'))) %>%
  group_by(worker.type, voting.method, label.type, granularity) %>%
  dplyr::summarize_at(vars(recall, precision), mean, na.rm = TRUE) %>%
  ungroup()

# Get registered user recall to compare to
reg.recall <-
  voting.method.data %>%
  filter(worker.type == 'reg') %>%
  select(recall) %>%
  as.numeric() %>%
  format(digits = 2)
reg.precision <-
  voting.method.data %>%
  filter(worker.type == 'reg') %>%
  select(precision) %>%
  as.numeric() %>%
  format(digits = 2)
```


Since dealing with false positives is pretty easy (relative to walking through GSV), the most important thing for us is to maximize recall. So how does recall look if we consider a label placed by at least one turker as a potential attribute (i.e., we use the "at least one" voting method)?

For reference, registered users tended to have the best performance among our user groups, and their recall for problem vs no problem was `r reg.recall` and their precision was `r reg.precision`.

NOTE: In this section we are looking at _problem vs no problem_, the data are binary (not ordinal), the data are at the street level (not 5 meter level), and we are looking at 5 clustered turkers with the "at least one" voting method.

*Takeaways*:

* The mean recall improves significantly when going from majority vote to the "at least one" voting method, accompanied by a much smaller decrease in precision. Since recall is much more important to us, this is the voting method we should likely use going forward.

```{r turk.voting.method.analysis, echo=FALSE}
kable(
  voting.method.data %>%
    filter(worker.type != 'reg') %>%
    select(voting.method, recall, precision),
  digits = 3,
  align = 'l'
)
```


### Descriptive stats for users

Next we have some descriptive statistics of users, by user group. These are average (mean/median) stats.

NOTE: In this table, we are only considering single users auditing (i.e., no multi-user clustering or majority vote), and we only consider the first turker to audit each route.

```{r turk.showing.stats.6, echo=FALSE}
kable(
  turk.study.summary.stats.with.labels %>%
    filter(granularity == 'street') %>%  # granularity doesn't affect these stats
    rename(m.p.min = meters.per.min, sec.p.label = seconds.to.label) %>%
    summarise_vars('worker.type',
                   var.names = c('labels.per.100m', 'm.p.min', 'minutes_audited', 'sec.p.label'),
                   include.n = TRUE),
  digits = 3,
  align = 'l'
  )
```


Below, we have a table of aggregate (sum) stats by user group.

NOTE: In this table, we are only considering single users auditing (i.e., no multi-user clustering or majority vote), and we only consider the first turker to audit each route.

```{r turk.showing.stats.7, echo=FALSE}
kable(
  turk.study.summary.stats.with.labels %>%
    filter(granularity == 'street') %>%  # granularity doesn't affect these stats
    mutate(distance.miles = distance.feet / 5280,
           distance.km = distance.feet / 3280.84) %>%
    summarise_vars('worker.type',
                   c('n.missions', 'distance.miles', 'distance.km', 'n.labels', 'hours.audited'),
                   funcs = sum,
                   include.n = TRUE),
  digits = 3,
  align = 'l'
  )
```


### IRR

Our average (mean) IRR over the 7 rounds, by label type, is in the table below:

NOTE: In this table, the data is binary (not ordinal), and is at the street level (not 5 meter level).

```{r turk.irr.stats, echo=FALSE}
kable(
  clean.agreement.data %>%
    group_by(label.type) %>%
    dplyr::summarize(mean.kripp.alpha = mean(kripp.alpha)),
  digits = 3,
  align = 'l'
  )
```



## Possible Stories

### Granularity: Street-level vs 5 meter-level

Below we compare street vs 5 meter level recall and precision by label type.

NOTE: In this section, the data is binary (not ordinal), we are only considering single users auditing (i.e., no multi-user clustering or majority vote), and we only consider the first turker to audit each route.

Below is a table showing label type accuracy at the two granularity levels, followed by a graph that gives a visual representation of the mean and standard error.

```{r turk.granularity.analysis, echo=FALSE, fig.width=7.5, fig.height=4.5}
granularity.analysis.data <-
  data.with.raw.accuracy %>%
  filter(included.severity == 'all',
         binary == TRUE,
         n.workers == 1,
         label.type %in% LABEL_TYPES_TO_ANALYZE) %>%
  gather(accuracy.type, accuracy.value, recall, precision, na.rm = TRUE, factor_key = TRUE)

granularity.analysis.summary <-
  granularity.analysis.data %>%
  group_by(label.type, accuracy.type, granularity) %>%
  dplyr::summarize(sd = sd(accuracy.value, na.rm = TRUE),
                   se = sd(accuracy.value, na.rm = TRUE) / sqrt(n()),
                   mean.accuracy.value = mean(accuracy.value, na.rm = TRUE),
                   median.accuracy.value = median(accuracy.value, na.rm = TRUE),
                   accuracy.value = mean(accuracy.value, na.rm = TRUE),
                   error.bar.min = max(0, accuracy.value - se),
                   error.bar.max = min(1, accuracy.value + se)) %>%
  ungroup()

kable(
  granularity.analysis.summary %>%
    mutate(mean.accuracy = mean.accuracy.value,
           median.accuracy = median.accuracy.value) %>%
    select(accuracy.type, label.type, granularity, mean.accuracy, median.accuracy, sd, se) %>%
    arrange(accuracy.type),
  digits = 3,
  align = 'l'
)

# Make the set of labels that give the number of observations for each sub-graph.
granularity.analysis.labels <-
  granularity.analysis.data %>%
  filter(granularity == 'street', accuracy.type == 'recall') %>%
  group_by(label.type) %>%
  dplyr::summarise(n.users = sum(!is.na(accuracy.value))) %>%
  inner_join(gt.count.by.label.type, by = 'label.type') %>%
  mutate(label = paste0(label.type, '\nN=', n.users, ';L=', gt.labels)) %>%
  pull(label)

ggplot(data = granularity.analysis.summary, aes(x = label.type, y = accuracy.value)) +
  geom_col(aes(group = granularity, fill = granularity), position = position_dodge()) +
  geom_errorbar(aes(ymin = error.bar.min, ymax = error.bar.max, group = granularity),
                width = 0.25, position = position_dodge(0.9)) +
  scale_x_discrete(labels = granularity.analysis.labels) +
  scale_y_continuous(breaks = ACCURACY_BREAKS, limits = c(0, 1.0), expand = c(0.01, 0)) +
  facet_grid(accuracy.type ~ .) +
  ggtitle('Street vs. 5 Meter level Granularity') +
  my.theme.discrete.x
```

*Takeaways*:

* Analyzing at the 5 meter level shows higher raw accuracy and specificity, both because of the large number of true negatives that we get from splitting into 5 meter segments; there are very few street segments with no labels at all.

* Analyzing at the street level shows higher recall, implying that there were relatively fewer false negatives at the street level. This may mean that users aren't finding _every_ issue, but they are more likely to find _at least one_ issue of that type when there are multiple that occur on the same street.

* Analyzing at the street level shows higher precision, implying that there were relatively fewer false positives at the street level. I suspect that this is due to fundamental misunderstandings about how to label (implying both that labeling is complex and difficult and that our onboarding is insufficient) which are persistent/consistent and frequent (think: labeling driveways as curb ramps, labeling storm drains as missing curb ramps, and labeling fire hydrants or street signs that are not in the way as obstacles). In those cases where the mistake is made frequently (multiple times per street), relatively fewer false positives makes sense when moving to street level analysis.

* Analyzing at the street level shows higher f-measure. This clearly comes from the higher recall and precision.

* CurbRamp pretty much outperforms all other label types across the board, regardless of accuracy type of 5 meter vs. street level. This is likely because curb ramps are the easiest label type to understand and find in GSV (both because they are large and easy to see, and because you know where to expect them -- at intersections).

* The SurfaceProblem label type seems to have the highest precision and lowest recall among the different types of issues (I'm excluding CurbRamp here). I guess that, relative to the other types of issues, there are just fewer cases of mistaking something of a surface problem and more cases of not finding a surface problem that was visible in GSV (so maybe surface problems require increased diligence from users, and the other issues require better treatment in onboarding).

* The Problem type seems to perform better than the surface problem and obstacle label types (except for surface problem precision, mentioned in the previous bullet).

* NoCurbRamp seems to have high recall and low precision. This fits my intuition; since users know to expect curb ramps at intersections, if they arrive at an intersection and a curb ramp is not there, they know to place a NoCurbRamp label. However, if there was no sidewalk at all, then we did not add the missing curb ramp labels to the ground truth dataset, and this is not something that we covered during onboarding. I suspect that this, paired with users marking storm drains as missing curb ramps, were the main reasons for the low recall. Both could be addressed through proper training.


### Accuracy by user group

NOTE: In these two tables, the data is binary (not ordinal) and these are mean/median accuracies aggregated across all label types (all.\*) and for the problem vs no problem type (prob.\*).

#### Summary stats

Mean/median/sd accuracy by user group - street level:

```{r turk.showing.stats.4, echo=FALSE}
turk.showing.stats.4.and.5 <-
  turk.study.summary.stats %>%
    group_by(worker.type, granularity) %>%
    dplyr::summarize(
      all.rec.mn = mean(all.recall, na.rm = TRUE),
      all.rec.md = median(all.recall, na.rm = TRUE),
      all.rec.sd = sd(all.recall, na.rm = TRUE),
      all.prec.mn = mean(all.precision, na.rm = TRUE),
      all.prec.md = median(all.precision, na.rm = TRUE),
      all.prec.sd = sd(all.precision, na.rm = TRUE),
      all.f.mn = mean(all.f.measure, na.rm = TRUE),
      all.f.md = median(all.f.measure, na.rm = TRUE),
      all.f.sd = sd(all.f.measure, na.rm = TRUE),
      prob.rec.mn = mean(prob.recall, na.rm = TRUE),
      prob.rec.md = median(prob.recall, na.rm = TRUE),
      prob.rec.sd = sd(prob.recall, na.rm = TRUE),
      prob.prec.mn = mean(prob.precision, na.rm = TRUE),
      prob.prec.md = median(prob.precision, na.rm = TRUE),
      prob.prec.sd = sd(prob.precision, na.rm = TRUE),
      prob.f.mn = mean(prob.f.measure, na.rm = TRUE),
      prob.f.md = median(prob.f.measure, na.rm = TRUE),
      prob.f.sd = sd(prob.f.measure, na.rm = TRUE)
    ) %>%
    rename(user.type = worker.type)

kable(
  turk.showing.stats.4.and.5 %>%
    filter(granularity == 'street') %>%
    select(-granularity),
  digits = 3,
  align = 'l'
  )
```

Mean/median/sd accuracy by user group - 5 meter level:

```{r turk.showing.stats.5, echo=FALSE}
kable(
  turk.showing.stats.4.and.5 %>%
    filter(granularity == '5_meter') %>%
    select(-granularity),
  digits = 3,
  align = 'l'
  )
```


#### Statistical significance

NOTE: This is at the street level (not 5 meter level).

We created binomial mixed effects models to determine the relationship between user group and recall/precision. We had user group as the fixed effect and route id as the random effect. We modeled recall/precision as binomial and used a logistic link function.

```{r turk.user.group.accuracy.glmm, echo=FALSE}
# TODO include the fact that the single turker is included in turk3, etc as a random effect
user.group.test.data <-
  data.with.raw.accuracy %>%
  filter(worker.type != 'turk0',
         n.workers == 1 || (n.workers == 3 && worker.thresh = 2) || (n.workers == 5 && worker.thresh == 3),
         granularity == 'street',
         binary == TRUE,
         included.severity == 'all',
         label.type %in% c('All', 'Problem')) %>%
  mutate(label.type = factor(label.type),
         condition.id = factor(condition.id),
         worker.type = factor(worker.type))

user.group.test.data.recall.all <-
  user.group.test.data %>%
  filter(label.type == 'All') %>%
  select(condition.id, worker.type, true.pos, false.neg, recall) %>%
  na.omit() %>%
  droplevels()
user.group.test.data.prec.all <-
  user.group.test.data %>%
  filter(label.type == 'All') %>%
  select(condition.id, worker.type, true.pos, false.pos, precision) %>%
  na.omit() %>%
  droplevels()
user.group.test.data.recall.prob <-
  user.group.test.data %>%
  filter(label.type == 'Problem') %>%
  select(condition.id, worker.type, true.pos, false.neg, recall) %>%
  na.omit() %>%
  droplevels()
user.group.test.data.prec.prob <-
  user.group.test.data %>%
  filter(label.type == 'Problem') %>%
  select(condition.id, worker.type, true.pos, false.pos, precision) %>%
  na.omit() %>%
  droplevels()

user.group.recall.all.model <- glmer(recall ~ worker.type + (1 | condition.id),
                           data = user.group.test.data.recall.all,
                           family = binomial(link = 'logit'),
                           weights = true.pos + false.neg,
                           control = glmerControl(optimizer = "bobyqa",
                                                  optCtrl = list(maxfun = 100000)))
lrt.user.group.recall.all <- drop1(user.group.recall.all.model, test = 'Chisq')

user.group.prec.all.model <- glmer(precision ~ worker.type + (1 | condition.id),
                         data = user.group.test.data.prec.all,
                         family = binomial(link = 'logit'),
                         weights = true.pos + false.pos,
                         control = glmerControl(optimizer = "bobyqa",
                                                optCtrl = list(maxfun = 100000)))
lrt.user.group.prec.all <- drop1(user.group.prec.all.model, test = 'Chisq')

user.group.recall.prob.model <- glmer(recall ~ worker.type + (1 | condition.id),
                           data = user.group.test.data.recall.prob,
                           family = binomial(link = 'logit'),
                           weights = true.pos + false.neg,
                           control = glmerControl(optimizer = "bobyqa",
                                                  optCtrl = list(maxfun = 100000)))
lrt.user.group.recall.prob <- drop1(user.group.recall.prob.model, test = 'Chisq')

user.group.prec.prob.model <- glmer(precision ~ worker.type + (1 | condition.id),
                         data = user.group.test.data.prec.prob,
                         family = binomial(link = 'logit'),
                         weights = true.pos + false.pos,
                         control = glmerControl(optimizer = "bobyqa",
                                                optCtrl = list(maxfun = 100000)))
lrt.user.group.prec.prob <- drop1(user.group.prec.prob.model, test = 'Chisq')
```

Using likelihood ratio tests (LRTs), we found the contribution of the fixed effect (worker type) to have a statistically significant association with both recall and precision for both the Problem type and all label types aggregated (results shown below).

To test that the orderings of the user groups are statistically significant (e.g., that turk1 recall is significantly lower than registered user recall for the Problem type, etc), we do post-hoc Tukey's HSD tests. This essentially gives us a pairwise test between each user group, which lets us determine what parts of the ordering are significant. The results of which are shown in the tables below.

NOTE: `*` means less than 0.05, `**` means less than 0.01, and `***` means less than 0.001

NOTE: In places where one user group's accuracy was not statistically different from the one with the closest accuracy to it, I also am showing comparisons to user groups with larger differences in accuracy.

Recall, all label types: `r print.lrt.results(lrt.user.group.recall.all, user.group.recall.all.model, 'worker.type')`.

```{r turk.user.group.recall.all.tukey, echo=FALSE}
# Extracts the info we want from the Tukey HSD, and formats it nicely for a table.
tukey.user.group.recall.all <- summary(glht(user.group.recall.all.model, linfct = mcp(worker.type = 'Tukey')),
                             test = adjusted('holm'))
tukey.user.group.recall.all.clean <- format.tukey.table(tukey.user.group.recall.all,
                                                  'recall',
                                                  'worker.type',
                                                  user.group.test.data.recall.all,
                                                  descending = TRUE)
kable(tukey.user.group.recall.all.clean, digits = 3, align = 'l')
```

Precision, all label types: `r print.lrt.results(lrt.user.group.prec.all, user.group.prec.all.model, 'worker.type')`

```{r turk.user.group.prec.all.tukey, echo=FALSE}
tukey.user.group.prec.all <- summary(glht(user.group.prec.all.model, linfct = mcp(worker.type = 'Tukey')),
                           test = adjusted('holm'))
tukey.user.group.prec.all.clean <- format.tukey.table(tukey.user.group.prec.all,
                                            'precision',
                                            'worker.type',
                                            user.group.test.data.prec.all,
                                            descending = TRUE)
kable(tukey.user.group.prec.all.clean, digits = 3, align = 'l')
```

Recall, Problem type: `r print.lrt.results(lrt.user.group.recall.prob, user.group.recall.prob.model, 'worker.type')`

```{r turk.user.group.recall.prob.tukey, echo=FALSE}
tukey.user.group.recall.prob <- summary(glht(user.group.recall.prob.model, linfct = mcp(worker.type = 'Tukey')),
                             test = adjusted('holm'))
tukey.user.group.recall.prob.clean <- format.tukey.table(tukey.user.group.recall.prob,
                                                  'recall',
                                                  'worker.type',
                                                  user.group.test.data.recall.prob,
                                                  descending = TRUE)
kable(tukey.user.group.recall.prob.clean, digits = 3, align = 'l')
```

Precision, Problem type: `r print.lrt.results(lrt.user.group.prec.prob, user.group.prec.prob.model, 'worker.type')`

NOTE: Although anon user have a higher average problem type precision than turk5, the model actually says that turk5 has higher precision (though it is not statistically significant). This is because there is just higher precision across the board on the anon user routes; the mixed effects model takes this into account! More on this below.

```{r turk.user.group.prec.prob.tukey, echo=FALSE}
tukey.user.group.prec.prob <- summary(glht(user.group.prec.prob.model, linfct = mcp(worker.type = 'Tukey')),
                           test = adjusted('holm'))
tukey.user.group.prec.prob.clean <- format.tukey.table(tukey.user.group.prec.prob,
                                            'precision',
                                            'worker.type',
                                            user.group.test.data.prec.prob,
                                            ordering = c('turk5', 'anon', 'turk3', 'turk1', 'reg'),
                                            descending = TRUE)
kable(tukey.user.group.prec.prob.clean, digits = 3, align = 'l')

# Why no significant diff between anon user and others for problem precision..? Because anon users
# seem to have gotten the routes where everyone had more precision:
problem.prec.anon.routes.table <-
  kable(
    user.group.test.data %>%
      filter(label.type == 'Problem', is.anon.route == TRUE) %>%
      group_by(worker.type) %>%
      dplyr::summarise(problem.precision.on.anon.routes = mean(precision, na.rm = TRUE)),
    digits = 3,
    align = 'l'
  )
```

One interesting thing I am seeing is anon users have a much higher average precision for the Problem type than other user groups, but the difference is not statistically significant. It turns out that on routes audited by anonymous users, turkers _also_ had much higher Problem type precision than for registered user routes. This can be seen in the following table:
`r problem.prec.anon.routes.table`


### Accuracy by label type

NOTE: In the two tables below, the data are binary (not ordinal), we are only considering single users auditing (i.e., no multi-user clustering or majority vote), and we only consider the first turker to audit each route.

#### Summary stats

Mean/median/sd accuracy by label type - street level:

```{r turk.showing.stats.by.label.type.1, echo=FALSE}
turk.showing.stats.by.label.type <-
  data.with.raw.accuracy %>%
  filter(binary == TRUE, n.workers == 1, included.severity == 'all', label.type != 'AllProb')

kable(
  turk.showing.stats.by.label.type %>%
    filter(granularity == 'street') %>%
    rename(prec = precision, f = f.measure) %>%
    summarise_vars('label.type', c('recall', 'prec', 'f')),
  digits = 3,
  align = 'l'
  )
```

Mean/median/sd accuracy by label type - 5 meter level:

```{r turk.showing.stats.by.label.type.2, echo=FALSE}
kable(
  turk.showing.stats.by.label.type %>%
    filter(granularity == '5_meter') %>%
    rename(prec = precision, f = f.measure) %>%
    summarise_vars('label.type', c('recall', 'prec', 'f')),
  digits = 3,
  align = 'l'
  )
```


#### Statistical significance

NOTE: This is at the street level (not 5 meter level).

We created binomial mixed effects models to determine the relationship between label type and recall/precision. We had label type as the fixed effect and user id nested in route id as random effects. We modeled recall/precision as binomial and used a logistic link function.

```{r turk.label.type.accuracy.glmm, echo=FALSE}
type.test.data <-
  turk.showing.stats.by.label.type %>%
  filter(granularity == 'street',
         label.type %in% NON_AGGREGATE_TYPES) %>%
  mutate(label.type = factor(label.type),
         user.id = factor(worker1),
         condition.id = factor(condition.id))

type.test.data.recall <-
  type.test.data %>%
  select(condition.id, user.id, label.type, true.pos, false.neg, recall) %>%
  na.omit() %>%
  droplevels()
type.test.data.prec <-
  type.test.data %>%
  select(condition.id, user.id, label.type, true.pos, false.pos, precision) %>%
  na.omit() %>%
  droplevels()

type.recall.model <- glmer(recall ~ label.type + (1 | condition.id / user.id),
                           data = type.test.data.recall,
                           family = binomial(link = 'logit'),
                           weights = true.pos + false.neg,
                           control = glmerControl(optimizer = "bobyqa",
                                                  optCtrl = list(maxfun = 100000)))
lrt.type.recall <- drop1(type.recall.model, test = 'Chisq')
tukey.type.recall <- summary(glht(type.recall.model, linfct = mcp(label.type = 'Tukey')),
                             test = adjusted('holm'))


type.prec.model <- glmer(precision ~ label.type + (1 | condition.id / user.id),
                         data = type.test.data.prec,
                         family = binomial(link = 'logit'),
                         weights = true.pos + false.pos,
                         control = glmerControl(optimizer = "bobyqa",
                                                optCtrl = list(maxfun = 100000)))
lrt.type.prec <- drop1(type.prec.model, test = 'Chisq')
tukey.type.prec <- summary(glht(type.prec.model, linfct = mcp(label.type = 'Tukey')),
                           test = adjusted('holm'))
```

Using likelihood ratio tests (LRTs), we found the contribution of the fixed effect (label type) to have a statistically significant association with recall (`r print.lrt.results(lrt.type.recall, type.recall.model, 'label.type')`). We also found label type to have a statistically significant association with precision (`r print.lrt.results(lrt.type.prec, type.prec.model, 'label.type')`).

To test that the ordering of the label types are statistically significant (e.g., that NoCurbRamp recall is significantly lower than CurbRamp recall, etc), we do post-hoc Tukey's HSD tests. This essentially gives us a pairwise test between each label type, which lets us determine what parts of the ordering are significant. The results of which are shown in a tables below (first recall, then precision).

NOTE: `*` means less than 0.05, `**` means less than 0.01, and `***` means less than 0.001

```{r turk.label.type.accuracy.tukey, echo=FALSE}
# Extracts the info we want from the Tukey HSD, and formats it nicely for a table.
tukey.type.recall.clean <- format.tukey.table(tukey.type.recall,
                                              'recall',
                                              'label.type',
                                              type.test.data.recall,
                                              descending = TRUE)
tukey.type.prec.clean <- format.tukey.table(tukey.type.prec,
                                            'precision',
                                            'label.type',
                                            type.test.data.prec,
                                            descending = TRUE)

kable(tukey.type.recall.clean, digits = 3, align = 'l')
kable(tukey.type.prec.clean, digits = 3, align = 'l')
```


### Visual search time: Time to label by type

NOTE: In this section, the data are binary (not ordinal), and is at the street level (not 5 meter level), we are only considering single users auditing (i.e., no multi-user clustering or majority vote), and we only consider the first turker to audit each route.

NOTE: We believe that a user's "typical" visual search time is more accurately represented by their median visual search time than mean due to the long right tail of the distribution of search times.

Below is a table that shows the average time to place a label by label type along with the average recall and precision. The results match my intuition for the most part: CurbRamp has the shortest labeling time, SurfaceProblem has the longest labeling time, and NoCurbRamp and Obstacle are somewhere in between. However, I do find it a bit surprising that NoCurbRamp took longer to label than Obstacle.

Time to place a label is defined as follows:

* For the first label a user places on a specific panorama, the time that elapsed between stepping into the panorama and placing the label.
* For subsequent labels on the same panorama, the time that elapsed between placing the previous label and placing the current label.

```{r turk.label.difficulty.analysis, echo=FALSE}
accuracy.by.label.type <-
  turk.showing.stats.by.label.type %>%
  filter(granularity == 'street') %>%
  group_by(label.type) %>%
  dplyr::summarize(mean_recall = mean(recall, na.rm = TRUE),
                   mean_precision = mean(precision, na.rm = TRUE)) %>%
  mutate(label.type = as.character(label.type))

distinct.workers <- data.with.raw.accuracy %>% distinct(worker1) %>% pull()

ave.labeling.time.by.type.and.user <-
  user.search.times %>%
  filter(user.id %in% distinct.workers,
         label.type %not-in% c('All', 'Problem')) %>%
  droplevels() %>%
  mutate(log.seconds.to.label = log(seconds.to.label))

ave.labeling.time.by.type <-
  ave.labeling.time.by.type.and.user %>%
  group_by(label.type) %>%
  dplyr::summarise(median.s.to.label = median(seconds.to.label), # across users, by label type
                   mean.sec.to.label = mean(seconds.to.label),
                   sd.s.to.label = sd(seconds.to.label)) %>%
  mutate(label.type = as.character(label.type)) %>%
  inner_join(accuracy.by.label.type, by = 'label.type') %>%
  arrange(mean.sec.to.label)

kable(
  ave.labeling.time.by.type,
  digits = 2,
  align = 'l'
)

n.ramp.users <- sum(ave.labeling.time.by.type.and.user$label.type == 'CurbRamp')
n.surface.prob.users <- sum(ave.labeling.time.by.type.and.user$label.type == 'NoCurbRamp')
```

Now we want to check if label types have a statistically significant difference in visual search time. We also want to see if this ordering is statistically significant. We would normally do an ANOVA followed by Tukey's HSD post-hoc analysis to see if ordering is significant. Since each user's labeling time is recorded _for each label type_, our next idea would be to run a Repeated Measures ANOVA.

However, rANOVA would require us to throw out data for any user who did not place a label of each label type. We have `r n.ramp.users` users with labeling time values for curb ramps, but only `r n.surface.prob.users` users have data for surface problems. This would mean throwing away a large amount of data, giving us less power and possibly biasing the results. Thus, we turn to our next option: linear mixed-effect models.

Another reason for using a linear mixed-effect model is because we want to take into account the differences between users, the differences between routes, and the fact that the user factor is nested in the route factor (each user appears in only one route).

For some background on linear mixed-effect models, the reference I found most helpful was this one: https://stats.idre.ucla.edu/other/mult-pkg/introduction-to-linear-mixed-models/.

_The Model_: To determine the association between visual search time and label type, we use a linear mixed-effects model where our outcome variable is visual search time, we have label type as a fixed effect, and we have user id nested in route id as random effects. We model the random effects as intercepts, meaning that we assume different baseline visual search times for each user and route, but expect the differences in visual search times between label types to be similar across users/routes.

_The Assumptions_: Our two assumptions are that the residuals of the fit are normally distributed (normality) and have constant variance across the range of fitted values (heteroscedasticity). To check for normality, we first use the Shapiro-Wilk test. In this test, the null hypothesis is that the residuals are normally distributed; if we _fail_ to reject the null, then we meet our assumption that the residuals are normally distributed. This test has a high type 1 error rate (often says that data are _not_ normally distributed when they really _are_), but it is good to check the test because it is a quick and easy way to say the data are normal if the test succeeds. If we fail the test, we check a histogram of the residuals to see if they are normally distributed. For heteroscedasticity, we make a scatterplot with the standardized residuals on the y-axis, and fitted visual search times on the x-axis. If the variance in the residuals (y-axis) is constant across the fitted values (x-axis), this constitutes "constant variance", and so we would meet the heteroscedasticity assumption.

```{r turk.visual.search.time.lme.assumption.1, echo=FALSE}
# Very helpful site that brought me from thinking in terms of repeated measures ANOVA to mixed model
# https://m-clark.github.io/docs/mixedModels/anovamixed.html
# Here is what the analysis would have looked like for rANOVA (you can see the similar F-scores):
# summary(aov(seconds.to.label ~ label.type + Error(user.id),
#             data = ave.labeling.time.by.type.and.user))

pre.test.data <- turk.showing.stats.by.label.type %>%
  filter(granularity == 'street') %>%
  filter(label.type %in% NON_AGGREGATE_TYPES) %>%
  mutate(label.type = factor(label.type), # drop unused levels
         condition.id = factor(condition.id)) %>%
  select(condition.id, worker1, recall, precision, label.type) %>%
  inner_join(ave.labeling.time.by.type.and.user,
             by = c('worker1' = 'user.id', 'label.type' = 'label.type')) %>%
  rename(user.id = worker1) %>%
  mutate(user.id = factor(user.id))

labeling.time.test.data <-
  pre.test.data %>%
  select(user.id, condition.id, label.type, seconds.to.label, log.seconds.to.label) %>%
  na.omit() %>%
  droplevels()

# After checking up on these different methods, using ?p.adjust you find the following:
# "There seems no reason to use the unmodified Bonferroni correction because it is dominated by
# Holm's method, which is also valid under arbitrary assumptions."
labeling.time.model <- lme(fixed = seconds.to.label ~ label.type,
                           random = ~ 1 | condition.id / user.id,
                           data = labeling.time.test.data,
                           method = 'ML')

labeling.time.shapiro <- shapiro.test(residuals(labeling.time.model))
```

Using the Shapiro-Wilk test, we reject the null (`r print.p.value(labeling.time.shapiro$p.value)`), meaning we have not proven that the residuals are normally distributed. The histogram on the left of the residuals looks relatively normal, but there are some outliers, and it has a long-ish right tail. More importantly, we really do not seem to meet the heteroscedasticity assumption, given how the variance seems much larger for longer labeling times in the qq plot on the right.

```{r turk.visual.search.time.lme.assumption.2, echo=FALSE, fig.width=3.5, fig.height=3.5}
ggplot(aes(x = residuals.labeling.time.model.),
       data = data.frame(residuals(labeling.time.model))) +
  geom_histogram(binwidth = 2) +
  theme_bw()
plot(labeling.time.model, resid(., type = "p") ~ fitted(.), abline = 0)


log.labeling.time.model <- update(labeling.time.model, fixed = log.seconds.to.label ~ label.type)

log.labeling.time.shapiro <- shapiro.test(residuals(log.labeling.time.model))
```

To try and deal with not meeting the normality or heteroscedasticity assumptions, we perform a log transform on our outcome variable (labeling time). After the transformation, we still fail the Shapiro-Wilk test (`r print.p.value(log.labeling.time.shapiro$p.value)`), but the histogram on the left looks clearly normal, and the residuals on the right seem to have nearly constant variance. Thus, we meet the assumptions necessary to use this model after the transformation.

```{r turk.visual.search.time.lme.test.1, echo=FALSE, fig.width=3.5, fig.height=3.5}
ggplot(aes(x = residuals.log.labeling.time.model.),
       data = data.frame(residuals(log.labeling.time.model))) +
  geom_histogram(binwidth = 0.15) +
  theme_bw()
plot(log.labeling.time.model, resid(., type = "p") ~ fitted(.), abline = 0)


lrt.labeling.time <- drop1(log.labeling.time.model, test = 'Chisq')
```

Using a likelihood ratio test, we find the contribution by the fixed effect (label type) to be statistically significant (`r print.lrt.results(lrt.labeling.time, log.labeling.time.model, 'label.type')`), i.e., the association between label type and labeling time is statistically significant. To test that the ordering of the label types are statistically significant (e.g., that CurbRamp labeling time is significantly lower than Obstacle labeling time, etc), we do a post-hoc Tukey's HSD test. This essentially gives us a pairwise test between each label type, which lets us determine what parts of the ordering are significant. The results of which are shown in a table below.

NOTE: `*` means less than 0.05, `**` means less than 0.01, and `***` means less than 0.001

```{r turk.visual.search.time.lme.test.2, echo=FALSE}
tukey.log.labeling.time <- summary(glht(log.labeling.time.model,
                                        linfct = mcp(label.type = 'Tukey')),
                                   test = adjusted('holm'))

# Extracts the info we want from the Tukey HSD, and formats it nicely for a table.
tukey.log.labeling.time.clean <- format.tukey.table(tukey.log.labeling.time,
                                                    'seconds.to.label',
                                                    'label.type',
                                                    labeling.time.test.data,
                                                    descending = FALSE)

kable(tukey.log.labeling.time.clean, digits = 3, align = 'l')
```


### Zone type: Land use effect on accuracy

#### Definitions

First, we have our definitions of the different zone types with shortened names that we use in parens (these can also be found in doc/zone_type_descriptions.txt):

* Downtown: High-density commercial and residential development
* Mixed-Use: Moderate-density residential and non-residential buildings (e.g., retail, art use)
* Neighborhood Mixed-Use (Nbhd Mixed-Use): Low-density mixed-use development with an emphasis on residential
* Production, distribution, and repair (Industrial): Moderate-density commercial and production, distribution, and repair (with heavy machinery)
* Residential: Predominantly residential with detached houses on medium-to-large lots
* Residential Apartment (Residential Apt): Medium-to-high density residential including apartments and some rowhouses
* Residential Flat: Low-to-moderate density residential rowhouses
* Special Purpose: Includes Fort McNair Naval Facility (and nearby high-density residential), some moderate-density residential, and some undeveloped land.
* Unzoned: Predominantly wooded areas (e.g., large parks, golf courses, and cemeteries) and a military base.

NOTE: For the Special Purpose zone, there are no public roads in the Naval Facility and very few roads in the undeveloped land. Thus, the majority of the _roads_ with the Special Purpose zone tag are in high-density residential areas, particularly apartments.

However, we are going to look at two different groupings of these zone types, because a lot of these zones have very little data. The first grouping is done semantically, and the second is done based on density of development.

* Semantic grouping
    * Commercial = Downtown + Industrial
    * Mixed-Use = Mixed-Use + Neighborhood Mixed-Use
    * Residential Apt = Residential Apt + Special Purpose
    * Residential House = Residential + Residential Flat
    * Unzoned = Unzoned
* Density grouping
    * Low to Moderate Density = Neighborhood Mixed-Use + Residential + Residential Flat + Unzoned
    * Medium to High Density = Downtown + Mixed-Use + Industrial + Residential Apt + Special Purpose

```{r turk.zone.type.setup, echo=FALSE}
all.dc.zoning.data <- data_frame(
  volunteer.type = c('All DC Streets'),
  zone.type = c('Downtown Zone',
                'Mixed-Use Zone',
                'Neighborhood Mixed-Use Zone',
                'Production, Distribution, and Repair Zone',
                'Residential Apartment Zone',
                'Residential Flat Zone',
                'Residential Zone',
                'Special Purpose Zone',
                'Unzoned'),
  percent.of.routes = c(5.74, 11.73, 1.02, 2.47, 14.82, 15.01, 42.89, 1.30, 5.03)
)

recode.semantic <- function(zone.type) {
  fct_recode(zone.type,
             'Commercial'        = 'Downtown Zone',
             'Mixed-Use'         = 'Mixed-Use Zone',
             'Mixed-Use'         = 'Neighborhood Mixed-Use Zone',
             'Commercial'        = 'Production, Distribution, and Repair Zone',
             'Residential Apt'   = 'Residential Apartment Zone',
             'Residential House' = 'Residential Flat Zone',
             'Residential House' = 'Residential Zone',
             'Residential Apt'   = 'Special Purpose Zone',
             'Unzoned'           = 'Unzoned')
}
recode.density <- function(zone.type) {
  fct_recode(zone.type,
             'Medium-High Density'  = 'Downtown Zone',
             'Medium-High Density'  = 'Mixed-Use Zone',
             'Low-Moderate Density' = 'Neighborhood Mixed-Use Zone',
             'Medium-High Density'  = 'Production, Distribution, and Repair Zone',
             'Medium-High Density'  = 'Residential Apartment Zone',
             'Low-Moderate Density' = 'Residential Flat Zone',
             'Low-Moderate Density' = 'Residential Zone',
             'Medium-High Density'  = 'Special Purpose Zone',
             'Low-Moderate Density' = 'Unzoned')
}

# Get predominant zone type for each condition.
get.zone.for.conditions <- function(data, recode.func) {
  data %>%
    select(condition.id, is.anon.route) %>%
    distinct() %>%
    inner_join(zoning.data, by = 'condition.id') %>%
    mutate(zone.type = recode.func(zone_type)) %>%
    group_by(condition.id, is.anon.route) %>%
    count(zone.type) %>%
    slice(which.max(n)) %>%
    ungroup()
}
get.zone.distro <- function(data, recode.func) {
  data %>%
    mutate(volunteer.type = factor(is.anon.route,
                                   levels = c(FALSE, TRUE),
                                   labels = c(paste('Reg User Routes:', n.reg, 'total'),
                                              paste('Anon User Routes:', n.anon, 'total')))) %>%
    group_by(volunteer.type, zone.type) %>%
    dplyr::summarise(n.routes = n()) %>%
    ungroup() %>%
    complete(zone.type, fill = list(n.routes = 0)) %>%
    mutate(percent.of.routes = 100 * n.routes / sum(n.routes)) %>%
    select(volunteer.type, zone.type, percent.of.routes) %>%
    rbind(all.dc.zoning.data %>% mutate(zone.type = recode.func(zone.type))) %>%
    group_by(volunteer.type, zone.type) %>%
    dplyr::summarise(percent.of.routes = sum(percent.of.routes)) %>%
    ungroup()
}

get.gt.count.distro <- function(data, recode.func) {
  data %>%
    mutate(meters = if_else(is.anon.route, 609.6, 1219.2)) %>%
    inner_join(gt.count.by.route.and.label.type %>% filter(label.type %in% NON_AGGREGATE_TYPES),
               by = 'condition.id') %>%
    group_by(label.type, zone.type) %>%
    summarise_at(vars(gt.labels, meters), sum) %>%
    ungroup() %>%
    mutate(gt.labels.per.100m = 100 * gt.labels / meters) %>%
    select(label.type, zone.type, gt.labels, gt.labels.per.100m)
}

plot.gt.dist.for.zone.type <- function(data, n.data, l.data) {
  xlabs <-
    data %>%
    distinct(zone.type) %>%
    mutate(lab = paste0(zone.type, '\nN=', n.data[zone.type], '; L=', l.data[zone.type])) %>%
    pull(lab)

  ggplot(data = data,
         aes(x = zone.type, y = gt.labels.per.100m, fill = label.type)) +
    geom_col(position = 'dodge') +
    my.theme.discrete.x +
    scale_fill_manual(values = c('#66CC66', 'red', 'blue', 'orange')) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
    scale_x_discrete(labels = xlabs) +
    ylab('GT Labels per 100 Meters') +
    scale_y_continuous(expand = c(0.01, 0)) # removes weird white space at bottom of plot
}
```

#### Semantic grouping

As a reminder, the grouping is defined as follows:

* Commercial = Downtown + Industrial
* Mixed-Use = Mixed-Use + Neighborhood Mixed-Use
* Residential Apt = Residential Apt + Special Purpose
* Residential House = Residential + Residential Flat
* Unzoned = Unzoned

##### Distribution

Here is the zone type distribution for the mturk study. We assigned each street in DC a zone type based on the zone in which one of its endpoints is located. We then assigned a zone type to each route as the plurality zone type among the streets on that route. The graph below compares the zone type distributions of the anon user routes, registered user routes, and all of DC for reference.

NOTE: For regsitered and anon user routes below, it is the percentage of _routes_ marked as that zone type. But for All DC Streets, it is the percentage of _streets_ in DC, since we don't have a set of "routes" that makes up DC.

```{r turk.zone.type.distribution.semantic, echo=FALSE, fig.width=4, fig.height=3}
conditions.w.semantic.zone <- get.zone.for.conditions(data.with.raw.accuracy, recode.semantic)
semantic.zone.distro.by.user.group <- get.zone.distro(conditions.w.semantic.zone, recode.semantic)

ggplot(data = semantic.zone.distro.by.user.group,
       aes(x = zone.type, y = percent.of.routes, fill = volunteer.type)) +
  geom_col(position = 'dodge') +
  my.theme.discrete.x +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
  scale_y_continuous(expand = c(0.01, 0)) # removes weird white space at bottom of plot

kable(
  conditions.w.semantic.zone %>%
    inner_join(route.regions, by = c('condition.id' = 'condition_id')) %>%
    group_by(zone.type) %>%
    dplyr::summarise(n.routes = n(),
                     n.neighborhoods = length(unique(region_id))),
  align = 'l'
)
```


Below, we look at the distribution of the label type densities in the ground truth, by zone type. Commercial seems to be the only zone that has a noticeably different distribution of label types. However, we have only two routes for that zone type so we can't draw anything of statistical significance from that.

```{r gt.label.dist.zone.plot.semantic, fig.height=4, fig.width=6, warning=FALSE, message=FALSE, echo=FALSE}
gt.count.semantic.zone <- get.gt.count.distro(conditions.w.semantic.zone, recode.semantic)

n.routes.by.zone.type.semantic <-
  conditions.w.semantic.zone %>%
  group_by(zone.type) %>%
  dplyr::summarise(N = n())
route.count.zone.lookup.semantic <- setNames(n.routes.by.zone.type.semantic$N,
                                             n.routes.by.zone.type.semantic$zone.type)

total.gt.by.zone.semantic <-
  gt.count.semantic.zone %>%
  group_by(zone.type) %>%
  dplyr::summarise(L = sum(gt.labels))
gt.label.count.zone.lookup.semantic <- setNames(total.gt.by.zone.semantic$L,
                                                total.gt.by.zone.semantic$zone.type)

plot.gt.dist.for.zone.type(gt.count.semantic.zone,
                           route.count.zone.lookup.semantic,
                           gt.label.count.zone.lookup.semantic)
```


##### Relationship with accuracy

The first graph shows all label types aggregated, the second shows the problem vs. no problem type.

NOTE: In this section, the data are binary (not ordinal), and is at the street level (not 5 meter level), we are only considering single users auditing (i.e., no multi-user clustering or majority vote), and we only consider the first turker to audit each route.

NOTE: The red dots on the graphs are means.

NOTE: N in the graphs below means number of routes that are predominantly of that zone type. However, there are 3 users who completed each route, so there are actually n * 3 data points.

The only noticeable difference I see right away is particularly high recall and low precision for Unzoned relative to the other zones. However, there were only 2 routes classified as Unzoned, so we don't have enough data to make any judgements on that. The 3 zone types with significant data (Mixed-Use, Residential Apt, and Residential House) all seem to have roughly equal accuracies.

```{r turk.zone.type.analysis.semantic, echo=FALSE, fig.width=4.5, fig.height=4}
zone.type.analysis.data.semantic <-
  data.with.raw.accuracy %>%
  inner_join(conditions.w.semantic.zone, by = 'condition.id') %>%
  filter(binary == TRUE,
         granularity == 'street',
         included.severity == 'all',
         n.workers == 1,
         label.type %in% c('All', 'Problem')) %>%
  gather(accuracy.type, accuracy.value, recall, precision, na.rm = TRUE, factor_key = TRUE)

x.labels.with.n.semantic <-
  n.routes.by.zone.type.semantic %>%
  inner_join(total.gt.by.zone.semantic, by = 'zone.type') %>%
  mutate(lab = paste0(zone.type, '\nN=', N, '; L=', L)) %>%
  pull(lab)

# Create the trellis boxplots.
ggplot(data = zone.type.analysis.data.semantic %>% filter(label.type == 'All'),
       aes(x = zone.type, y = accuracy.value)) +
  geom_boxplot() +
  stat_summary(fun.y = mean, geom = 'point', color = 'red', size = 1) +
  scale_x_discrete(labels = x.labels.with.n.semantic) +
  facet_grid(accuracy.type ~ .) +
  ggtitle('Zone Type Accuracy - All Label Types') +
  my.theme.discrete.x +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
  scale_y_continuous(expand = c(0.01, 0)) # removes weird white space at bottom of plot

ggplot(data = zone.type.analysis.data.semantic %>% filter(label.type == 'Problem'),
       aes(x = zone.type, y = accuracy.value)) +
  geom_boxplot() +
  stat_summary(fun.y = mean, geom = 'point', color = 'red', size = 1) +
  scale_x_discrete(labels = x.labels.with.n.semantic) +
  facet_grid(accuracy.type ~ .) +
  ggtitle('Zone Type Accuracy - Problem v NoProblem') +
  my.theme.discrete.x +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
  scale_y_continuous(expand = c(0.01, 0)) # removes weird white space at bottom of plot
```

#### Density grouping

As a reminder, the grouping is defined as follows:

* Low to Moderate Density = Neighborhood Mixed-Use + Residential + Residential Flat + Unzoned
* Medium to High Density = Downtown + Mixed-Use + Industrial + Residential Apt + Special Purpose

##### Distribution

Here is the zone type distribution for the mturk study. We assigned each street in DC a zone type based on the zone in which one of its endpoints is located. We then assigned a zone type to each route as the plurality zone type among the streets on that route. The graph below compares the zone type distributions of the anon user routes, registered user routes, and all of DC for reference.

NOTE: For regsitered and anon user routes below, it is the percentage of _routes_ marked as that zone type. But for All DC Streets, it is the percentage of _streets_ in DC, since we don't have a set of "routes" that makes up DC.

```{r turk.zone.type.distribution.density, echo=FALSE, fig.width=4, fig.height=3}
conditions.w.density.zone <- get.zone.for.conditions(data.with.raw.accuracy, recode.density)
density.zone.distro.by.user.group <- get.zone.distro(conditions.w.density.zone, recode.density)

ggplot(data = density.zone.distro.by.user.group,
       aes(x = zone.type, y = percent.of.routes, fill = volunteer.type)) +
  geom_col(position = 'dodge') +
  my.theme.discrete.x +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
  scale_y_continuous(expand = c(0.01, 0)) # removes weird white space at bottom of plot

kable(
  conditions.w.density.zone %>%
    inner_join(route.regions, by = c('condition.id' = 'condition_id')) %>%
    group_by(zone.type) %>%
    dplyr::summarise(n.routes = n(),
                     n.neighborhoods = length(unique(region_id))),
  align = 'l'
)

# Setup needed for paragraph below.
n.residential.routes.by.density <-
  conditions.w.density.zone %>%
  inner_join(conditions.w.semantic.zone, by = 'condition.id', suffix = c('.density', '.semantic')) %>%
  filter(zone.type.semantic %in% c('Residential House', 'Residential Apt')) %>%
  count(zone.type.density)
residential.route.count.lookup.density <- setNames(n.residential.routes.by.density$n,
                                                   n.residential.routes.by.density$zone.type.density)
n.routes.by.zone.type.density <-
  conditions.w.density.zone %>%
  group_by(zone.type) %>%
  dplyr::summarise(N = n())
route.count.zone.lookup.density <- setNames(n.routes.by.zone.type.density$N,
                                            n.routes.by.zone.type.density$zone.type)
percentage.residential.lookup <-
  setNames(100 * n.residential.routes.by.density$n / n.routes.by.zone.type.density$N,
           n.routes.by.zone.type.density$zone.type)
```

Since there are many residential routes in each group, the number of residential routes is of interest. Of the `r unname(route.count.zone.lookup.density['Low-Moderate Density'])` low-moderate density routes, `r unname(residential.route.count.lookup.density['Low-Moderate Density'])` were residential (`r format(unname(percentage.residential.lookup['Low-Moderate Density']), digits = 4)`%). And of the `r unname(route.count.zone.lookup.density['Medium-High Density'])` medium-high density routes, `r unname(residential.route.count.lookup.density['Medium-High Density'])` were residential (`r format(unname(percentage.residential.lookup['Medium-High Density']), digits = 4)`%).


Below, we look at the distribution of the label type densities in the ground truth, by zone type. The types of density appear to have similar distributions of label types.

```{r gt.label.dist.zone.plot.density, fig.height=4, fig.width=6, warning=FALSE, message=FALSE, echo=FALSE}
gt.count.density.zone <- get.gt.count.distro(conditions.w.density.zone, recode.density)

total.gt.by.zone.density <-
  gt.count.density.zone %>%
  group_by(zone.type) %>%
  dplyr::summarise(L = sum(gt.labels))
gt.label.count.zone.lookup.density <- setNames(total.gt.by.zone.density$L,
                                                total.gt.by.zone.density$zone.type)

plot.gt.dist.for.zone.type(gt.count.density.zone,
                           route.count.zone.lookup.density,
                           gt.label.count.zone.lookup.density)
```


##### Relationship with accuracy

We first show a table with the mean/median/sd for accuracy in the two zones. This is followed by a pair of graphs where the first graph shows all label types aggregated, the second shows the problem vs. no problem type.

NOTE: In this section, the data are binary (not ordinal), and is at the street level (not 5 meter level), we are only considering single users auditing (i.e., no multi-user clustering or majority vote), and we only consider the first turker to audit each route.

NOTE: The red dots on the graphs are means.

NOTE: N in the graphs below means number of routes that are predominantly of that zone type. However, there are 3 users who completed each route, so there are actually n * 3 data points.

There does not appear to be a significant difference in accuracy between the densities.

```{r turk.zone.type.analysis.density, echo=FALSE, fig.width=4, fig.height=5}
zone.type.analysis.data.density <-
  data.with.raw.accuracy %>%
  inner_join(conditions.w.density.zone, by = 'condition.id') %>%
  filter(binary == TRUE,
         granularity == 'street',
         included.severity == 'all',
         n.workers == 1,
         label.type %in% c('All', 'Problem')) %>%
  gather(accuracy.type, accuracy.value, recall, precision, na.rm = TRUE, factor_key = TRUE)

# Table with mean/median/sd
kable(
  zone.type.analysis.data.density %>%
    group_by(accuracy.type, label.type, zone.type) %>%
    dplyr::summarise(mean.accuracy = mean(accuracy.value),
                     median.accuracy = median(accuracy.value),
                     sd = sd(accuracy.value)),
  digits = 3,
  align = 'l'
)

x.labels.with.n.density <-
  n.routes.by.zone.type.density %>%
  inner_join(total.gt.by.zone.density, by = 'zone.type') %>%
  mutate(lab = paste0(zone.type, '\nN=', N, '; L=', L)) %>%
  pull(lab)

# Create the trellis boxplots.
ggplot(data = zone.type.analysis.data.density %>% filter(label.type == 'All'),
       aes(x = zone.type, y = accuracy.value)) +
  geom_boxplot() +
  stat_summary(fun.y = mean, geom = 'point', color = 'red', size = 1) +
  scale_x_discrete(labels = x.labels.with.n.density) +
  facet_grid(accuracy.type ~ .) +
  ggtitle('Zone Type Accuracy - All Label Types') +
  my.theme.discrete.x +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
  scale_y_continuous(expand = c(0.01, 0)) # removes weird white space at bottom of plot

ggplot(data = zone.type.analysis.data.density %>% filter(label.type == 'Problem'),
       aes(x = zone.type, y = accuracy.value)) +
  geom_boxplot() +
  stat_summary(fun.y = mean, geom = 'point', color = 'red', size = 1) +
  scale_x_discrete(labels = x.labels.with.n.density) +
  facet_grid(accuracy.type ~ .) +
  ggtitle('Zone Type Accuracy - Problem v NoProblem') +
  my.theme.discrete.x +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
  scale_y_continuous(expand = c(0.01, 0)) # removes weird white space at bottom of plot
```



### User behavior: Does auditing speed, etc influence accuracy

Variables being investigated: labeling frequency, auditing speed, and visual search time association with recall and precision. I'm also taking a look at both the All and Problem (vs. NoProblem) label types; we had been planning to only look at the All type, but it was easy enough for me to add both, and we can see if there is anything interesting there.

NOTE: In this section, the data are binary (not ordinal), at the street level granularity (not 5 meter level) we are only considering single users auditing (i.e., no multi-user clustering or majority vote), and we only consider the first turker to audit each route.

```{r turk.user.behavior.setup, echo=FALSE}
# Combines user behavior and accuracy data from various sources. So we start by filtering for the
# accuracy data we are using (single user, binary, etc.), then we do joins to get label counts and
# auditing times, and visual search times.
behavior.test.data.unscaled <-
  data.with.raw.accuracy %>%
  filter(worker.type %in% c('anon', 'reg', 'turk1'),
         binary == TRUE,
         included.severity == 'all',
         granularity == 'street',
         label.type %in% c('All', 'Problem')) %>%
  dplyr::select(condition.id, worker1, label.type, true.pos:recall, is.anon.route) %>%
  droplevels() %>%
  inner_join(labels.turk.study, by = c('worker1' = 'user_id', 'condition.id' = 'condition_id')) %>%
  group_by_at(vars(condition.id:is.anon.route)) %>%
  dplyr::summarise(n.labels = n_distinct(label_id, na.rm = TRUE)) %>%
  ungroup() %>%
  inner_join(times.turk.study, by = c('worker1' = 'user_id')) %>%
  mutate(distance.feet = if_else(is.anon.route, 2000, 4000),  # intermediate value
         distance.meters = distance.feet / 3.28084,           # intermediate value
         labels.per.100m = 100 * n.labels / distance.meters,
         meters.per.min = distance.meters / minutes_audited,
         user.id = worker1) %>%
  inner_join(user.search.times %>% filter(label.type %in% c('All', 'Problem')) %>% droplevels(),
             by = c('user.id', 'label.type')) %>%
  gather(key = accuracy.type, value = accuracy.value, recall, precision, factor_key = TRUE) %>%
  mutate(user.id = factor(worker1),
         route.id = factor(condition.id))

# Scale the variables used in the tests.
behavior.test.data <-
  behavior.test.data.unscaled %>%
  mutate(viz.search.time = scale(seconds.to.label),
         label.freq = scale(labels.per.100m),
         audit.speed = scale(meters.per.min))

# Subset the data for each individual model
behavior.test.data.recall.prob <-
  behavior.test.data %>%
  filter(label.type == 'Problem',
         accuracy.type == 'recall') %>%
  rename(recall = accuracy.value) %>%
  select(condition.id, user.id, viz.search.time:audit.speed, true.pos, false.neg, recall) %>%
  na.omit() %>%
  droplevels()
behavior.test.data.recall.all <-
  behavior.test.data %>%
  filter(label.type == 'All',
         accuracy.type == 'recall') %>%
  rename(recall = accuracy.value) %>%
  select(condition.id, user.id, viz.search.time:audit.speed, true.pos, false.neg, recall) %>%
  na.omit() %>%
  droplevels()
behavior.test.data.prec.prob <-
  behavior.test.data %>%
  filter(label.type == 'Problem',
         accuracy.type == 'precision') %>%
  rename(precision = accuracy.value) %>%
  select(condition.id, user.id, viz.search.time:audit.speed, true.pos, false.pos, precision) %>%
  na.omit() %>%
  droplevels()
behavior.test.data.prec.all <-
  behavior.test.data %>%
  filter(label.type == 'All',
         accuracy.type == 'precision') %>%
  rename(precision = accuracy.value) %>%
  select(condition.id, user.id, viz.search.time:audit.speed, true.pos, false.pos, precision) %>%
  na.omit() %>%
  droplevels()
```

First, let's take a look at the relationships between the variables.

```{r turk.user.behavior.graphs, echo=FALSE, fig.width=4.5, fig.height=4.5}
# Plot relationships between variables.
ggplot(behavior.test.data.unscaled, aes(x = labels.per.100m, y = accuracy.value)) +
  geom_point(na.rm = TRUE) +
  stat_smooth(method = 'loess', na.rm = TRUE) +
  theme_bw() +
  coord_cartesian(ylim = c(0, 1)) +
  xlab('Labeling Frequency (# labels / 100 meters)') +
  facet_grid(accuracy.type ~ label.type)
ggplot(behavior.test.data.unscaled, aes(x = meters.per.min, y = accuracy.value)) +
  geom_point(na.rm = TRUE) +
  stat_smooth(method = 'loess', na.rm = TRUE) +
  theme_bw() +
  coord_cartesian(ylim = c(0, 1)) +
  xlab('Auditing Speed (meters / minute)') +
  facet_grid(accuracy.type ~ label.type)
ggplot(behavior.test.data.unscaled, aes(x = seconds.to.label, y = accuracy.value)) +
  geom_point(na.rm = TRUE) +
  stat_smooth(method = 'loess', na.rm = TRUE) +
  theme_bw() +
  coord_cartesian(ylim = c(0, 1)) +
  xlab('Visual Search Time (seconds to label)') +
  facet_grid(accuracy.type ~ label.type)
```

From these graphs, the potential associations I was seeing (before running the tests) were a possible positive association between labeling frequency and recall (both Problem and All types) and a possible negative association between visual search time and recall (both Problem and All types). In fact, these are the four cases where we find statistically significant results.

To test for the associations between the user behaviors and accuracy, we created 4 binomial mixed effect models (one for accuracy type, precision and recall; and label type, All and Problem). We had the 3 user behaviors as individual fixed effects (labeling frequency, audit speed, and visual search time), which we scaled and centered so that estimates and standard errors between the predictors is easier. We used user id nested in route id as the random effects. We modeled recall and precision as binomial and used the standard logistic link function. We performed likelihood ratio tests (LRTs) to determine the significance of the predictors.

Below is a table showing the summaries of the models and results of the LRTs. The estimate and standard error columns come from the models (along with the association column, which denotes direction of relationship), and the p value and LRT stat come from the likelihood ratio tests.

```{r turk.user.behavior.tests, echo=FALSE}
# Create the different models, along with likelihood ratio tests and data frames with relevant
# stats extracted from the model and LRT.
# Found the following link helpful in looking for solutions to convergence errors:
# https://rstudio-pubs-static.s3.amazonaws.com/33653_57fc7b8e5d484c909b615d8633c01d51.html
behavior.recall.prob.model <-
  glmer(recall ~ label.freq + audit.speed + viz.search.time + (1 | condition.id / user.id),
        data = behavior.test.data.recall.prob,
        family = binomial(link = 'logit'),
        weights = true.pos + false.neg,
        control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 100000)))
lrt.behavior.recall.prob <- drop1(behavior.recall.prob.model, test = 'Chisq')
coeff.recall.prob <-
  data.frame(summary(behavior.recall.prob.model)$coefficients) %>%
  mutate(param = dimnames(summary(behavior.recall.prob.model)$coefficients)[[1]],
         label.type = 'Problem',
         accuracy.type = 'recall',
         LRT = lrt.behavior.recall.prob$LRT,
         df = lrt.behavior.recall.prob$Df,
         n = nobs(behavior.recall.prob.model),
         p = lrt.behavior.recall.prob$`Pr(Chi)`)

behavior.recall.all.model <-
  glmer(recall ~ label.freq + audit.speed + viz.search.time + (1 | condition.id / user.id),
        data = behavior.test.data.recall.all,
        family = binomial(link = 'logit'),
        weights = true.pos + false.neg,
        control = glmerControl(optimizer = "Nelder_Mead", optCtrl = list(maxfun = 100000)))
lrt.behavior.recall.all <- drop1(behavior.recall.all.model, test = 'Chisq')
coeff.recall.all <-
  data.frame(summary(behavior.recall.all.model)$coefficients) %>%
  mutate(param = dimnames(summary(behavior.recall.all.model)$coefficients)[[1]],
         label.type = 'All',
         accuracy.type = 'recall',
         LRT = lrt.behavior.recall.all$LRT,
         df = lrt.behavior.recall.all$Df,
         n = nobs(behavior.recall.all.model),
         p = lrt.behavior.recall.all$`Pr(Chi)`)

behavior.prec.prob.model <-
  glmer(precision ~  label.freq +  audit.speed + viz.search.time + (1 | condition.id / user.id),
        data = behavior.test.data.prec.prob,
        family = binomial(link = 'logit'),
        weights = true.pos + false.pos,
        control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 100000)))
lrt.behavior.prec.prob <- drop1(behavior.prec.prob.model, test = 'Chisq')
coeff.precision.prob <-
  data.frame(summary(behavior.prec.prob.model)$coefficients) %>%
  mutate(param = dimnames(summary(behavior.prec.prob.model)$coefficients)[[1]],
         label.type = 'Problem',
         accuracy.type = 'precision',
         LRT = lrt.behavior.prec.prob$LRT,
         df = lrt.behavior.prec.prob$Df,
         n = nobs(behavior.prec.prob.model),
         p = lrt.behavior.prec.prob$`Pr(Chi)`)

behavior.prec.all.model <-
  glmer(precision ~  label.freq +  audit.speed + viz.search.time + (1 | condition.id / user.id),
        data = behavior.test.data.prec.all,
        family = binomial(link = 'logit'),
        weights = true.pos + false.pos,
        control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 100000)))
lrt.behavior.prec.all <- drop1(behavior.prec.all.model, test = 'Chisq')
coeff.precision.all <-
  data.frame(summary(behavior.prec.all.model)$coefficients) %>%
  mutate(param = dimnames(summary(behavior.prec.all.model)$coefficients)[[1]],
         label.type = 'All',
         accuracy.type = 'precision',
         LRT = lrt.behavior.prec.all$LRT,
         df = lrt.behavior.prec.all$Df,
         n = nobs(behavior.prec.all.model),
         p = lrt.behavior.prec.all$`Pr(Chi)`)

# Create nice looking table with important stats from each of the models/tests above.
all.coeffs <-
  rbind(coeff.recall.all, coeff.recall.prob, coeff.precision.all, coeff.precision.prob) %>%
  filter(param != '(Intercept)') %>%
  mutate(association = case_when(Estimate > 0 & p < 0.05 ~ '+',
                                 Estimate < 0 & p < 0.05 ~ '-',
                                 TRUE                    ~ 'NA'),
         p.value = p.num.to.p.str.with.sig.code(p)) %>%
  rename(estimate = Estimate, std.err = Std..Error) %>%
  select(accuracy.type, label.type, param, association, estimate, std.err, p.value, LRT, df, n)

kable(all.coeffs, digits = 3, align = 'l')
```

The positive association between labeling frequency and recall is expected, as someone who placed more labels probably correctly found more attributes. The negative association between visual search time and recall is less intuitive, and I don't think we have a solid reason for why this might be the case. It may be that users that take longer to place a label are more distracted, so they are more likely to miss things. Or longer visual search time may mean that they are having a harder time panning with the tool (possibly those using a laptop track pad instead of a mouse), and so they may not want to pan as far.


### User group: Reg vs anon vs turk1 vs turk3 vs turk5

TODO: Make some graphs.

*Takeaways*:


```{r turk.user.group.analysis, echo=FALSE, fig.width=8, fig.height=6}
```


### Low severity: Removing low severity effect on recall

NOTE: I did this analysis using both >=3 and >=4, and both produced significant results. The difference between low and high severity is larger for >=3 compared to >=4, and thus the p-value is smaller. However, we could use >=4 if the story is more compelling.

NOTE: In this section, the data are binary (not ordinal), and is at the street level (not 5 meter level), we are only considering single users auditing (i.e., no multi-user clustering or majority vote), and we only consider the first turker to audit each route.

```{r turk.high.severity.setup.1, echo=FALSE}
low.severity.check.data.1 <-
  data.with.raw.accuracy %>%
  filter(included.severity %in% c('all', '<=3', '>=4'),
         label.type == 'Problem',
         worker.type %in% c('reg', 'anon', 'turk1'),
         granularity == 'street') %>%
  droplevels %>%
  mutate(included.severity = factor(included.severity,
                                    labels = c('all', '<=3', '>=4'),
                                    levels = c('all', '<=3', '>=4')))

recall.by.severity.1 <-
  low.severity.check.data.1 %>%
  filter(binary == TRUE) %>%
  group_by(included.severity) %>%
  summarize(n.users = sum(!is.na(recall)),
            mean.recall = mean(recall, na.rm = TRUE),
            median.recall = median(recall, na.rm = TRUE),
            sd.recall = sd(recall, na.rm = TRUE))

gt.counts.by.severity.1 <-
  low.severity.check.data.1 %>%
  filter(worker.type == 'turk1',
         binary == FALSE) %>%
  group_by(included.severity) %>%
  summarize(gt.problem.labels = sum(true.pos, false.neg))

severity.test.data.1 <-
  low.severity.check.data.1 %>%
  filter(binary == TRUE, included.severity != 'all') %>%
  mutate(user.id = factor(worker1),
         condition.id = factor(condition.id)) %>%
  select(user.id, condition.id, included.severity, true.pos, false.neg, recall) %>%
  na.omit() %>%
  droplevels()

severity.model.1 <- glmer(recall ~ included.severity + (1 | condition.id / user.id),
                        data = severity.test.data.1,
                        family = binomial(link = 'logit'),
                        weights = true.pos + false.neg,
                        control = glmerControl(optimizer = "bobyqa",
                                               optCtrl = list(maxfun = 100000)))

lrt.severity.1 <- drop1(severity.model.1, test = 'Chisq')
```

Below is a table showing the average recall across all users for labels that had severity <=3 (in the ground truth) and labels that had severity >=4, along with the number of labels that fall into each of those categories.

We also created a binomial mixed effects model to determine the relationship between severity and recall. We had severity (high or low) as the fixed effect and user id nested in route id as random effects. We modeled recall as binomial and used a logistic link function. Using a likelihood ratio test (LRT), we found the contribution of the fixed effect (severity) to be statistically significant (`r print.lrt.results(lrt.severity.1, severity.model.1, 'included.severity')`).

```{r turk.high.severity.analysis.1, echo=FALSE, fig.width=8, fig.height=6}
kable(
  inner_join(gt.counts.by.severity.1, recall.by.severity.1, by = 'included.severity'),
  format = 'markdown',
  digits = 3,
  align = 'l'
  )
```


```{r turk.high.severity.setup.2, echo=FALSE}
low.severity.check.data.2 <-
  data.with.raw.accuracy %>%
  filter(included.severity %in% c('all', '<=2', '>=3'),
         label.type == 'Problem',
         worker.type %in% c('reg', 'anon', 'turk1'),
         granularity == 'street') %>%
  droplevels %>%
  mutate(included.severity = factor(included.severity,
                                    labels = c('all', '<=2', '>=3'),
                                    levels = c('all', '<=2', '>=3')))

recall.by.severity.2 <-
  low.severity.check.data.2 %>%
  filter(binary == TRUE) %>%
  group_by(included.severity) %>%
  summarize(n.users = sum(!is.na(recall)),
            mean.recall = mean(recall, na.rm = TRUE),
            median.recall = median(recall, na.rm = TRUE),
            sd.recall = sd(recall, na.rm = TRUE))

gt.counts.by.severity.2 <-
  low.severity.check.data.2 %>%
  filter(worker.type == 'turk1',
         binary == FALSE) %>%
  group_by(included.severity) %>%
  summarize(gt.problem.labels = sum(true.pos, false.neg))

severity.test.data.2 <-
  low.severity.check.data.2 %>%
  filter(binary == TRUE, included.severity != 'all') %>%
  mutate(user.id = factor(worker1),
         condition.id = factor(condition.id)) %>%
  select(user.id, condition.id, included.severity, true.pos, false.neg, recall) %>%
  na.omit() %>%
  droplevels()

severity.model.2 <- glmer(recall ~ included.severity + (1 | condition.id / user.id),
                        data = severity.test.data.2,
                        family = binomial(link = 'logit'),
                        weights = true.pos + false.neg,
                        control = glmerControl(optimizer = "bobyqa",
                                               optCtrl = list(maxfun = 100000)))

lrt.severity.2 <- drop1(severity.model.2, test = 'Chisq')

# Possible diagnostic plots...
# qqnorm(residuals(severity.model))
# ggplot(aes(sample = residuals.severity.model.),
#        data = data.frame(residuals(severity.model))) +
#   geom_qq() +
#   geom_qq_line()
```

Below is a table showing the average recall across all users for labels that had severity <=2 (in the ground truth) and labels that had severity >=3, along with the number of labels that fall into each of those categories.

We also created a binomial mixed effects model to determine the relationship between severity and recall. We had severity (high or low) as the fixed effect and user id nested in route id as random effects. We modeled recall as binomial and used a logistic link function. Using a likelihood ratio test (LRT), we found the contribution of the fixed effect (severity) to be statistically significant ((`r print.lrt.results(lrt.severity.2, severity.model.2, 'included.severity')`).

```{r turk.high.severity.analysis.2, echo=FALSE, fig.width=8, fig.height=6}
kable(
  inner_join(gt.counts.by.severity.2, recall.by.severity.2, by = 'included.severity'),
  format = 'markdown',
  digits = 3,
  align = 'l'
  )
```


### Binary vs ordinal issues per segment

For simplicity, the first graph looks at the 5 meter level, and the second looks at street level. All user groups are also combined (the groups being: registered volunteers, anonymous volunteers, and individual turkers).

NOTE: In this section, the data is at the street level (not 5 meter level), we are only considering single users auditing (i.e., no multi-user clustering or majority vote), and we only consider the first turker to audit each route.

NOTE: The red dots on the graphs are means.

*Takeaways*:

* 5 meter level (first graph): Considering multiple issues per segment results in _very slightly_ lower accuracy for pretty much every type of label and type of accuracy (except precision). I suspect that this comes mostly from our method of clustering, which makes it unlikely that users end up with multiple labels per 5 meter segment. We do not have this restriction in the ground truth, so those few cases where we have more than one label per 5 meter segment in the GT usually results in an additional false negative when moving to ordinal analysis. However, the difference here is very small, so our clustering method seems fine to me.

* Street level (second graph) recall: If we do this analysis at the street level, the decreases in accuracy are more pronounced. At this level, the clustering shouldn't have much effect. The decrease in recall suggests that users are finding _some_ of the problems, but not _all_ of them (meaning an increase in false negatives when we move to ordinal analysis).

* Street level (second graph) recall: I suspect that the reason for the decrease in precision when moving to ordinal analysis at the street level is the same reason as why 5 meter level has lower precision than street level (seen in the previous section). That is, users' misunderstandings of how to label certain common things (driveways as curb ramps, etc.); since these mistakes are common, they may happen many times on a single street edge, which means that you start racking up the false positives when you move to ordinal analysis.


```{r turk.issues.per.seg.analysis, echo=FALSE, fig.width=8, fig.height=4}
issues.per.seg.analysis.data <-
  data.with.raw.accuracy %>%
  filter(included.severity == 'all',
         n.workers == 1,
         label.type %in% LABEL_TYPES_TO_ANALYZE) %>%
  mutate(issues.per.segment = factor(if_else(binary, 'binary', 'ordinal'))) %>%
  gather(accuracy.type, accuracy.value, recall, precision, na.rm = TRUE, factor_key = TRUE)

# Make the set of labels that give the number of observations for each sub-graph.
issues.per.seg.analysis.labels <-
  issues.per.seg.analysis.data %>%
  group_by(label.type, accuracy.type, granularity, issues.per.segment) %>%
  dplyr::summarize(n = sum(!is.na(accuracy.value))) %>%
  dplyr::summarize(n_label = paste0('n = (',
                                    first(n[issues.per.segment == 'binary']), ',',
                                    first(n[issues.per.segment == 'ordinal']),
                                    ')')) %>%
  ungroup()

# Create the trellis boxplots.
ggplot(data = issues.per.seg.analysis.data %>% filter(granularity == '5_meter'),
       aes(x = issues.per.segment, y = accuracy.value)) +
  geom_boxplot() +
  stat_summary(fun.y = mean, geom = 'point', color = 'red', size = 1) +
  scale_y_continuous(breaks = ACCURACY_BREAKS, limits = c(0, 1.15)) +
  geom_hline(aes(yintercept = 1.0), linetype = 'dotted') +
  geom_text(data = issues.per.seg.analysis.labels %>% filter(granularity == '5_meter'),
             mapping = aes(x = -Inf, y = 1.13, hjust = 'inward', label = n_label), size = 3.5) +
  facet_grid(accuracy.type ~ label.type) +
  ggtitle('5 Meter-Level Analysis of Issues per Segment') +
  my.theme.discrete.x

ggplot(data = issues.per.seg.analysis.data %>% filter(granularity == 'street'),
       aes(x = issues.per.segment, y = accuracy.value)) +
  geom_boxplot() +
  stat_summary(fun.y = mean, geom = 'point', color = 'red', size = 1) +
  scale_y_continuous(breaks = ACCURACY_BREAKS, limits = c(0, 1.15)) +
  geom_hline(aes(yintercept = 1.0), linetype = 'dotted') +
  geom_text(data = issues.per.seg.analysis.labels %>% filter(granularity == 'street'),
             mapping = aes(x = -Inf, y = 1.13, hjust = 'inward', label = n_label), size = 3.5) +
  facet_grid(accuracy.type ~ label.type) +
  ggtitle('Street-Level Analysis of Issues per Segment') +
  my.theme.discrete.x
```
