---
title: "Statistics for Paper"
author: "Mikey Saugstad"
date: "April 17, 2018"
output:
  github_document:
    toc: yes
  html_document:
    toc: yes
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(knitr.kable.NA = '-')

library(knitr)
library(conflicted)
library(RPostgreSQL)
library(multcomp) # experimenting with stuff (glht), might remove later
library(nlme)
library(lme4)
library(ggplot2) # may need to use devtools::install_github('tidyverse/ggplot2') if version < 2.2.2
library(tidyr)
library(dplyr)
library(forcats)
library(janitor)
library(Kendall)
library(stringr)

# Run the following in console when you want to generate github flavored markdown as well.
# library(rmarkdown)
# render('R/stats_for_paper.Rmd', c('html_document', 'github_document'))

# If true, queries local postges database for data (could take a very long time), then saves in
# CSVs, so you should set to FALSE after running with the actual queries once.
REFRESH_PUBLIC_DEPLOYMENT_DATA = FALSE
REFRESH_TURK_STUDY_DATA = FALSE

LABEL_TYPE_MAPPING <- c(
  '1' = 'CurbRamp',
  '2' = 'NoCurbRamp',
  '3' = 'Obstacle',
  '4' = 'SurfaceProblem',
  '5' = 'Other',
  '6' = 'Occlusion',
  '7' = 'NoSidewalk'
)
TOTAL_STREET_DIST_METERS <- 1730179
TOTAL_STREET_DIST_MILES <- TOTAL_STREET_DIST_METERS / 1609.34

FIRST_100_END_TIMESTAMP <- '2017-12-10 07:45:44.674+00'

LABEL_TYPES_TO_ANALYZE = c('All', 'Problem', 'CurbRamp', 'NoCurbRamp', 'Obstacle', 'SurfaceProb')
NON_AGGREGATE_TYPES = c('CurbRamp', 'NoCurbRamp', 'Obstacle', 'SurfaceProb')

# Variables commonly used in plotting
my.theme.discrete.x <-
  theme_bw() +
  theme(panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank())
ACCURACY_BREAKS <- c(0, 0.25, 0.5, 0.75, 1.0)

# Helpful functions
'%not-in%' <- function(x,y)!('%in%'(x,y))
se <- function(x, na.rm = TRUE) { sqrt(var(x, na.rm = TRUE) / length(na.omit(x))) }
print.p.value <- function(p) {
  if_else(p < 0.001, 'p < 0.001', paste0('p = ', base::format(round(p, digits = 3),
                                                              scientific = FALSE)))
}
p.num.to.p.str.with.sig.code <- function(p.value) {
  sig.code <- case_when(
    p.value < 0.001 ~ '***',
    p.value < 0.01  ~ '**',
    p.value < 0.05  ~ '*',
    TRUE            ~ '')
  formatted.p.value <- if_else(
    p.value < 0.001,
    '< 0.001',
    base::format(round(p.value, digits = 3), scientific = FALSE)
  )
  paste(formatted.p.value, sig.code)
}
extract_label_type <- Vectorize(
  function(note.str) {
    str_extract_all(note.str, '[:alpha:]+')[[1]][2]
  }
)
extract_label_type_radio_change <- Vectorize(
  function(note.str, action.str) {
    if (action.str == 'ContextMenu_RadioChange' | action.str == 'LabelingCanvas_FinishLabeling')
      str_extract_all(note.str, '[:alpha:]+')[[1]][2]
    else
      NA_character_
  }
)
extract_severity_radio_change <- Vectorize(
  function(note.str, action.str) {
    if (action.str == 'ContextMenu_RadioChange')
      str_extract_all(note.str, '[:alnum:]+')[[1]][4]
    else
      NA_character_
  }
)
print.lrt.results <- function(test, model, fixed.effect) {
  if ('Pr(Chi)' %in% names(test)) p.col <- 'Pr(Chi)' else p.col <- 'Pr(>Chi)'
  paste0('likelihood ratio = ', base::format(test[fixed.effect,'LRT'], digits = 5),
         ', df = ', test[fixed.effect,'Df'],
         ', n = ', nobs(model),
         ', ', print.p.value(test[fixed.effect, p.col]))
}
format.tukey.table <- function(tukey, outcome.var, fixed.var, test.data, ordering = NULL,
                               descending = FALSE) {
  ordering.sign <- if_else(descending == TRUE, '<', '>')

  outcome.by.type <-
    test.data %>%
    mutate(!!fixed.var := as.character(!!sym(fixed.var))) %>%
    group_by(!!sym(fixed.var)) %>%
    dplyr::summarize_at(dplyr::vars(outcome.var), mean)

  if (is.null(ordering)) {
    ordering <-
      if (descending == TRUE) {
        outcome.by.type %>% arrange(desc(!!sym(outcome.var))) %>% pull(fixed.var)
      } else {
        outcome.by.type %>% arrange(!!sym(outcome.var)) %>% pull(fixed.var)
      }
  }

  data.frame(comparison = names(tukey$test$pvalues),
             p = unname(tukey$test$pvalues),
             z.value = unname(tukey$test$tstat)) %>%
    mutate(comparison = if_else((descending == TRUE & z.value > 0)
                                | (descending == FALSE & z.value < 0),
                                gsub("([[:alnum:]|\\.]+) - ([[:alnum:]|\\.]+)",
                                     "\\2 - \\1",
                                     comparison),
                                as.character(comparison)),
           first.type = gsub("([[:alnum:]|\\.]+) - ([[:alnum:]|\\.]+)", "\\1", comparison),
           second.type = gsub("([[:alnum:]|\\.]+) - ([[:alnum:]|\\.]+)", "\\2", comparison),
           first.index = match(first.type, ordering),
           second.index = match(second.type, ordering),
           test = paste(ordering.sign, second.type),
           p.value = p.num.to.p.str.with.sig.code(p),
           z.value = base::format(abs(z.value), digits = 4, scientific = FALSE)) %>%
    right_join(outcome.by.type, by = c('first.type' = fixed.var)) %>%
    mutate(!!fixed.var := factor(first.type, levels = ordering)) %>%
    arrange(!!sym(fixed.var), desc(second.index)) %>%
    dplyr::select(fixed.var, test, p.value, z.value, outcome.var)
}
summarise_vars <- function(data, group, var.names = NULL, col.start = NULL, col.end = NULL,
                           funcs = funs(md = median, mn = mean, sd = sd), only.numeric = TRUE,
                           include.n = FALSE) {

  if (!is.null(col.start) & !is.null(col.end) & is.null(var.names)) {
    var.names <- names(data)[match(col.start, names(data)):match(col.end, names(data))]
  }

  if (is.null(var.names) & only.numeric) {
    summarised <- data %>% group_by(!!sym(group)) %>% summarise_if(is.numeric, funcs, na.rm = TRUE)
  } else if (is.null(var.names)) {
    summarised <- data %>% group_by(!!sym(group)) %>% summarise_all(funcs, na.rm = TRUE)
  } else {
    summarised <- data %>% group_by(!!sym(group)) %>% summarise_at(dplyr::vars(var.names), funcs, na.rm = TRUE)
  }

  if (include.n) {
    data %>%
      group_by(!!sym(group)) %>%
      dplyr::summarise(n.users = n()) %>%
      inner_join(summarised, by = group) %>%
      dplyr::select(group,
                    n.users,
                    2 + order(colnames(.)[colnames(.) %not-in% c(group, 'n.users')]))
  } else {
    summarised %>%
      dplyr::select(group, 1 + order(colnames(.)[colnames(.) != group]))
  }
}
```

```{r connect, echo=FALSE}
if (REFRESH_PUBLIC_DEPLOYMENT_DATA | REFRESH_TURK_STUDY_DATA) drv <- dbDriver("PostgreSQL")
if (REFRESH_PUBLIC_DEPLOYMENT_DATA) {
  db.connection.public.deployment <- dbConnect(drv, dbname = "sidewalk",
                   host = "localhost", port = 5432,
                   user = "sidewalk", password = 'sidewalk')
}
if (REFRESH_TURK_STUDY_DATA) {
  db.connection.turk.study <- dbConnect(drv, dbname = "sidewalkturk",
                   host = "localhost", port = 5432,
                   user = "sidewalk", password = 'sidewalk')
  db.connection.turk.study.production <- dbConnect(drv, dbname = "production-gt",
                   host = "localhost", port = 5432,
                   user = "sidewalk", password = 'sidewalk')
}
```


```{r reading.data, echo=FALSE, include=FALSE}
audits.path <- '../data/stats_for_paper_audits.csv'
times.path <- '../data/stats_for_paper_times.csv'
tutorial.times.path <- '../data/stats_for_paper_tutorial_times.csv'
labels.path <- '../data/stats_for_paper_labels.csv'
attributes.path <- '../data/stats_for_paper_attributes.csv'
sessions.path <- '../data/stats_for_paper_sessions.csv'
missions.path <- '../data/stats_for_paper_reg_missions.csv'
tutorials.path <- '../data/stats_for_paper_tutorials.csv'
dropoffs.path <- '../data/stats_for_paper_dropoffs.csv'
if (REFRESH_PUBLIC_DEPLOYMENT_DATA) {
  reg.labels <-
    dbGetQuery(
      db.connection.public.deployment,
      'SELECT label_id, label_type_id, audit_task.audit_task_id, user_role.user_id, role.role,
              gsv_panorama_id IN (SELECT gsv_panorama_id FROM gsv_onboarding_pano) AS tutorial
      FROM role
      INNER JOIN user_role ON role.role_id = user_role.role_id
      INNER JOIN audit_task ON user_role.user_id = audit_task.user_id
      INNER JOIN street_edge ON audit_task.street_edge_id = street_edge.street_edge_id
      INNER JOIN label ON audit_task.audit_task_id = label.audit_task_id
      WHERE label.deleted = FALSE
          AND street_edge.deleted = FALSE
          AND user_role.user_id <> \'97760883-8ef0-4309-9a5e-0c086ef27573\''
      )
  anon.labels <-
    dbGetQuery(
      db.connection.public.deployment,
      'SELECT DISTINCT label_id,
                       label_type_id,
                       audit_task.audit_task_id,
                       ip_address AS user_id,
                       \'Anonymous\' AS role,
                       gsv_panorama_id IN
                       (
                           SELECT gsv_panorama_id FROM gsv_onboarding_pano
                       ) AS tutorial
      FROM audit_task
      INNER JOIN street_edge
          ON audit_task.street_edge_id = street_edge.street_edge_id
      INNER JOIN label
          ON audit_task.audit_task_id = label.audit_task_id
      INNER JOIN audit_task_environment
          ON audit_task.audit_task_id = audit_task_environment.audit_task_id
      WHERE label.deleted = FALSE
          AND street_edge.deleted = FALSE
          AND user_id = \'97760883-8ef0-4309-9a5e-0c086ef27573\''
      )
  reg.audits <-
    dbGetQuery(
      db.connection.public.deployment,
      'SELECT audit_task.user_id, role.role, audit_task.task_end, audit_task.completed,
              audit_task.audit_task_id, street_edge.street_edge_id,
              ST_LENGTH(ST_TRANSFORM(geom,26918)) * 3.28084 AS feet_audited
      FROM street_edge
      INNER JOIN audit_task
          ON street_edge.street_edge_id = audit_task.street_edge_id
      INNER JOIN user_role
          ON audit_task.user_id = user_role.user_id
      INNER JOIN role
          ON user_role.role_id = role.role_id
      WHERE street_edge.deleted = FALSE
          AND audit_task.user_id <> \'97760883-8ef0-4309-9a5e-0c086ef27573\''
      )
  anon.audits <-
    dbGetQuery(
      db.connection.public.deployment,
      'SELECT DISTINCT ip_address AS user_id, \'Anonymous\' AS role, audit_task.task_end,
              audit_task.completed, audit_task.audit_task_id, street_edge.street_edge_id,
              ST_LENGTH(ST_TRANSFORM(geom,26918)) * 3.28084 AS feet_audited
      FROM street_edge
      INNER JOIN audit_task
          ON street_edge.street_edge_id = audit_task.street_edge_id
      INNER JOIN audit_task_environment
          ON audit_task.audit_task_id = audit_task_environment.audit_task_id
      WHERE street_edge.deleted = FALSE
          AND user_id = \'97760883-8ef0-4309-9a5e-0c086ef27573\''
      )
  reg.times <-
    dbGetQuery(
      db.connection.public.deployment,
      'SELECT user_audit_times.user_id,
             CAST(extract( second from SUM(diff) ) / 60 +
                  extract( minute from SUM(diff) ) +
                  extract( hour from SUM(diff) ) * 60 AS decimal(10,2)) AS minutes_audited
      FROM
      (
          SELECT audit_task.user_id,
                 (timestamp - LAG(timestamp, 1) OVER(PARTITION BY user_id ORDER BY timestamp)) AS diff
          FROM audit_task_interaction
          INNER JOIN audit_task ON audit_task.audit_task_id = audit_task_interaction.audit_task_id
          WHERE action IN (\'ViewControl_MouseDown\', \'LabelingCanvas_MouseDown\')
              AND audit_task.user_id <> \'97760883-8ef0-4309-9a5e-0c086ef27573\'
      ) user_audit_times
      WHERE diff < \'00:05:00.000\' AND diff > \'00:00:00.000\'
      GROUP BY user_audit_times.user_id;'
      )
  anon.times <-
    dbGetQuery(
      db.connection.public.deployment,
      'SELECT user_audit_times.ip_address AS user_id,
             CAST(extract( second from SUM(diff) ) / 60 +
                  extract( minute from SUM(diff) ) +
                  extract( hour from SUM(diff) ) * 60 AS decimal(10,2)) AS minutes_audited
      FROM
      (
          SELECT  user_id, ip_address,
                  (
                      timestamp - Lag(timestamp, 1)
                      OVER(PARTITION BY ip_address ORDER BY timestamp)
                  ) AS diff
          FROM audit_task_interaction
          INNER JOIN audit_task ON audit_task.audit_task_id = audit_task_interaction.audit_task_id
          INNER JOIN audit_task_environment
              ON audit_task.audit_task_id = audit_task_environment.audit_task_id
          WHERE action IN (\'ViewControl_MouseDown\', \'LabelingCanvas_MouseDown\')
          AND audit_task.user_id = \'97760883-8ef0-4309-9a5e-0c086ef27573\'
          AND ip_address IN
          (
              SELECT ip_address
              FROM audit_task_environment
              INNER JOIN audit_task
                  ON audit_task.audit_task_id = audit_task_environment.audit_task_id
              WHERE completed = true
          )
      ) user_audit_times
      WHERE diff < \'00:05:00.000\' AND diff > \'00:00:00.000\'
      GROUP BY ip_address;'
      )
  reg.tutorial.times <-
    dbGetQuery(
      db.connection.public.deployment,
      'SELECT user_audit_times.user_id,
             CAST(extract( second from SUM(diff) ) / 60 +
                  extract( minute from SUM(diff) ) +
                  extract( hour from SUM(diff) ) * 60 AS decimal(10,2)) AS minutes_audited
      FROM
      (
          SELECT tutorial_task.user_id,
                 (timestamp - LAG(timestamp, 1) OVER(PARTITION BY user_id ORDER BY timestamp)) AS diff
          FROM
          (
              SELECT DISTINCT ON (audit_task.user_id)
                     audit_task.audit_task_id, audit_task.user_id
              FROM audit_task_interaction
              INNER JOIN audit_task ON audit_task_interaction.audit_task_id = audit_task.audit_task_id
              INNER JOIN user_role ON audit_task.user_id = user_role.user_id
              INNER JOIN role ON user_role.role_id = role.role_id
              WHERE action = \'Onboarding_End\'
                  AND audit_task.user_id <> \'97760883-8ef0-4309-9a5e-0c086ef27573\'
                ORDER BY audit_task.user_id, audit_task_interaction.timestamp, audit_task_interaction.audit_task_interaction_id
          ) tutorial_task
          INNER JOIN audit_task_interaction ON tutorial_task.audit_task_id = audit_task_interaction.audit_task_id
          WHERE action IN (\'ViewControl_MouseDown\', \'LabelingCanvas_MouseDown\', \'Onboarding_Transition\')
      ) user_audit_times
      WHERE diff < \'00:05:00.000\' AND diff > \'00:00:00.000\'
      GROUP BY user_audit_times.user_id;'
    )
  anon.tutorial.times <-
    dbGetQuery(
      db.connection.public.deployment,
      'SELECT user_audit_times.ip_address AS user_id,
             CAST(extract( second from SUM(diff) ) / 60 +
                  extract( minute from SUM(diff) ) +
                  extract( hour from SUM(diff) ) * 60 AS decimal(10,2)) AS minutes_audited
      FROM
      (
          SELECT  ip_address,
                  (
                      timestamp - Lag(timestamp, 1)
                      OVER(PARTITION BY ip_address ORDER BY timestamp)
                  ) AS diff
          FROM
          (
              SELECT DISTINCT ON (audit_task_environment.ip_address)
                     audit_task.audit_task_id, audit_task_environment.ip_address
              FROM audit_task_interaction
              INNER JOIN audit_task ON audit_task_interaction.audit_task_id = audit_task.audit_task_id
              INNER JOIN audit_task_environment ON audit_task.audit_task_id = audit_task_environment.audit_task_id
              WHERE action = \'Onboarding_End\'
                  AND audit_task.user_id = \'97760883-8ef0-4309-9a5e-0c086ef27573\'
                  AND ip_address IN
                  (
                      SELECT ip_address
                      FROM audit_task_environment
                      INNER JOIN audit_task
                          ON audit_task.audit_task_id = audit_task_environment.audit_task_id
                      WHERE completed = true
                  )
              ORDER BY audit_task_environment.ip_address, audit_task_interaction.timestamp, audit_task_interaction.audit_task_interaction_id
          ) tutorial_task
          INNER JOIN audit_task_interaction ON tutorial_task.audit_task_id = audit_task_interaction.audit_task_id
          WHERE action IN (\'ViewControl_MouseDown\', \'LabelingCanvas_MouseDown\', \'Onboarding_Transition\')
      ) user_audit_times
      WHERE diff < \'00:05:00.000\' AND diff > \'00:00:00.000\'
      GROUP BY ip_address;'
    )
  reg.sessions <-
    dbGetQuery(
      db.connection.public.deployment,
      'SELECT "user".user_id, 1 + COALESCE(n_sessions_minus_one, 0) AS n_sessions
      FROM
      (
          SELECT user_audit_times.user_id, COUNT(diff) AS n_sessions_minus_one
          FROM
          (
              SELECT  audit_task.user_id,
                      (
                          timestamp - LAG(timestamp, 1)
                          OVER(PARTITION BY user_id ORDER BY timestamp)
                      ) AS diff
              FROM audit_task_interaction
              INNER JOIN audit_task
                  ON audit_task.audit_task_id = audit_task_interaction.audit_task_id
              WHERE action IN (\'ViewControl_MouseDown\', \'LabelingCanvas_MouseDown\')
                  AND audit_task.user_id <> \'97760883-8ef0-4309-9a5e-0c086ef27573\'
          ) user_audit_times
          WHERE diff > \'01:00:00.000\'
          GROUP BY user_audit_times.user_id
      ) sess_counts
      RIGHT JOIN "user" ON sess_counts.user_id = "user".user_id;'
      )
  anon.sessions <-
    dbGetQuery(
      db.connection.public.deployment,
      'SELECT ips.ip_address AS user_id, 1 + COALESCE(n_sessions_minus_one, 0) AS n_sessions
      FROM
      (
          SELECT DISTINCT(ip_address) FROM audit_task_environment
      ) ips
      LEFT JOIN
      (
          SELECT user_audit_times.ip_address, COUNT(diff) AS n_sessions_minus_one
          FROM
          (
              SELECT  user_id,
                      ip_address,
                      (
                          timestamp - Lag(timestamp, 1)
                          OVER(PARTITION BY user_id ORDER BY timestamp)
                      ) AS diff
              FROM audit_task_interaction
              INNER JOIN audit_task
                  ON audit_task.audit_task_id = audit_task_interaction.audit_task_id
              INNER JOIN audit_task_environment
                  ON audit_task.audit_task_id = audit_task_environment.audit_task_id
              WHERE action IN (\'ViewControl_MouseDown\', \'LabelingCanvas_MouseDown\')
                  AND audit_task.user_id = \'97760883-8ef0-4309-9a5e-0c086ef27573\'
                  AND ip_address IN
              (
                  SELECT ip_address
                  FROM audit_task_environment
                  INNER JOIN audit_task
                      ON audit_task.audit_task_id = audit_task_environment.audit_task_id
                  WHERE completed = true
              )
          ) user_audit_times
          WHERE diff > \'01:00:00.000\'
          GROUP BY ip_address
      ) sess_counts
          ON ips.ip_address = sess_counts.ip_address;'
      )
  reg.missions <-
    dbGetQuery(
      db.connection.public.deployment,
      'SELECT "user".user_id, COALESCE(count, 0) AS mission_count
      FROM "user"
      LEFT JOIN
      (
          SELECT "user".user_id, COUNT(mission_user_id)
          FROM "user"
          INNER JOIN mission_user ON "user".user_id = mission_user.user_id
          INNER JOIN mission ON mission_user.mission_id = mission.mission_id
          WHERE mission.label <> \'onboarding\'
          GROUP BY "user".user_id
      ) nonzero_counts
          ON nonzero_counts.user_id = "user".user_id
      WHERE "user".user_id <> \'97760883-8ef0-4309-9a5e-0c086ef27573\''
      )
  attributes <-
    dbGetQuery(
      db.connection.public.deployment,
      'SELECT label_type.label_type, COUNT(*) AS count
      FROM global_attribute
      INNER JOIN label_type ON global_attribute.label_type_id = label_type.label_type_id
      GROUP BY label_type.label_type'
      )
  turk.tutorials <-
    dbGetQuery(
      db.connection.public.deployment,
      'SELECT action, user_id, note, ips.audit_task_id, timestamp, TRUE AS is_turker
      FROM
      (
          SELECT DISTINCT audit_task_interaction.audit_task_id, user_role.user_id
          FROM audit_task_interaction
          INNER JOIN audit_task ON audit_task_interaction.audit_task_id = audit_task.audit_task_id
          INNER JOIN user_role ON audit_task.user_id = user_role.user_id
          INNER JOIN role ON user_role.role_id = role.role_id
          WHERE role.role = \'Turker\'
              AND task_end > \'2017-07-10 00:00:00.000+00\'
              AND action = \'Onboarding_Start\'
      ) ips
      INNER JOIN audit_task_interaction
          ON ips.audit_task_id = audit_task_interaction.audit_task_id
      WHERE action IN
          (
              \'Onboarding_Start\',
              \'Onboarding_Transition\',
              \'ModeSwitch_CurbRamp\',
              \'KeyboardShortcut_Severity_1\',
              \'ModeSwitch_NoCurbRamp\',
              \'KeyboardShortcut_Severity_3\',
              \'ModeSwitch_NoSidewalk\',
              \'LabelingCanvas_FinishLabeling\',
              \'ViewControl_DoubleClick\',
              \'Onboarding_End\',
              \'ContextMenu_RadioChange\'
          )
          AND
          (
              action NOT IN (\'ContextMenu_RadioChange\', \'LabelingCanvas_FinishLabeling\')
              OR (action = \'ContextMenu_RadioChange\' AND note LIKE \'%CurbRamp,RadioValue:1%\')
              OR (action = \'ContextMenu_RadioChange\' AND note LIKE \'%NoCurbRamp,RadioValue:3%\')
              OR (action = \'LabelingCanvas_FinishLabeling\' AND note LIKE \'%NoSidewalk%\')
          );'
    )
  volunteer.tutorials <-
    dbGetQuery(
      db.connection.public.deployment,
      'SELECT action, user_id, note, ips.audit_task_id, timestamp, FALSE AS is_turker
      FROM
      (
          SELECT DISTINCT audit_task_interaction.audit_task_id, ip_address AS user_id
          FROM audit_task_interaction
          INNER JOIN audit_task
              ON audit_task_interaction.audit_task_id = audit_task.audit_task_id
          INNER JOIN user_role
              ON audit_task.user_id = user_role.user_id
          INNER JOIN role
              ON user_role.role_id = role.role_id
          INNER JOIN audit_task_environment
              ON audit_task.audit_task_id = audit_task_environment.audit_task_id
          WHERE role.role = \'User\'
              AND task_end > \'2017-07-10 00:00:00.000+00\'
              AND action = \'Onboarding_Start\'
      ) ips
      INNER JOIN audit_task_interaction
          ON ips.audit_task_id = audit_task_interaction.audit_task_id
      WHERE action IN
          (
              \'Onboarding_Start\',
              \'Onboarding_Transition\',
              \'ModeSwitch_CurbRamp\',
              \'KeyboardShortcut_Severity_1\',
              \'ModeSwitch_NoCurbRamp\',
              \'KeyboardShortcut_Severity_3\',
              \'ModeSwitch_NoSidewalk\',
              \'LabelingCanvas_FinishLabeling\',
              \'ViewControl_DoubleClick\',
              \'Onboarding_End\',
              \'ContextMenu_RadioChange\'
          )
          AND
          (
              action NOT IN (\'ContextMenu_RadioChange\', \'LabelingCanvas_FinishLabeling\')
              OR (action = \'ContextMenu_RadioChange\' AND note LIKE \'%CurbRamp,RadioValue:1%\')
              OR (action = \'ContextMenu_RadioChange\' AND note LIKE \'%NoCurbRamp,RadioValue:3%\')
              OR (action = \'LabelingCanvas_FinishLabeling\' AND note LIKE \'%NoSidewalk%\')
          );'
    )
  turk.dropoffs <-
    dbGetQuery(
      db.connection.public.deployment,
      'SELECT DISTINCT ips1.user_id, action, timestamp, TRUE AS is_turker
      FROM
      (
          SELECT DISTINCT audit_task.audit_task_id, user_role.user_id
          FROM audit_task_interaction
          INNER JOIN audit_task ON audit_task_interaction.audit_task_id = audit_task.audit_task_id
          INNER JOIN user_role ON audit_task.user_id = user_role.user_id
          INNER JOIN role ON user_role.role_id = role.role_id
          WHERE role.role = \'Turker\'
              AND task_end > \'2017-07-10 00:00:00.000+00\'
              AND action = \'Onboarding_End\'
      ) ips1
      INNER JOIN
      (
          SELECT DISTINCT audit_task.audit_task_id, user_role.user_id
          FROM audit_task_interaction
          INNER JOIN audit_task ON audit_task_interaction.audit_task_id = audit_task.audit_task_id
          INNER JOIN user_role ON audit_task.user_id = user_role.user_id
          INNER JOIN role ON user_role.role_id = role.role_id
          WHERE role.role = \'Turker\'
              AND task_end > \'2017-07-10 00:00:00.000+00\'
              AND action = \'ViewControl_DoubleClick\'
      ) ips2
      ON ips1.audit_task_id = ips2.audit_task_id
          AND ips1.user_id = ips2.user_id
      INNER JOIN audit_task
          ON ips2.user_id = audit_task.user_id
      INNER JOIN audit_task_interaction
          ON audit_task.audit_task_id = audit_task_interaction.audit_task_id
      WHERE task_end > \'2017-07-10 00:00:00.000+00\'
          AND action IN (\'Onboarding_End\', \'MissionComplete\', \'PanoId_Changed\');'
    )
  volunteer.dropoffs <-
    dbGetQuery(
      db.connection.public.deployment,
      'SELECT DISTINCT ips1.user_id, action, timestamp, FALSE AS is_turker
      FROM
      (
          SELECT DISTINCT audit_task_interaction.audit_task_id, ip_address AS user_id
          FROM audit_task_interaction
          INNER JOIN audit_task
              ON audit_task_interaction.audit_task_id = audit_task.audit_task_id
          INNER JOIN user_role
              ON audit_task.user_id = user_role.user_id
          INNER JOIN role
              ON user_role.role_id = role.role_id
          INNER JOIN audit_task_environment
              ON audit_task.audit_task_id = audit_task_environment.audit_task_id
          WHERE role.role = \'User\'
              AND task_end > \'2017-07-10 00:00:00.000+00\'
              AND action = \'Onboarding_End\'
      ) ips1
      INNER JOIN
      (
          SELECT DISTINCT audit_task_interaction.audit_task_id, ip_address AS user_id
          FROM audit_task_interaction
          INNER JOIN audit_task
              ON audit_task_interaction.audit_task_id = audit_task.audit_task_id
          INNER JOIN user_role
              ON audit_task.user_id = user_role.user_id
          INNER JOIN role
              ON user_role.role_id = role.role_id
          INNER JOIN audit_task_environment
              ON audit_task.audit_task_id = audit_task_environment.audit_task_id
          WHERE role.role <> \'Turker\'
              AND task_end > \'2017-07-10 00:00:00.000+00\'
              AND action = \'ViewControl_DoubleClick\'
      ) ips2
      ON ips1.audit_task_id = ips2.audit_task_id
          AND ips1.user_id = ips2.user_id
      INNER JOIN audit_task_environment
          ON ips2.user_id = audit_task_environment.ip_address
      INNER JOIN audit_task
          ON audit_task_environment.audit_task_id = audit_task.audit_task_id
      INNER JOIN audit_task_interaction
          ON audit_task.audit_task_id = audit_task_interaction.audit_task_id
      WHERE task_end > \'2017-07-10 00:00:00.000+00\'
          AND action IN (\'Onboarding_End\', \'MissionComplete\', \'PanoId_Changed\');'
    )
  dbDisconnect(db.connection.public.deployment)
  
  # Combine registered and anonymous user results.
  audits <- bind_rows(reg.audits, anon.audits)
  times <- bind_rows(reg.times, anon.times)
  tutorial.times <- bind_rows(reg.tutorial.times, anon.tutorial.times)
  labels <- bind_rows(reg.labels, anon.labels)
  sessions <- bind_rows(reg.sessions, anon.sessions)
  tutorials <- bind_rows(turk.tutorials, volunteer.tutorials)
  dropoffs <- bind_rows(turk.dropoffs, volunteer.dropoffs)

  # Copy the old data files over with timestamp in case we want to look back at them.
  now <- gsub(' ', '_', Sys.time())
  file.copy(audits.path, paste('../data/old/stats_for_paper_audits', now, sep = '_'))
  file.copy(times.path, paste('../data/old/stats_for_paper_times.csv', now, sep = '_'))
  file.copy(tutorial.times.path, paste('../data/old/stats_for_paper_tutorial_times.csv', now, sep = '_'))
  file.copy(labels.path, paste('../data/old/stats_for_paper_labels.csv', now, sep = '_'))
  file.copy(attributes.path, paste('../data/old/stats_for_paper_attributes.csv', now, sep = '_'))
  file.copy(sessions.path, paste('../data/old/stats_for_paper_sessions.csv', now, sep = '_'))
  file.copy(missions.path, paste('../data/old/stats_for_paper_reg_missions.csv', now, sep = '_'))
  file.copy(tutorials.path, paste('../data/old/stats_for_paper_tutorials.csv', now, sep = '_'))
  file.copy(dropoffs.path, paste('../data/old/stats_for_paper_dropoffs.csv', now, sep = '_'))

  # Overwrite old data files with new results of queries.
  write.csv(audits, audits.path, row.names = FALSE)
  write.csv(times, times.path, row.names = FALSE)
  write.csv(tutorial.times, tutorial.times.path, row.names = FALSE)
  write.csv(labels, labels.path, row.names = FALSE)
  write.csv(attributes, attributes.path, row.names = FALSE)
  write.csv(sessions, sessions.path, row.names = FALSE)
  write.csv(reg.missions, missions.path, row.names = FALSE)
  write.csv(tutorials, tutorials.path, row.names = FALSE)
  write.csv(dropoffs, dropoffs.path, row.names = FALSE)
} else {
  audits <- read.csv(audits.path,
                     colClasses = c('character', 'character', 'POSIXct',
                                    'logical', replicate(3, 'numeric')))
  times <- read.csv(times.path, colClasses = c('character', 'numeric'))
  tutorial.times <- read.csv(tutorial.times.path, colClasses = c('character', 'numeric'))
  labels <- read.csv(labels.path,
                     colClasses = c(replicate(3, 'numeric'), 'character', 'character', 'logical'))
  attributes <- read.csv(attributes.path, colClasses = c('character', 'numeric'))
  sessions <- read.csv(sessions.path, colClasses = c('character', 'numeric'))
  reg.missions <- read.csv(missions.path, colClasses = c('character', 'numeric'))
  tutorials <- read.csv(tutorials.path,
                        colClasses = c(replicate(3, 'character'), 'numeric', 'POSIXct', 'logical'))
  dropoffs <- read.csv(dropoffs.path,
                       colClasses = c('character', 'character', 'POSIXct', 'logical'))
}

turk.times.path <- '../data/stats_for_paper_times-turk.csv'
turk.labels.path <- '../data/stats_for_paper_labels-turk.csv'
turk.labeling.times.path <- '../data/stats_for_paper_labeling_times-turk.csv'
route.region.path <- '../data/stats_for_paper_route_region.csv'
if (REFRESH_TURK_STUDY_DATA) {
  reg.labels.turk.study <-
    dbGetQuery(
      db.connection.turk.study.production,
      'SELECT label_id, user_id, label_type_id, audit_task.audit_task_id, amt_condition_id AS condition_id
      FROM amt_condition
      INNER JOIN amt_volunteer_route ON amt_condition.volunteer_id = amt_volunteer_route.volunteer_id
      INNER JOIN route_street ON amt_volunteer_route.route_id = route_street.route_id
      INNER JOIN audit_task ON  route_street.current_street_edge_id = audit_task.street_edge_id
                            AND amt_volunteer_route.volunteer_id = audit_task.user_id
      INNER JOIN label ON audit_task.audit_task_id = label.audit_task_id
      WHERE amt_condition.amt_condition_id NOT IN (71, 104, 123, 124, 138)
          AND label.deleted <> TRUE'
      )
  anon.labels.turk.study <-
    dbGetQuery(
      db.connection.turk.study.production,
      'SELECT DISTINCT label_id, audit_task_environment.ip_address AS user_id, label_type_id,
                       audit_task.audit_task_id, amt_condition_id AS condition_id
      FROM amt_condition
      INNER JOIN amt_volunteer_route
          ON amt_condition.volunteer_id = amt_volunteer_route.volunteer_id
      INNER JOIN route_street ON amt_volunteer_route.route_id = route_street.route_id
      INNER JOIN audit_task
          ON  route_street.current_street_edge_id = audit_task.street_edge_id
      INNER JOIN audit_task_environment
          ON  audit_task.audit_task_id = audit_task_environment.audit_task_id
          AND amt_volunteer_route.ip_address = audit_task_environment.ip_address
      INNER JOIN label ON audit_task.audit_task_id = label.audit_task_id
      WHERE amt_condition.amt_condition_id NOT IN (71, 104, 123, 124, 138)
          AND label.deleted <> TRUE'
      )
  turk.labels.turk.study <-
    dbGetQuery(
      db.connection.turk.study,
      'SELECT label_id, turker_id AS user_id, label_type_id, audit_task.audit_task_id, condition_id
      FROM amt_assignment
      INNER JOIN audit_task ON amt_assignment.amt_assignment_id = audit_task.amt_assignment_id
      INNER JOIN label ON audit_task.audit_task_id = label.audit_task_id
      WHERE amt_assignment.condition_id NOT IN (71, 104, 123, 124, 138)
          AND label.deleted <> TRUE'
      )
  reg.times.turk.study <-
    dbGetQuery(
      db.connection.turk.study.production,
      'SELECT user_audit_times.user_id,
             CAST(extract( second from SUM(diff) ) / 60 +
                  extract( minute from SUM(diff) ) +
                  extract( hour from SUM(diff) ) * 60 AS decimal(10,2)) AS minutes_audited
      FROM
      (
          SELECT audit_task.user_id,
                 (
                     timestamp - LAG(timestamp, 1)
                     OVER(PARTITION BY user_id ORDER BY timestamp)
                 ) AS diff
          FROM amt_condition
          INNER JOIN amt_volunteer_route
              ON amt_condition.volunteer_id = amt_volunteer_route.volunteer_id
          INNER JOIN route_street ON amt_volunteer_route.route_id = route_street.route_id
          INNER JOIN audit_task
              ON route_street.current_street_edge_id = audit_task.street_edge_id
              AND amt_volunteer_route.volunteer_id = audit_task.user_id
          INNER JOIN audit_task_interaction
              ON audit_task.audit_task_id = audit_task_interaction.audit_task_id
          WHERE action IN (\'ViewControl_MouseDown\', \'LabelingCanvas_MouseDown\')
              AND amt_condition_id NOT IN (71, 104, 123, 124, 138)
          ) user_audit_times
      WHERE diff < \'00:05:00.000\' AND diff > \'00:00:00.000\'
      GROUP BY user_audit_times.user_id;'
      )
  anon.times.turk.study <-
    dbGetQuery(
      db.connection.turk.study.production,
      'SELECT user_audit_times.user_id,
             CAST(extract( second from SUM(diff) ) / 60 +
                  extract( minute from SUM(diff) ) +
                  extract( hour from SUM(diff) ) * 60 AS decimal(10,2)) AS minutes_audited
      FROM
      (
          SELECT amt_volunteer_route.ip_address AS user_id,
                 (
                     timestamp - LAG(timestamp, 1)
                     OVER(PARTITION BY amt_volunteer_route.ip_address ORDER BY timestamp)
                 ) AS diff
          FROM amt_condition
          INNER JOIN amt_volunteer_route
              ON amt_condition.volunteer_id = amt_volunteer_route.volunteer_id
          INNER JOIN route_street ON amt_volunteer_route.route_id = route_street.route_id
          INNER JOIN audit_task ON route_street.current_street_edge_id = audit_task.street_edge_id
          INNER JOIN audit_task_environment
              ON  audit_task.audit_task_id = audit_task_environment.audit_task_id
              AND amt_volunteer_route.ip_address = audit_task_environment.ip_address
          INNER JOIN audit_task_interaction
              ON audit_task.audit_task_id = audit_task_interaction.audit_task_id
          WHERE action IN (\'ViewControl_MouseDown\', \'LabelingCanvas_MouseDown\')
              AND amt_condition_id NOT IN (71, 104, 123, 124, 138)
          ) user_audit_times
      WHERE diff < \'00:05:00.000\' AND diff > \'00:00:00.000\'
      GROUP BY user_audit_times.user_id;'
      )
  turk.times.turk.study <-
    dbGetQuery(
      db.connection.turk.study,
      'SELECT user_audit_times.user_id,
             CAST(extract( second from SUM(diff) ) / 60 +
                  extract( minute from SUM(diff) ) +
                  extract( hour from SUM(diff) ) * 60 AS decimal(10,2)) AS minutes_audited
      FROM
      (
          SELECT turker_id AS user_id,
                 (timestamp - LAG(timestamp, 1) OVER(PARTITION BY turker_id ORDER BY timestamp)) AS diff
          FROM amt_assignment
          INNER JOIN audit_task ON amt_assignment.amt_assignment_id = audit_task.amt_assignment_id
          INNER JOIN audit_task_interaction ON audit_task.audit_task_id = audit_task_interaction.audit_task_id
          WHERE action IN (\'ViewControl_MouseDown\', \'LabelingCanvas_MouseDown\')
              AND condition_id NOT IN (71, 104, 123, 124, 138)
          ) user_audit_times
      WHERE diff < \'00:05:00.000\' AND diff > \'00:00:00.000\'
      GROUP BY user_audit_times.user_id;'
      )
  reg.labeling.times.turk.study <-
    dbGetQuery(
      db.connection.turk.study.production,
      'SELECT user_id, note,
              EXTRACT
              (
                  EPOCH FROM
                  (
                      pano_time - LAG(pano_time, 1, \'00:00:00.000\')
                          OVER (PARTITION BY user_id, gsv_panorama_id ORDER BY timestamp)
                  )
              ) AS seconds_to_label
      FROM
      (
          SELECT user_id, gsv_panorama_id, note, action, timestamp,
                 SUM(timediff)
                     OVER (PARTITION BY user_id, gsv_panorama_id ORDER BY timestamp) AS pano_time
          FROM
          (
              SELECT user_id, panos.gsv_panorama_id, timestamp, note, action,
                      timestamp - LAG(timestamp) OVER
                          (
                              PARTITION BY user_id, panos.gsv_panorama_id ORDER BY timestamp
                          ) AS timediff
              FROM
              (
                  SELECT DISTINCT audit_task.user_id,
                                  gsv_panorama_id,
                                  audit_task_environment.audit_task_id
                  FROM amt_condition
                  INNER JOIN amt_volunteer_route
                      ON amt_condition.volunteer_id = amt_volunteer_route.volunteer_id
                  INNER JOIN route_street
                      ON amt_volunteer_route.route_id = route_street.route_id
                  INNER JOIN audit_task
                      ON  route_street.current_street_edge_id = audit_task.street_edge_id
                      AND amt_volunteer_route.volunteer_id = audit_task.user_id
                  INNER JOIN audit_task_interaction
                      ON audit_task.audit_task_id = audit_task_interaction.audit_task_id
                  INNER JOIN audit_task_environment
                      ON audit_task_interaction.audit_task_id = audit_task_environment.audit_task_id
                  WHERE action = \'LabelingCanvas_FinishLabeling\'
                      AND amt_condition_id NOT IN (71, 104, 123, 124, 138)
              ) panos
              INNER JOIN audit_task_interaction
                  ON panos.audit_task_id = audit_task_interaction.audit_task_id
                  AND panos.gsv_panorama_id = audit_task_interaction.gsv_panorama_id
          ) interactions_with_time_diffs
          WHERE timediff < \'00:01:00.000\'
      ) interactions_with_time_since_entering_pano
      WHERE action = \'LabelingCanvas_FinishLabeling\';'
    )
  anon.labeling.times.turk.study <-
    dbGetQuery(
      db.connection.turk.study.production,
      'SELECT user_id, note,
              EXTRACT
              (
                  EPOCH FROM
                  (
                      pano_time - LAG(pano_time, 1, \'00:00:00.000\')
                          OVER (PARTITION BY user_id, gsv_panorama_id ORDER BY timestamp)
                  )
              ) AS seconds_to_label
      FROM
      (
          SELECT user_id, gsv_panorama_id, note, action, timestamp,
                 SUM(timediff)
                     OVER (PARTITION BY user_id, gsv_panorama_id ORDER BY timestamp) AS pano_time
          FROM
          (
              SELECT user_id, panos.gsv_panorama_id, timestamp, note, action,
                     timestamp - LAG(timestamp) OVER
                         (
                             PARTITION BY user_id, panos.gsv_panorama_id ORDER BY timestamp
                         ) AS timediff
              FROM
              (
                  SELECT DISTINCT amt_volunteer_route.ip_address AS user_id,
                                  gsv_panorama_id,
                                  audit_task_environment.audit_task_id
                  FROM amt_condition
                  INNER JOIN amt_volunteer_route
                      ON amt_condition.volunteer_id = amt_volunteer_route.volunteer_id
                  INNER JOIN route_street
                      ON amt_volunteer_route.route_id = route_street.route_id
                  INNER JOIN audit_task
                      ON  route_street.current_street_edge_id = audit_task.street_edge_id
                  INNER JOIN audit_task_environment
                      ON audit_task.audit_task_id = audit_task_environment.audit_task_id
                      AND amt_volunteer_route.ip_address = audit_task_environment.ip_address
                  INNER JOIN audit_task_interaction
                      ON audit_task_environment.audit_task_id = audit_task_interaction.audit_task_id
                  WHERE action = \'LabelingCanvas_FinishLabeling\'
                      AND amt_condition_id NOT IN (71, 104, 123, 124, 138)
              ) panos
              INNER JOIN audit_task_interaction
                  ON panos.audit_task_id = audit_task_interaction.audit_task_id
                  AND panos.gsv_panorama_id = audit_task_interaction.gsv_panorama_id
          ) interactions_with_time_diffs
          WHERE timediff < \'00:01:00.000\'
      ) interactions_with_time_since_entering_pano
      WHERE action = \'LabelingCanvas_FinishLabeling\';'
    )
  turk.labeling.times.turk.study <-
    dbGetQuery(
      db.connection.turk.study,
      'SELECT user_id, note,
              EXTRACT
              (
                  EPOCH FROM
                  (
                      pano_time - LAG(pano_time, 1, \'00:00:00.000\')
                          OVER (PARTITION BY user_id, gsv_panorama_id ORDER BY timestamp)
                  )
              ) AS seconds_to_label
      FROM
      (
          SELECT user_id, gsv_panorama_id, note, action, timestamp,
                 SUM(timediff)
                     OVER (PARTITION BY user_id, gsv_panorama_id ORDER BY timestamp) AS pano_time
          FROM
          (
              SELECT user_id, panos.gsv_panorama_id, timestamp, note, action,
                     timestamp - LAG(timestamp) OVER
                         (
                             PARTITION BY user_id, panos.gsv_panorama_id ORDER BY timestamp
                         ) AS timediff
              FROM
              (
                  SELECT DISTINCT amt_assignment.turker_id AS user_id,
                                  gsv_panorama_id,
                                  audit_task_environment.audit_task_id
                  FROM amt_assignment
                  INNER JOIN
                  (
                      SELECT turker_id, COUNT(DISTINCT(route_id))
                      FROM amt_assignment
                      WHERE completed = TRUE
                      GROUP BY turker_id
                  ) completed_route_counts
                      ON amt_assignment.turker_id = completed_route_counts.turker_id
                  INNER JOIN audit_task
                      ON amt_assignment.amt_assignment_id = audit_task.amt_assignment_id
                  INNER JOIN audit_task_interaction
                      ON audit_task.audit_task_id = audit_task_interaction.audit_task_id
                  INNER JOIN audit_task_environment
                      ON audit_task_interaction.audit_task_id = audit_task_environment.audit_task_id
                  WHERE ((count > 1 AND condition_id >= 122) OR (count > 2 AND condition_id < 122))
                      AND action = \'LabelingCanvas_FinishLabeling\'
                      AND condition_id NOT IN (71, 104, 123, 124, 138)
              ) panos
              INNER JOIN audit_task_interaction
                  ON panos.audit_task_id = audit_task_interaction.audit_task_id
                  AND panos.gsv_panorama_id = audit_task_interaction.gsv_panorama_id
          ) interactions_with_time_diffs
          WHERE timediff < \'00:01:00.000\'
      ) interactions_with_time_since_entering_pano
      WHERE action = \'LabelingCanvas_FinishLabeling\';'
      )
  route.regions <-
    dbGetQuery(
      db.connection.turk.study,
      'SELECT DISTINCT amt_condition_id AS condition_id, region_id
      FROM amt_condition
      INNER JOIN amt_volunteer_route
          ON amt_condition.volunteer_id = amt_volunteer_route.volunteer_id
      INNER JOIN route
          ON amt_volunteer_route.route_id = route.route_id
      WHERE amt_condition_id NOT IN (71, 104, 123, 124, 138)'
    )
  dbDisconnect(db.connection.turk.study.production)
  dbDisconnect(db.connection.turk.study)
  
  # Combine results of registerd volunteers, anon volunteers, and turkers.
  times.turk.study <- bind_rows(reg.times.turk.study, anon.times.turk.study, turk.times.turk.study)
  labels.turk.study <- bind_rows(reg.labels.turk.study, anon.labels.turk.study, turk.labels.turk.study)
  labeling.times.turk.study <- bind_rows(reg.labeling.times.turk.study,
                                         anon.labeling.times.turk.study,
                                         turk.labeling.times.turk.study)

  # Copy the old data files over with timestamp in case we want to look back at them.
  now <- gsub(' ', '_', Sys.time())
  file.copy(turk.times.path, paste('../data/old/stats_for_paper_times-turk.csv', now, sep = '_'))
  file.copy(turk.labels.path, paste('../data/old/stats_for_paper_labels-turk.csv', now, sep = '_'))
  file.copy(turk.labeling.times.path,
            paste('../data/old/stats_for_paper_labeling_times-turk.csv', now, sep = '_'))
  file.copy(route.region.path, paste('../data/old/stats_for_paper_route_region.csv', now, sep = '_'))

  # Overwrite old data files with new results of queries.
  write.csv(times.turk.study, turk.times.path, row.names = FALSE)
  write.csv(labels.turk.study, turk.labels.path, row.names = FALSE)
  write.csv(labeling.times.turk.study, turk.labeling.times.path, row.names = FALSE)
  write.csv(route.regions, route.region.path, row.names = FALSE)
} else {
  times.turk.study <- read.csv(turk.times.path, colClasses = c('character', 'numeric'))
  labels.turk.study <- read.csv(turk.labels.path,
                                colClasses = c('numeric', 'character', replicate(3, 'numeric')))
  labeling.times.turk.study <- read.csv(turk.labeling.times.path,
                                        colClasses = c('character', 'character', 'numeric'))
  route.regions <- read.csv(route.region.path, colClasses = c('numeric', 'numeric'))
}

zoning.classes <- c('numeric', 'numeric', 'numeric', 'factor', 'factor', 'logical')
zoning.data <- read.csv('../data/street_zoning_info.csv', colClasses = zoning.classes) %>%
  rename(condition.id = condition_id)
```

```{r transforming.data, echo=FALSE, warning=FALSE}
# Change the _ column names to . separated.
names(labeling.times.turk.study) <- gsub("\\_", ".", names(labeling.times.turk.study))
names(tutorials) <- gsub("\\_", ".", names(tutorials))
names(dropoffs) <- gsub("\\_", ".", names(dropoffs))

# Compute median labeling time by label type for each user.
individual.search.times <-
  labeling.times.turk.study %>%
  mutate(label.type = factor(extract_label_type(note))) %>%
  mutate(label.type = fct_recode(label.type, 'SurfaceProb' = 'SurfaceProblem')) %>%
  dplyr::filter(label.type %in% LABEL_TYPES_TO_ANALYZE) %>%
  dplyr::select(-note)
individual.search.times.prob <-
  individual.search.times %>%
  dplyr::filter(label.type %in% c('NoCurbRamp', 'Obstacle', 'SurfaceProb')) %>%
  mutate(label.type = factor('Problem'))
individual.search.times.all <-
  individual.search.times %>%
  mutate(label.type = factor('All'))

user.search.times <-
  rbind(individual.search.times.all, individual.search.times.prob, individual.search.times) %>%
  group_by(user.id, label.type) %>%
  summarize(seconds.to.label = median(seconds.to.label)) %>%
  ungroup() %>%
  droplevels()

# Takes a distance in feet and approximates the number of missions the user completed. The
# approximation assumes that the first missions are 500, 500, 1000, 2000, and 1280 feet, and all the
# remaining missions are half a mile (2640 feet).
approximate.missions.completed <- Vectorize(
  function(dist.in.feet) {
    if (dist.in.feet > 7920) 5 + floor((dist.in.feet - 5280) / 2640)
    else if (dist.in.feet > 5280) 5
    else if (dist.in.feet > 4000) 4
    else if (dist.in.feet > 2000) 3
    else if (dist.in.feet > 1000) 2
    else if (dist.in.feet > 500) 1
    else 0
  }
)

# Computes minutes_per_mission as either minutes_audited / missions_completed if at least one
# mission was completed, other wise 500 * minutes_audited / feet_audited (how long the first mission
# would take, given the auditing speed for what they had done)
approx.min.per.mission <- Vectorize(
  function(time.audited, missions.completed, dist.in.feet) {
    if (missions.completed > 0) time.audited / missions.completed
    else 500 * time.audited / dist.in.feet
  }
)

tutorial.labels <- labels %>% dplyr::filter(tutorial == TRUE) %>% dplyr::select(-tutorial)
non.tutorial.labels <- labels %>% dplyr::filter(tutorial == FALSE) %>% dplyr::select(-tutorial)

n.tutorial.labels <- n_distinct(tutorial.labels$label_id)
n.non.tutorial.labels <- n_distinct(non.tutorial.labels$label_id)

pre.100.labels <-
  inner_join(non.tutorial.labels, audits, by = c('user_id', 'role', 'audit_task_id')) %>%
  dplyr::filter(task_end < FIRST_100_END_TIMESTAMP) %>%
  mutate(dataset = 'first_100%') %>%
  dplyr::select(names(non.tutorial.labels), dataset)
label.counts <-
  non.tutorial.labels %>%
  mutate(dataset = 'all') %>%
  bind_rows(pre.100.labels) %>%
  group_by(user_id, role, audit_task_id, dataset) %>%
  dplyr::summarize(label_count = n(),
                   problem_label_count = sum(label_type_id %in% 2:4))

# Selects distinct on audit_task_id, then sums dist audited.
pre.100.audits <-
  audits %>%
  dplyr::filter(task_end < FIRST_100_END_TIMESTAMP) %>%
  mutate(dataset = 'first_100%')
audit.length <-
  audits %>%
  mutate(dataset = 'all') %>%
  bind_rows(pre.100.audits) %>%
  group_by(user_id, role, dataset) %>%
  distinct(audit_task_id, .keep_all = TRUE) %>%
  left_join(label.counts, by  = c('user_id', 'role', 'dataset', 'audit_task_id')) %>%
  replace_na(list(label_count = 0, problem_label_count = 0)) %>%
  dplyr::summarise(label_count = sum(label_count),
                   problem_label_count = sum(problem_label_count),
                   feet_audited = sum(feet_audited[completed == TRUE]),
                   audit_count = length(audit_task_id[completed == TRUE])) %>%
  replace_na(list(feet_audited = 0)) %>%
  ungroup() %>%
  mutate(miles_audited = feet_audited / 5280) %>%
  left_join(sessions %>% mutate(dataset = 'all'), by = c('user_id', 'dataset')) %>%
  mutate(dataset = factor(dataset),
         role = factor(role),
         role = recode(role, User = 'Registered', .default = levels(role))) %>%
  mutate(labels_per_100m = 328.084 * label_count / feet_audited,
         problem_labels_per_100m = 328.084 * problem_label_count / feet_audited)

user.list <-
  audit.length %>%
  dplyr::filter(dataset == 'all', audit_count > 0, label_count > 0) %>%
  pull(user_id)
good.user.list <-
  audit.length %>%
  dplyr::filter(dataset == 'all', audit_count > 0, labels_per_100m >= 3.75) %>%
  pull(user_id)

# Computes approximate missions completed for anon users.
all.mission.counts <-
  audit.length %>%
  dplyr::filter(dataset == 'all',
                role == "Anonymous") %>%
  mutate(mission_count = approximate.missions.completed(feet_audited)) %>%
  dplyr::select(user_id, mission_count) %>%
  bind_rows(reg.missions)

# Computes a few metrics based on distance audited, time auditing, and missions completed.
speeds <-
  audit.length %>%
  dplyr::filter(dataset == 'all',
                user_id %in% user.list) %>%
  left_join(all.mission.counts, by = 'user_id') %>%
  left_join(times, by = 'user_id') %>%
  mutate(meters_audited = feet_audited / 3.28084,
         m_per_min = meters_audited / minutes_audited,
         mins_per_mission = approx.min.per.mission(minutes_audited, mission_count, feet_audited),
         hours_audited = minutes_audited / 60,
         minutes_per_session = minutes_audited / n_sessions,
         total_paid = if_else(role == 'Turker', 0.43 + 4.17 * feet_audited / 5280, 0),
         pay_per_hr = total_paid / hours_audited)

filtered.speeds <-
  speeds %>%
  dplyr::filter(role %in% c('Registered', 'Anonymous', 'Turker'),
                audit_count > 0,
                labels_per_100m >= 3.75) %>%
  mutate(role = factor(role, levels = c('Anonymous', 'Registered', 'Turker')))

filtered.labels <- dplyr::filter(non.tutorial.labels, user_id %in% good.user.list)
```

# Public Deployment

NOTE: Public deployment data includes all data up through March 31st (and part of April 1st). This includes all data through the most recent deployment on mturk. We only consider someone a "user" if they have completed an audit of at least one street and have placed at least one label after the tutorial. Later on, we also filter out users with low labeling frequency.

## High-level results

### Top-line numbers (no filtering)

The following are the label counts (not attribute counts) by user group and label type. There are a total of `r n.tutorial.labels + n.non.tutorial.labels` labels, `r n.tutorial.labels` are tutorial labels and `r n.non.tutorial.labels` are non tutorial labels. We consider only non tutorial labels throughout this document.

```{r public.deployment.how.much.data, echo=FALSE, warning=FALSE}
ordered.label.type.levs <- c('CurbRamp', 'NoCurbRamp', 'Obstacle', 'SurfaceProblem', 'NoSidewalk',
                             'Occlusion', 'Other')
n.users.by.group <-
  audit.length %>%
  dplyr::filter(user_id %in% user.list) %>%
  distinct(user_id, .keep_all = TRUE) %>%
  mutate(role = recode_factor(role,
                              `Anonymous` = 'Anon',
                              `User` = 'Registered',
                              `Turker` = 'Turker',
                              `Administrator` = 'Researcher',
                              `Owner` = 'Researcher')) %>%
  count(role)
n.users.list <- setNames(as.character(n.users.by.group$n),
                         n.users.by.group$role)
kable(
  non.tutorial.labels %>%
    distinct(label_id, .keep_all = TRUE) %>%
    mutate(label_type = LABEL_TYPE_MAPPING[label_type_id]) %>%
    mutate(label_type = factor(label_type, ordered.label.type.levs)) %>%
    mutate(role = recode_factor(
      role,
      `Anonymous` = paste0('Anon ', '(', n.users.list['Anon'], ')'),
      `User` = paste0('Registered ', '(', n.users.list['Registered'], ')'),
      `Turker` = paste0('Turker ', '(', n.users.list['Turker'], ')'),
      `Researcher` = paste0('Researcher ', '(', n.users.list['Researcher'], ')'),
      `Administrator` = paste0('Researcher ', '(', n.users.list['Researcher'], ')'),
      `Owner` = paste0('Researcher ', '(', n.users.list['Researcher'], ')'))) %>%
    dplyr::select(label_type, role) %>%
    tabyl(label_type, role) %>%
    adorn_totals(where = c('row', 'col')) %>%
    adorn_percentages(denominator = 'all') %>%
    adorn_pct_formatting() %>%
    adorn_ns(position = 'front'),
  align = 'l'
)
```

### Top-line numbers (with filtering)

```{r public.deployment.how.much.data.filtered, echo=FALSE, warning=FALSE}
ordered.label.type.levs <- c('CurbRamp', 'NoCurbRamp', 'Obstacle', 'SurfaceProblem', 'NoSidewalk',
                             'Occlusion', 'Other')
n.users.by.group.filtered <-
  audit.length %>%
  dplyr::filter(user_id %in% good.user.list) %>%
  distinct(user_id, .keep_all = TRUE) %>%
  mutate(role = recode_factor(role,
                              `Anonymous` = 'Anon',
                              `User` = 'Registered',
                              `Turker` = 'Turker',
                              `Administrator` = 'Researcher',
                              `Owner` = 'Researcher')) %>%
  count(role)
n.users.list.filtered <- setNames(as.character(n.users.by.group.filtered$n),
                                  n.users.by.group.filtered$role)
kable(
  filtered.labels %>%
    distinct(label_id, .keep_all = TRUE) %>%
    mutate(label_type = LABEL_TYPE_MAPPING[label_type_id]) %>%
    mutate(label_type = factor(label_type, ordered.label.type.levs)) %>%
    mutate(role = recode_factor(
      role,
      `Anonymous` = paste0('Anon ', '(', n.users.list.filtered['Anon'], ')'),
      `User` = paste0('Registered ', '(', n.users.list.filtered['Registered'], ')'),
      `Turker` = paste0('Turker ', '(', n.users.list.filtered['Turker'], ')'),
      `Researcher` = paste0('Researcher ', '(', n.users.list.filtered['Researcher'], ')'),
      `Administrator` = paste0('Researcher ', '(', n.users.list.filtered['Researcher'], ')'),
      `Owner` = paste0('Researcher ', '(', n.users.list.filtered['Researcher'], ')'))) %>%
    dplyr::select(label_type, role) %>%
    tabyl(label_type, role) %>%
    adorn_totals(where = c('row', 'col')) %>%
    adorn_percentages(denominator = 'all') %>%
    adorn_pct_formatting() %>%
    adorn_ns(position = 'front'),
  align = 'l'
)
```

### Attribute counts by type

Here are the counts of attributes by attribute type after single and multi user clustering.

NOTE: Clustering only uses the filtered data! Only those who meet the labeling frequency threshold.

```{r public.deployment.attributes, echo=FALSE, warning=FALSE}
ordered.attribute.type.levs <- c('CurbRamp', 'NoCurbRamp', 'Obstacle', 'SurfaceProblem',
                                 'NoSidewalk', 'Occlusion', 'Other', 'Problem', 'Total')
total.attributes <- attributes %>% dplyr::filter(label_type != 'Problem') %>% pull(count) %>% sum
kable(
  attributes %>%
    rbind(data.frame(label_type = c('Total'), count = c(total.attributes))) %>%
    mutate(attribute.type = factor(label_type, ordered.attribute.type.levs)) %>%
    dplyr::select(attribute.type, count) %>%
    mutate(percentage = if_else(attribute.type != 'Problem',
                                paste0(base::format(100 * count / total.attributes, digits = 1),
                                       '%'),
                                '-')) %>%
    arrange(attribute.type),
  align = 'l')
```

### Dataset 1st 100% vs full deployment

The first table describes the dataset at the point where we hit 100% completion. The second table describes the full dataset. Both include _all_ users, not just "good" users. We define a user to be someone who both completed an audit of at least one street segment _and_ placed at least one label after the tutorial.

```{r public.deployment.first.100, echo=FALSE, warning=FALSE}
kable(
  audit.length %>%
    dplyr::filter(dataset == 'first_100%', user_id %in% user.list) %>%
    mutate(role = fct_recode(role, 'Researcher' = 'Administrator', 'Researcher' = 'Owner')) %>%
    summarise_vars('role', c('miles_audited', 'label_count'), funcs = sum, include.n = TRUE),
  digits = 0,
  align = 'l'
)
kable(
  audit.length %>%
    dplyr::filter(dataset == 'all', user_id %in% user.list) %>%
    mutate(role = fct_recode(role, 'Researcher' = 'Administrator', 'Researcher' = 'Owner')) %>%
    summarise_vars('role', c('miles_audited', 'label_count'), funcs = sum, include.n = TRUE),
  digits = 0,
  align = 'l'
)
```


### Data characteristics

This is the start of filtering out users with low labeling frequency.

```{r public.deployment.data.characteristics, echo=FALSE, warning=FALSE}
kable(
  filtered.labels %>%
    distinct(label_id, .keep_all = TRUE) %>%
    count(label_type_id) %>%
    mutate(label_type = LABEL_TYPE_MAPPING[label_type_id]) %>%
    dplyr::select(label_type, n) %>%
    bind_rows(list(label_type = 'Total', n = n_distinct(filtered.labels$label_id))) %>%
    spread(label_type, n),
  align = 'l'
)

filtered.audits <- dplyr::filter(audits, user_id %in% good.user.list, completed == TRUE)
n.audits <- nrow(filtered.audits)
n.streets <- n_distinct(filtered.audits$street_edge_id)

streets.with.mult.audits <-
  filtered.audits %>%
  group_by(street_edge_id) %>%
  dplyr::summarise(n.audits = n()) %>%
  dplyr::filter(n.audits > 1)
```

There have been a total of `r n.audits` audits by our "good" users across `r n.streets` streets, averaging `r base::format(n.audits/n.streets, digits = 3)` audits per street. Of the `r nrow(streets.with.mult.audits)` streets that have been audited multiple times, there are an average of `r base::format(sum(streets.with.mult.audits$n.audits) / nrow(streets.with.mult.audits), digits = 3)` audits per street.


### Data lost due to filtering

```{r public.deployment.data.lost, echo=FALSE, warning=FALSE}
bad.user.list <-
  speeds %>% distinct(user_id) %>% dplyr::filter(user_id %not-in% good.user.list) %>% pull()

n.users.pre.filter <- n_distinct(speeds$user_id)
n.bad.users <- length(bad.user.list)
n.users.remaining <- length(good.user.list)

percent.bad <- base::format(100 * n.bad.users / n.users.pre.filter, digits = 3)
percent.users.remaining <- base::format(100 * n.users.remaining / n.users.pre.filter, digits = 3)

n.labels.pre.filter <- n_distinct(non.tutorial.labels$label_id)
n.labels.bad <- n_distinct(non.tutorial.labels[non.tutorial.labels$user_id %in% bad.user.list,'label_id'])
n.labels.remaining <- n_distinct(filtered.labels$label_id)

percent.labels.bad <- base::format(100 * n.labels.bad / n.labels.pre.filter, digits = 3)
percent.labels.remaining <- base::format(100 * n.labels.remaining / n.labels.pre.filter, digits = 3)
```

There were `r n.users.pre.filter` users who placed `r n.labels.pre.filter` labels pre-filtering. Those with low labeling frequency accounted for `r n.bad.users` of the users (`r percent.bad`%) and `r n.labels.bad` of the labels (`r percent.labels.bad`%). This means that we are left with `r n.users.remaining` of the users (`r percent.users.remaining`%) and `r n.labels.remaining` of the labels (`r percent.labels.remaining`%).


### User stats and tool usage

TODO: Missions started vs missions completed (not sure we can do this; I expect it to be difficult, without much benefit).

Below are the means/medians/sds for a few metrics (followed by sums), split by user group.

In the first 2 tables, we have the non-filtered data: this includes all users that audited at least one street and placed at least one label after the tutorial. The latter 2 tables include the filtered dataset: users were only included if they audited at least one street and had a labeling frequency of at least 3.75 labels per 100 meters.

NOTE: A "session" below is defined as a sequence of audit task interactions for a user where the minimum time between consecutive interactions is less than one hour.

Non filtered dataset:

```{r public.deployment.showing.stats.nonfiltered, echo=FALSE, warning=FALSE}
nonfiltered.speeds <-
  speeds %>%
  dplyr::filter(user_id %in% user.list) %>%
  mutate(role = fct_recode(role, 'Researcher' = 'Administrator', 'Researcher' = 'Owner')) %>%
  mutate(role = factor(role, levels = c('Anonymous', 'Registered', 'Turker', 'Researcher')))

kable(
  nonfiltered.speeds %>%
    mutate(km = meters_audited / 1000) %>%
    dplyr::select(-meters_audited, -feet_audited, -mins_per_mission, -hours_audited,
                  -total_paid, -pay_per_hr) %>%
    summarise_vars('role', include.n = TRUE),
  digits = 3,
  align = 'l'
  )
kable(
  nonfiltered.speeds %>%
    mutate(km = meters_audited / 1000,
           mult_sess = as.numeric(n_sessions > 1)) %>%
    rename(miles = miles_audited,
           audits = audit_count,
           labels = label_count,
           missions = mission_count) %>%
    dplyr::select(role, miles, km, missions, audits, hours_audited, labels, mult_sess) %>%
    summarise_vars('role', funcs = sum, include.n = TRUE) %>%
    mutate(coverage = paste0(base::format(100 * miles / TOTAL_STREET_DIST_MILES, digits = 2), '%'),
           '>1 sess' = paste0(base::format(100 * mult_sess / n.users, digits = 2), '%')) %>%
    dplyr::select(-mult_sess),
  align = 'l',
  digits = 0
  )
```

Filtered dataset:

```{r public.deployment.showing.stats.filtered, echo=FALSE, warning=FALSE}
filtered.speeds.with.researchers <-
  speeds %>%
  dplyr::filter(audit_count > 0,
                labels_per_100m >= 3.75) %>%
  mutate(role = fct_recode(role, 'Researcher' = 'Administrator', 'Researcher' = 'Owner')) %>%
  mutate(role = factor(role, levels = c('Anonymous', 'Registered', 'Turker', 'Researcher')))

kable(
  filtered.speeds.with.researchers %>%
    mutate(km = meters_audited / 1000) %>%
    dplyr::select(-meters_audited, -feet_audited, -mins_per_mission, -hours_audited,
                  -total_paid, -pay_per_hr) %>%
    summarise_vars('role', include.n = TRUE),
  digits = 3,
  align = 'l'
  )
kable(
  filtered.speeds.with.researchers %>%
    mutate(km = meters_audited / 1000,
           mult_sess = as.numeric(n_sessions > 1)) %>%
    rename(miles = miles_audited,
           audits = audit_count,
           labels = label_count,
           missions = mission_count) %>%
    dplyr::select(role, miles, km, missions, audits, hours_audited, labels, mult_sess) %>%
    summarise_vars('role', funcs = sum, include.n = TRUE) %>%
    mutate(coverage = paste0(base::format(100 * miles / TOTAL_STREET_DIST_MILES, digits = 2), '%'),
           '>1 sess' = paste0(base::format(100 * mult_sess / n.users, digits = 2), '%')) %>%
    dplyr::select(-mult_sess),
  align = 'l',
  digits = 0
  )
```

Below are trimmed down tables; non-filtered followed by filtered.

```{r public.deployment.showing.stats.trimmed, echo=FALSE, warning=FALSE}
important.average.stats <-
  nonfiltered.speeds %>%
  mutate(miles = feet_audited / 5280,
         miles_per_hr = 60 * miles / minutes_audited) %>%
  dplyr::select(role, minutes_audited, miles_per_hr, labels_per_100m, problem_labels_per_100m) %>%
  summarise_vars('role', funcs = funs(md = median), include.n = TRUE)
kable(
  nonfiltered.speeds %>%
    mutate(mult_sess = as.numeric(n_sessions > 1),
           miles = feet_audited / 5280) %>%
    rename(audits = audit_count,
           labels = label_count,
           missions = mission_count) %>%
    dplyr::select(role, miles, missions, audits, labels, mult_sess) %>%
    summarise_vars('role', funcs = sum, include.n = TRUE) %>%
    mutate('>1 sess' = paste0(base::format(100 * mult_sess / n.users, digits = 2), '%')) %>%
    dplyr::select(-mult_sess) %>%
    inner_join(important.average.stats, by = c('role', 'n.users')),
  align = 'l',
  digits = 2
)

important.average.stats.filtered <-
  filtered.speeds.with.researchers %>%
  mutate(miles = feet_audited / 5280,
         miles_per_hr = 60 * miles / minutes_audited) %>%
  dplyr::select(role, minutes_audited, miles_per_hr, labels_per_100m, problem_labels_per_100m) %>%
  summarise_vars('role', funcs = funs(md = median), include.n = TRUE)
kable(
  filtered.speeds.with.researchers %>%
    mutate(mult_sess = as.numeric(n_sessions > 1),
           miles = feet_audited / 5280) %>%
    rename(audits = audit_count,
           labels = label_count,
           missions = mission_count) %>%
    dplyr::select(role, miles, missions, audits, labels, mult_sess) %>%
    summarise_vars('role', funcs = sum, include.n = TRUE) %>%
    mutate('>1 sess' = paste0(base::format(100 * mult_sess / n.users, digits = 2), '%')) %>%
    dplyr::select(-mult_sess) %>%
    inner_join(important.average.stats.filtered, by = c('role', 'n.users')),
  align = 'l',
  digits = 2
)
```

### Time to complete tutorial by user group

NOTE: We don't have the time to complete the tutorial for every user that we have data for elsewhere in our analysis. I suspect that this is largely due to us increasing the amount of logging that occurs over time (this is backed up by us missing only a couple turkers, but dozens of users in other user groups). This can also happen if two users use the same computer, because whether you have completed the tutorial or not was saved in the browser.

First for the non-filtered dataset, then for the filtered dataset. This is in minutes.

```{r public.deployment.tutorial.times, echo=FALSE}
tutorial.times.nonfiltered.by.user.group <-
  tutorial.times %>%
  rename(tutorial_minutes = minutes_audited) %>%
  inner_join(speeds, by = 'user_id') %>%
  dplyr::select(user_id, role, tutorial_minutes) %>%
  mutate(role = recode_factor(role,
                              `Anonymous` = 'Anon',
                              `User` = 'Registered',
                              `Turker` = 'Turker',
                              `Administrator` = 'Researcher',
                              `Owner` = 'Researcher'),
         role = as.character(role))

tutorial.times.nonfiltered <-
  tutorial.times.nonfiltered.by.user.group %>%
  dplyr::filter(role %in% c('Anon', 'Turker', 'Registered')) %>%
  mutate(role = 'All Volunteers') %>%
  bind_rows(tutorial.times.nonfiltered.by.user.group)

kable(
  tutorial.times.nonfiltered %>%
    summarise_vars('role', include.n = TRUE),
  digits = 3,
  align = 'l'
)

tutorial.times.filtered <-
  tutorial.times.nonfiltered %>%
  dplyr::filter(user_id %in% good.user.list)

kable(
  tutorial.times.filtered %>%
    summarise_vars('role', include.n = TRUE),
  digits = 3,
  align = 'l'
)
```

## Possible Stories

### Data overlap and agreement between users

Among all the data collected in DC, how much of DC is labeled by multiple users and what is the disagreement among them? (see comment in Outline document for details on implementation)

```{r public.deployment.agreement.analysis, echo=FALSE, fig.width=7, fig.height=4}
percent.streets.with.multiple.audits <-
  filtered.audits %>%
  group_by(street_edge_id) %>%
  dplyr::summarise(n.audits = n()) %>%
  dplyr::filter(n.audits > 1) %>%
  nrow() * 100 / n.streets
```

A total of `r base::format(percent.streets.with.multiple.audits, digits = 3)`% of streets were audited by multiple users. 


### User behavior statistical tests

TODO add tests for adding label descriptions

We ran an ANOVA test for each of the variables we discuss in the paper: mission count, label count, auditing speed (in miles per hour), time audited (in minutes), onboarding completion time (in minutes), and label description count. Because the variables do not meet the homogeneity of variance assumption (i.e., all comparison groups should have approx the same variance), we use an inverse hyperbolic sine transformation on the data before running the tests; it is similar to a log transformation. Web reference that references actual papers: http://worthwhile.typepad.com/worthwhile_canadian_initi/2011/07/a-rant-on-inverse-hyperbolic-sine-transformations.html

Since we are comparing 3 user groups, we then use post-hoc Tukey's HSD tests to determine statistical orderings.

Here are the results for the _unfiltered_ data. ANOVA results followed by Tukey's HSD results.

NOTE you can find the actual values mean values and N for the different user groups in the [User stats and tool usage section](#user-stats-and-tool-usage) above. I'm not copying that info down here due to time constraints.

```{r public.deployment.user.behavior.stats.unfiltered, echo=FALSE, warning=FALSE}
nonfiltered.user.behavior.data <-
  speeds %>%
  dplyr::filter(role %in% c('Registered', 'Turker', 'Anonymous')) %>%
  mutate(miles_per_hour = m_per_min / 26.8224)
nonfiltered.tutorial.data <-
  tutorial.times %>%
  rename(tutorial_minutes = minutes_audited) %>%
  inner_join(nonfiltered.user.behavior.data, by = 'user_id')

filtered.user.behavior.data <-
  filtered.speeds %>%
  dplyr::filter(role %in% c('Registered', 'Turker', 'Anonymous')) %>%
  mutate(miles_per_hour = m_per_min / 26.8224)
filtered.tutorial.data <-
  tutorial.times %>%
  rename(tutorial_minutes = minutes_audited) %>%
  inner_join(filtered.user.behavior.data, by = 'user_id')

# Inverse hyperbolic sine (similar to log, but defined at 0)
# http://worthwhile.typepad.com/worthwhile_canadian_initi/2011/07/a-rant-on-inverse-hyperbolic-sine-transformations.html
ihs <- function(x) { log(x + sqrt(x ^ 2 + 1)) }

mission.aov.nonfilter <- aov(mission_count ~ role, data = nonfiltered.user.behavior.data)
label.aov.nonfilter <- aov(ihs(label_count) ~ role, data = nonfiltered.user.behavior.data)
speed.aov.nonfilter <- aov(ihs(miles_per_hour) ~ role, data = nonfiltered.user.behavior.data)
time.aov.nonfilter <- aov(ihs(minutes_audited) ~ role, data = nonfiltered.user.behavior.data)
tutorial.aov.nonfilter <- aov(ihs(tutorial_minutes) ~ role, data = nonfiltered.tutorial.data)
# description.aov.nonfilter

kable(
  bind_rows(
    base::summary(mission.aov.nonfilter)[[1]]['role',],
    base::summary(label.aov.nonfilter)[[1]]['role',],
    base::summary(speed.aov.nonfilter)[[1]]['role',],
    base::summary(time.aov.nonfilter)[[1]]['role',],
    base::summary(tutorial.aov.nonfilter)[[1]]['role',]
  ) %>%
    cbind(list(variable = c('mission_count', 'label_count', 'miles_per_hour', 'minutes_audited',
                            'tutorial_minutes'))) %>%
    mutate(F = !!sym('F value'),
           p = p.num.to.p.str.with.sig.code(!!sym('Pr(>F)'))) %>%
    dplyr::select(variable, Df, F, p),
  digits = 3,
  align = 'l'
)

kable(
  bind_rows(
    TukeyHSD(mission.aov.nonfilter)[[1]] %>% data.frame() %>%
      tibble::rownames_to_column('comparison') %>% mutate(variable = 'mission_count'),
    TukeyHSD(label.aov.nonfilter)[[1]] %>% data.frame() %>%
      tibble::rownames_to_column('comparison') %>% mutate(variable = 'label_count'),
    TukeyHSD(speed.aov.nonfilter)[[1]] %>% data.frame() %>%
      tibble::rownames_to_column('comparison') %>% mutate(variable = 'miles_per_hour'),
    TukeyHSD(time.aov.nonfilter)[[1]] %>% data.frame() %>%
      tibble::rownames_to_column('comparison') %>% mutate(variable = 'minutes_audited'),
    TukeyHSD(tutorial.aov.nonfilter)[[1]] %>% data.frame() %>%
      tibble::rownames_to_column('comparison') %>% mutate(variable = 'tutorial_minutes')
  ) %>%
    mutate(p = p.num.to.p.str.with.sig.code(p.adj)) %>%
    dplyr::select(variable, comparison, diff, p),
  digits = 3,
  align = 'l'
)
```

Here are the results for the _filtered_ data. ANOVA results followed by Tukey's HSD results.

```{r public.deployment.user.behavior.stats.filtered, echo=FALSE, warning=FALSE}
mission.aov.filter <- aov(ihs(mission_count) ~ role, data = filtered.user.behavior.data)
label.aov.filter <- aov(ihs(label_count) ~ role, data = filtered.user.behavior.data)
speed.aov.filter <- aov(ihs(miles_per_hour) ~ role, data = filtered.user.behavior.data)
time.aov.filter <- aov(ihs(minutes_audited) ~ role, data = filtered.user.behavior.data)
tutorial.aov.filter <- aov(ihs(tutorial_minutes) ~ role, data = filtered.tutorial.data)
# description.aov.filter

kable(
  bind_rows(
    base::summary(mission.aov.filter)[[1]]['role',],
    base::summary(label.aov.filter)[[1]]['role',],
    base::summary(speed.aov.filter)[[1]]['role',],
    base::summary(time.aov.filter)[[1]]['role',],
    base::summary(tutorial.aov.filter)[[1]]['role',]
  ) %>%
    cbind(list(variable = c('mission_count', 'label_count', 'miles_per_hour', 'minutes_audited',
                            'tutorial_minutes'))) %>%
    mutate(F = !!sym('F value'),
           p = p.num.to.p.str.with.sig.code(!!sym('Pr(>F)'))) %>%
    dplyr::select(variable, Df, F, p),
  digits = 3,
  align = 'l'
)

kable(
  bind_rows(
    TukeyHSD(mission.aov.filter)[[1]] %>% data.frame() %>%
      tibble::rownames_to_column('comparison') %>% mutate(variable = 'mission_count'),
    TukeyHSD(label.aov.filter)[[1]] %>% data.frame() %>%
      tibble::rownames_to_column('comparison') %>% mutate(variable = 'label_count'),
    TukeyHSD(speed.aov.filter)[[1]] %>% data.frame() %>%
      tibble::rownames_to_column('comparison') %>% mutate(variable = 'miles_per_hour'),
    TukeyHSD(time.aov.filter)[[1]] %>% data.frame() %>%
      tibble::rownames_to_column('comparison') %>% mutate(variable = 'minutes_audited'),
    TukeyHSD(tutorial.aov.filter)[[1]] %>% data.frame() %>%
      tibble::rownames_to_column('comparison') %>% mutate(variable = 'tutorial_minutes')
  ) %>%
    mutate(p = p.num.to.p.str.with.sig.code(p.adj)) %>%
    dplyr::select(variable, comparison, diff, p),
  digits = 3,
  align = 'l'
)
```

### User dropoffs

NOTE: there are graphs that combine the tutorial and post-tutorial dropoff graphs at the end of this section.

#### Tutorial dropoffs

```{r public.deployment.user.dropoff.graph.funcs, echo=FALSE}
plot.dropoffs <- function(users.remaining.data, dropoff.step.labels, use.percentages, add.labels) {
  if (use.percentages) {
    y.lab <- 'Users Remaining (%)'
    max.users <- max(users.remaining.data$users.remaining)
    data <- mutate(users.remaining.data, users.remaining = users.remaining / max.users)
  } else {
    y.lab <- 'Users Remaining'
    data <- users.remaining.data
  }
  
  partial.plot <-
    ggplot(aes(x = step, y = users.remaining), data = data) +
    geom_line() +
    geom_point() +
    scale_x_continuous(breaks = seq(0, length(dropoff.step.labels) - 1),
                       labels = dropoff.step.labels,
                       expand = c(0.015, 0)) +
    labs(x = 'Step', y = y.lab) + 
    theme_bw() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
    theme(panel.grid.minor.x = element_blank())
  if (use.percentages && add.labels) {
    partial.plot +
      scale_y_continuous(limits = c(0, NA), expand = c(0.015, 0), labels = scales::percent) +
      geom_text(aes(label = scales::percent(users.remaining)),
                nudge_x = 1, nudge_y = 0.04, check_overlap = TRUE)
  } else if (use.percentages) {
    partial.plot +
      scale_y_continuous(limits = c(0, NA), expand = c(0.015, 0), labels = scales::percent)
  } else if (add.labels) {
    partial.plot +
      scale_y_continuous(limits = c(0, NA), expand = c(0.015, 0)) +
      geom_text(aes(label = users.remaining), nudge_x = 0.45, nudge_y = 35, check_overlap = TRUE)
  } else {
    partial.plot + scale_y_continuous(limits = c(0, NA), expand = c(0.015, 0))
  }
}
plot.dropoffs.split <- function(users.remaining.split.data, dropoff.step.labels, use.percentages, add.labels) {
  if (use.percentages) {
    y.lab <- 'Users Remaining (%)'
    max.users <-
      users.remaining.split.data %>%
      group_by(is.turker) %>%
      dplyr::summarise(max.remaining = max(users.remaining))
    data <-
      users.remaining.split.data %>%
      inner_join(max.users, by = 'is.turker') %>%
      mutate(users.remaining = users.remaining / max.remaining)
  } else {
    y.lab <- 'Users Remaining'
    data <- users.remaining.split.data
  }
  data <- data %>% mutate(is.turker = factor(is.turker, labels = c('Volunteers', 'Turkers')))
  
  partial.plot <-
    ggplot(aes(x = step, y = users.remaining), data = data) +
    geom_line(aes(colour = is.turker)) +
    geom_point(aes(colour = is.turker)) +
    scale_x_continuous(breaks = seq(0, length(dropoff.step.labels) - 1),
                       labels = dropoff.step.labels,
                       expand = c(0.015, 0)) +
    labs(x = 'Step', y = y.lab) + 
    theme_bw() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
    theme(panel.grid.minor.x = element_blank(), legend.title = element_blank())
  if (use.percentages && add.labels) {
    partial.plot +
      scale_y_continuous(limits = c(0, NA), expand = c(0.015, 0), labels = scales::percent) +
      geom_text(aes(label = scales::percent(users.remaining)),
                nudge_x = 1, nudge_y = 0.04, check_overlap = TRUE)
  } else if (use.percentages) {
    partial.plot +
      scale_y_continuous(limits = c(0, NA), expand = c(0.015, 0), labels = scales::percent)
  } else if (add.labels) {
    partial.plot +
      scale_y_continuous(limits = c(0, NA), expand = c(0.015, 0)) +
      geom_text(aes(label = users.remaining), nudge_x = 0.45, nudge_y = 35)
  } else {
    partial.plot + scale_y_continuous(limits = c(0, NA), expand = c(0.015, 0))
  }
}
```

```{r public.deployment.tutorial.dropoff, echo=FALSE}
tutorials.parsed <-
  tutorials %>%
  mutate(type = factor(extract_label_type_radio_change(note, action)),
         sev = extract_severity_radio_change(note, action),
         action = factor(action),
         action = fct_recode(action,
                             'mouse_sev_change' = 'ContextMenu_RadioChange',
                             'transition' = 'Onboarding_Transition',
                             'key_sev_1' = 'KeyboardShortcut_Severity_1',
                             'key_sev_3' = 'KeyboardShortcut_Severity_3',
                             'nosidewalk' = 'LabelingCanvas_FinishLabeling',
                             'mode_ramp' = 'ModeSwitch_CurbRamp',
                             'mode_noramp' = 'ModeSwitch_NoCurbRamp',
                             'mode_nosidewalk' = 'ModeSwitch_NoSidewalk',
                             'start' = 'Onboarding_Start',
                             'end' = 'Onboarding_End',
                             'step' = 'ViewControl_DoubleClick')) %>%
  rename(time = timestamp)

tasks.where.tutorial.started <-
  tutorials.parsed %>%
  group_by(audit.task.id) %>%
  dplyr::summarise(has.transition = 'transition' %in% action) %>%
  dplyr::filter(has.transition == TRUE) %>%
  pull(audit.task.id)

suppressWarnings(
steps.by.user <-
  tutorials.parsed %>%
  dplyr::filter(audit.task.id %in% tasks.where.tutorial.started) %>%
  group_by(audit.task.id, user.id) %>%
  mutate(cur.time = min(time[action == 'start']),
         # Step 1: CurbRamp severity 1
         placed.label = 'mode_ramp' %in% action[time + 1 > cur.time],
         label.time = if_else(placed.label,
                              min(time[action == 'mode_ramp' & time + 1 > cur.time]),
                              as.POSIXct(NA)),
         rated.sev = 'key_sev_1' %in% action[time + 1 > label.time]
                      | 'CurbRamp' %in% type[time + 1 > label.time],
         sev.time = if_else(rated.sev,
                            min(time[action == 'key_sev_1' & time + 1 > label.time],
                                time[type == 'CurbRamp' & time + 1 > label.time],
                                na.rm = TRUE),
                            as.POSIXct(NA)),
         steps = if_else(rated.sev, 1, 0),
         cur.time = if_else(steps == 1, sev.time, cur.time),

         # Step 2: CurbRamp severity 1
         placed.label = 'mode_ramp' %in% action[time + 1 > cur.time],
         label.time = if_else(placed.label,
                              min(time[action == 'mode_ramp' & time + 1 > cur.time]),
                              as.POSIXct(NA)),
         rated.sev = 'key_sev_1' %in% action[time + 1 > label.time]
                      | 'CurbRamp' %in% type[time + 1 > label.time],
         sev.time = if_else(rated.sev,
                            min(time[action == 'key_sev_1' & time + 1 > label.time],
                                time[type == 'CurbRamp' & time + 1 > label.time],
                                na.rm = TRUE),
                            as.POSIXct(NA)),
         steps = if_else(rated.sev, 2, steps),
         cur.time = if_else(steps == 2, sev.time, cur.time),

         # Step 3: NoCurbRamp severity 3
         placed.label = 'mode_noramp' %in% action[time + 1 > cur.time],
         label.time = if_else(placed.label,
                              min(time[action == 'mode_noramp' & time + 1 > cur.time]),
                              as.POSIXct(NA)),
         rated.sev = 'key_sev_3' %in% action[time + 1 > label.time]
                        | 'NoCurbRamp' %in% type[time + 1 > label.time],
         sev.time = if_else(rated.sev,
                            min(time[action == 'key_sev_3' & time + 1 > label.time],
                                time[type == 'NoCurbRamp' & time + 1 > label.time],
                                na.rm = TRUE),
                            as.POSIXct(NA)),
         steps = if_else(rated.sev, 3, steps),
         cur.time = if_else(steps == 3, sev.time, cur.time),

         # Step 4: CurbRamp severity 1
         placed.label = 'mode_ramp' %in% action[time + 1 > cur.time],
         label.time = if_else(placed.label,
                              min(time[action == 'mode_ramp' & time + 1 > cur.time]),
                              as.POSIXct(NA)),
         rated.sev = 'key_sev_1' %in% action[time + 1 > label.time]
                      | 'CurbRamp' %in% type[time + 1 > label.time],
         sev.time = if_else(rated.sev,
                            min(time[action == 'key_sev_1' & time + 1 > label.time],
                                time[type == 'CurbRamp' & time + 1 > label.time],
                                na.rm = TRUE),
                            as.POSIXct(NA)),
         steps = if_else(rated.sev, 4, steps),
         cur.time = if_else(steps == 4, sev.time, cur.time),

         # Step 5: CurbRamp severity 1
         placed.label = 'mode_ramp' %in% action[time + 1 > cur.time],
         label.time = if_else(placed.label,
                              min(time[action == 'mode_ramp' & time + 1 > cur.time]),
                              as.POSIXct(NA)),
         rated.sev = 'key_sev_1' %in% action[time + 1 > label.time]
                      | 'CurbRamp' %in% type[time + 1 > label.time],
         sev.time = if_else(rated.sev,
                            min(time[action == 'key_sev_1' & time + 1 > label.time],
                                time[type == 'CurbRamp' & time + 1 > label.time],
                                na.rm = TRUE),
                            as.POSIXct(NA)),
         steps = if_else(rated.sev, 5, steps),
         cur.time = if_else(steps == 5, sev.time, cur.time),

         # Step 6: NoSidewalk
         placed.label = 'nosidewalk' %in% action[time + 1 > cur.time],
         label.time = if_else(placed.label,
                              min(time[action == 'nosidewalk' & time + 1 > cur.time]),
                              as.POSIXct(NA)),
         steps = if_else(placed.label, 6, steps),
         cur.time = if_else(steps == 6, label.time, cur.time),

         # Step 7: CurbRamp severity 1
         placed.label = 'mode_ramp' %in% action[time + 1 > cur.time],
         label.time = if_else(placed.label,
                              min(time[action == 'mode_ramp' & time + 1 > cur.time]),
                              as.POSIXct(NA)),
         rated.sev = 'key_sev_1' %in% action[time + 1 > label.time]
                      | 'CurbRamp' %in% type[time + 1 > label.time],
         sev.time = if_else(rated.sev,
                            min(time[action == 'key_sev_1' & time + 1 > label.time],
                                time[type == 'CurbRamp' & time + 1 > label.time],
                                na.rm = TRUE),
                            as.POSIXct(NA)),
         steps = if_else(rated.sev, 7, steps),
         cur.time = if_else(steps == 7, sev.time, cur.time),

         # Step 8: Step and finish tutorial
         has.step = 'step' %in% action[time + 1 > cur.time],
         has.end = 'end' %in% action[time + 1 > cur.time],
         old.steps = steps,
         steps = if_else(has.step & has.end, 8, steps),
         end.time = if_else(has.step & has.end,
                            min(time[action == 'end' & time + 1 > cur.time]),
                            cur.time)) %>%
  summarise_at(dplyr::vars(steps, end.time, is.turker), first) %>%
  summarise_at(dplyr::vars(steps, user.id, end.time, is.turker), first) %>%
  group_by(user.id) %>%
  dplyr::summarise(steps.complete = max(steps),
                   audit.task.id = first(audit.task.id[steps == max(steps)]),
                   end.timestamp = first(end.time[steps == max(steps)]),
                   is.turker = first(is.turker[steps == max(steps)]))
)

get.users.remaining <- Vectorize(
  function(data, step, is.turk = NA) {
    if (is.na(is.turk)) nrow(data[data$steps.complete >= step,])
    else nrow(dplyr::filter(data, steps.complete >= step, is.turker == is.turk))
  },
  vectorize.args = c('step', 'is.turk')
)
users.remaining.tutorial <-
  data.frame(step = seq(0, 8)) %>%
  mutate(users.remaining = get.users.remaining(steps.by.user, step))
users.remaining.tutorial.split <-
  data.frame(step = rep(seq(0, 8), 2),
             is.turker = c(rep(TRUE, 9), rep(FALSE, 9))) %>%
  mutate(users.remaining = get.users.remaining(steps.by.user, step, is.turker))

n.started.tutorial <- users.remaining.tutorial[1,'users.remaining']
n.did.first.step.tutorial <- users.remaining.tutorial[2,'users.remaining']
n.finished.tutorial <- users.remaining.tutorial[9,'users.remaining']
percent.did.first.step.tutorial <- 100 * n.did.first.step.tutorial / n.started.tutorial
percent.finished.tutorial <- 100 * n.finished.tutorial / n.started.tutorial
```

Below we look at how users drop off in the tutorial. Because most volunteers going through the tutorial have not yet registered, we simply use IP address as a "user" for the purpose of analyzing the tutorial in particular; however, turkers are automatically logged in immediately, so we use turker id for them. We consider only users who clicked on the "Let's get started!" button on the tutorial page as having started the tutorial. To simplify the analysis, we are looking at only tutorial attempts after the small additions we made to the tutorial last summer (so the data starts July 10th, 2017).

The steps in the graph below are as follows:

0. Clicked "Let's get started!"
1. Placed first curb ramp label _and correctly rated severity_
2. Placed second curb ramp label and correctly rated severity
3. Placed missing curb ramp label and correctly rated severity
4. Placed third curb ramp label and correctly rated severity
5. Placed fourth curb ramp label and correctly rated severity
6. Placed no sidewalk label
7. Placed fifth curb ramp label and correctly rated severity
8. Took step forward and clicked through all dialog boxes to finish tutorial

Of the `r n.started.tutorial` people who started the tutorial, `r n.did.first.step.tutorial` finished the first step (`r base::format(percent.did.first.step.tutorial, digits = 0)`%), and `r n.finished.tutorial` completed the tutorial (`r base::format(percent.finished.tutorial, digits = 0)`%).

The first graph shows both volunteers and turkers together, and the second shows them split.

```{r public.deployment.tutorial.dropoff.plots, echo=FALSE, fig.width=5, fig.height=3}
step.labels <- c('Start tutorial',
                 'First curb ramp',
                 'Second curb ramp',
                 'Missing curb ramp',
                 'Third curb ramp',
                 'Fourth curb ramp',
                 'No sidewalk',
                 'Fifth curb ramp',
                 'Finish tutorial')
plot.dropoffs(users.remaining.tutorial, step.labels, use.percentages = FALSE, add.labels = FALSE)
plot.dropoffs.split(users.remaining.tutorial.split, step.labels, use.percentages = FALSE, add.labels = FALSE)
```


#### Post tutorial dropoffs

```{r public.deployment.post.tutorial.dropoff, echo=FALSE}
# Takes users who have completed the tuturial (using same IPs from tutorial dropoff analysis), and
# counts how many missions they completed in the 2 hours after finishing the tutorial.
dropoffs.by.user <-
  dropoffs %>%
  dplyr::filter(action == 'MissionComplete') %>%
  mutate(timestamp = as.POSIXct(round(timestamp, units = 'mins'))) %>%
  distinct() %>%
  rbind(dplyr::filter(dropoffs, action == 'PanoId_Changed')) %>%
  inner_join(steps.by.user, by = c('user.id', 'is.turker')) %>%
  dplyr::filter(steps.complete == 8) %>%
  group_by(user.id) %>%
  dplyr::filter((action == 'MissionComplete' & difftime(timestamp, end.timestamp) > 0)
                | (action == 'PanoId_Changed' & difftime(timestamp, end.timestamp,
                                                         units = 'secs') > -1),
         difftime(timestamp, end.timestamp, units = 'hours') < 2) %>%
  dplyr::summarise(missions.complete = sum(action == 'MissionComplete'),
                   panos.changed = sum(action == 'PanoId_Changed'),
                   is.turker = first(is.turker)) %>%
  right_join(steps.by.user, by = c('user.id', 'is.turker')) %>%
  dplyr::filter(steps.complete == 8) %>%
  replace_na(list(missions.complete = 0, panos.changed = 0)) %>%
  mutate(steps.complete = case_when(missions.complete >= 10 ~ 11,
                                    missions.complete == 9 ~ 10,
                                    missions.complete == 8 ~ 9,
                                    missions.complete == 7 ~ 8,
                                    missions.complete == 6 ~ 7,
                                    missions.complete == 5 ~ 6,
                                    missions.complete == 4 ~ 5,
                                    missions.complete == 3 ~ 4,
                                    missions.complete == 2 ~ 3,
                                    missions.complete == 1 ~ 2,
                                    panos.changed >= 2     ~ 1,
                                    TRUE                   ~ 0))

users.remaining.post.tutorial <-
  data.frame(step = seq(0, 11)) %>%
  mutate(users.remaining = get.users.remaining(dropoffs.by.user, step))
users.remaining.post.tutorial.split <-
  data.frame(step = rep(seq(0, 11), 2),
             is.turker = c(rep(TRUE, 12), rep(FALSE, 12))) %>%
  mutate(users.remaining = get.users.remaining(dropoffs.by.user, step, is.turker))

n.took.step <- users.remaining.post.tutorial[2,'users.remaining']
n.did.one.mission <- users.remaining.post.tutorial[3,'users.remaining']
n.did.two.missions <- users.remaining.post.tutorial[4,'users.remaining']
n.did.ten.missions <- users.remaining.post.tutorial[12,'users.remaining']
percent.took.step <- 100 * n.took.step / n.finished.tutorial
percent.did.one.mission <- 100 * n.did.one.mission / n.finished.tutorial
percent.did.two.missions <- 100 * n.did.two.missions / n.finished.tutorial
percent.did.ten.missions <- 100 * n.did.ten.missions / n.finished.tutorial
```

Below we look at how users drop off after finishing the tutorial. For consistency with the tutorial analysis, we use IP address to denote a volunteer user and turker id to differentiate between turkers (the `r n.finished.tutorial` IP addresses and turker ids who finished the tutorial in the previous section are the same `r n.finished.tutorial` IPs/IDs that we are looking at below). As such, we are looking at only those who finished the tutorial after July 10th, 2017.

We also only counted missions completed in the two hours after finishing the tutorial. There was not a sizeable difference when looking at 1 hour, 2 hours, or 48 hours after finishing the tutorial, so I think that 2 hours works fine.

The steps in the graph below are as follows:

0. Finished tutorial
1. Took a step
2. Completed first mission
3. Completed second mission
4. And so on...

Of the `r n.finished.tutorial` people who finished the tutorial, `r n.took.step` took a step (`r base::format(percent.took.step, digits = 0)`%), `r n.did.one.mission` finished at least one mission afterwards (`r base::format(percent.did.one.mission, digits = 0)`%), `r n.did.two.missions` completed at least two missions afterwards (`r base::format(percent.did.two.missions, digits = 0)`%), and `r n.did.ten.missions` did at least 10 missions afterwards (`r base::format(percent.did.ten.missions, digits = 0)`%).

The first graph shows both volunteers and turkers together, and the second shows them split.

```{r public.deployment.post.tutorial.dropoff.plot, echo=FALSE, fig.width=5, fig.height=3}
dropoff.labels <- c('Finish tutorial',
                    'Take step',
                    '1 mission',
                    '2 missions',
                    '3 missions',
                    '4 missions',
                    '5 missions',
                    '6 missions',
                    '7 missions',
                    '8 missions',
                    '9 missions',
                    '10 missions')
plot.dropoffs(users.remaining.post.tutorial,
              dropoff.labels,
              use.percentages = FALSE,
              add.labels = FALSE)
plot.dropoffs.split(users.remaining.post.tutorial.split,
                    dropoff.labels,
                    use.percentages = FALSE,
                    add.labels = FALSE)
```


#### Tutorial and post tutorial dropoffs

Below are simply concatenations of the graphs from each of the two sub-sections above. These are followed by versions that use percentage of total users remaining instead of counts.


```{r public.deployment.user.dropoff.plot, echo=FALSE, fig.width=8, fig.height=5}
# Combine the datasets from the two previous sub-sections.
users.remaining.all <-
  users.remaining.post.tutorial %>%
  mutate(step = step + 8) %>%
  dplyr::filter(step > 8) %>%
  bind_rows(users.remaining.tutorial)
users.remaining.all.split <-
  users.remaining.post.tutorial.split %>%
  mutate(step = step + 8) %>%
  dplyr::filter(step > 8) %>%
  bind_rows(users.remaining.tutorial.split)

combined.dropoff.labels <- c(step.labels, dropoff.labels[2:length(dropoff.labels)])

plot.dropoffs(users.remaining.all, combined.dropoff.labels, use.percentages = FALSE, add.labels = FALSE)
plot.dropoffs.split(users.remaining.all.split, combined.dropoff.labels, use.percentages = FALSE, add.labels = FALSE)
plot.dropoffs(users.remaining.all, combined.dropoff.labels, use.percentages = TRUE, add.labels = FALSE)
plot.dropoffs.split(users.remaining.all.split, combined.dropoff.labels, use.percentages = TRUE, add.labels = FALSE)
plot.dropoffs(users.remaining.all, combined.dropoff.labels, use.percentages = FALSE, add.labels = TRUE)
plot.dropoffs.split(users.remaining.all.split, combined.dropoff.labels, use.percentages = FALSE, add.labels = TRUE)
plot.dropoffs(users.remaining.all, combined.dropoff.labels, use.percentages = TRUE, add.labels = TRUE)
plot.dropoffs.split(users.remaining.all.split, combined.dropoff.labels, use.percentages = TRUE, add.labels = TRUE)
```


# Turk Study

Update: This is now all of the data. There used to be 19 anonymous user routes, but three of them actually had no labels placed by the anonymous user (we had forgotten to check beforehand), thus we have only 16.

Even though 5 turkers did each route, the high level results for individual turkers looks only at the first turker to complete each set of routes. This makes aggregate stats more even, and a fairer comparison across user groups. (but maybe we should actually use all turkers when not aggregating, actually...)

## High level results

```{r turk.reading.cleaning.data, echo=FALSE, include=FALSE}
# Get IRR data
agreement.classes <- c('numeric', 'factor', 'factor', 'factor', 'numeric', 'numeric')
agreement.data <- read.csv('../data/irr_results-not_for_humans.csv', colClasses = agreement.classes)

label.type.levels.in.order <- c('CurbRamp','NoCurbRamp','Obstacle','SurfaceProb','Problem')
clean.agreement.data <-
  agreement.data %>%
  mutate(label.type = case_when(
            data.type %in% c('prob.binary', 'prob.ordinal') ~ 'Problem',
            label.type == 'SurfaceProblem'                  ~ 'SurfaceProb',
            TRUE                                            ~ as.character(label.type)
          ),
         data.type = if_else(data.type %in% c('prob.binary', 'binary'),
                             'binary',
                             'ordinal')) %>%
  dplyr::filter(level == 'street',
                data.type == 'binary',
                label.type %in% c(NON_AGGREGATE_TYPES, 'Problem')) %>%
  mutate(data.type = factor(data.type),
         level = factor(level),
         label.type = factor(label.type, label.type.levels.in.order))


# Get accuracy data
classes <- c('numeric', replicate(5, 'character'), 'numeric', 'numeric', 'logical', 'character',
             'factor', 'character', replicate(8, 'numeric'))
volunteer.data <- read.csv('../data/accuracies-volunteer.csv',
                           colClasses = classes,
                           na.strings = c('null')) %>%
  mutate(is.turker = FALSE)
turker.data <- read.csv('../data/accuracies-turker.csv',
                        colClasses = classes,
                        na.strings = c('null')) %>%
  mutate(is.turker = TRUE)

# Combine datasets
accuracy.data <- rbind(volunteer.data, turker.data)

# Create data for summing true/false pos/neg over all label types, and all problem label types.
# First summing over curb ramp, missing curb ramp, obstacle, and surface problem...
all.types.data <-
  accuracy.data %>%
  dplyr::filter(label.type %in% c('CurbRamp', 'NoCurbRamp', 'Obstacle', 'SurfaceProblem')) %>%
  group_by(condition.id, worker1, worker2, worker3, worker4, worker5, n.workers, worker.thresh,
           binary, included.severity, granularity, is.turker) %>%
  dplyr::summarize(label.type = 'All',
                   true.pos = sum(true.pos),
                   false.pos = sum(false.pos),
                   true.neg = sum(true.neg),
                   false.neg = sum(false.neg)) %>%
  ungroup()

# Then summing over just missing curb ramp, obstacle, and surface problem...
problem.types.data <-
  accuracy.data %>%
  dplyr::filter(label.type %in% c('NoCurbRamp', 'Obstacle', 'SurfaceProblem')) %>%
  group_by(condition.id, worker1, worker2, worker3, worker4, worker5, n.workers, worker.thresh,
           binary, included.severity, granularity, is.turker) %>%
  dplyr::summarize(label.type = 'AllProb',
                   true.pos = sum(true.pos),
                   false.pos = sum(false.pos),
                   true.neg = sum(true.neg),
                   false.neg = sum(false.neg)) %>%
  ungroup()

# Now we combine them and compute the accuracy metrics for each row, and attach to main dataset.
data.with.agg.types <-
  all.types.data %>%
  bind_rows(problem.types.data) %>%
  mutate(precision = true.pos / (true.pos + false.pos),
         recall = true.pos / (true.pos + false.neg),
         specificity = true.neg / (true.neg + false.pos),
         f.measure = 2 * (precision * recall) / (precision + recall)) %>%
  bind_rows(accuracy.data)

# Remove occlusion and other label types, rename SurfaceProblem label type as SurfaceProb for
# easier visualization and set order of factor levels.
label.types <- c('All', 'Problem', 'CurbRamp', 'NoCurbRamp', 'Obstacle', 'SurfaceProb', 'NoSidewalk', 'AllProb')
data.filtered.label.type <-
  data.with.agg.types %>%
  mutate(label.type = if_else(label.type == 'SurfaceProblem', 'SurfaceProb', label.type)) %>%
  dplyr::filter(label.type %in% label.types) %>%
  mutate(label.type = factor(label.type, levels = label.types, labels = label.types))

# More setup: remove binary analysis (except street level), add is.anon.route column, add
#             raw.accuracy column, add worker.type column, give granularity an ordering.
get.worker.type <- Vectorize(
  function(n.workers, worker.thresh, is.turker, is.anon.route) {
    if (n.workers == 5 & worker.thresh == 3) 'turk5'
    else if (n.workers == 3 & worker.thresh == 2) 'turk3'
    else if (n.workers == 1 & is.turker == TRUE) 'turk1'
    else if (is.turker == TRUE) 'turk0' # probably aren't analyzing any other turkers
    else if (is.anon.route == TRUE) 'anon'
    else 'reg'
  }
)
# The user.id column has the user id for reg and turk users, but for anon they are of the form
# '<anon.user.id>---<ip.address>'. Since the anon user id is the same for everyone, we just want the
# ip address. So this function splits on '---' (which only occurs for anon users), reverses the
# output of the split (meaning no ip address is now first in output list for anon users, and no
# change for other users, since the list is of length one), then takes first element of that list.
fix.anon.names <- Vectorize(
  function(user.id) {
    rev(strsplit(user.id, '---', fixed = TRUE)[[1]])[1]
  }
)
  
data.with.raw.accuracy <-
  data.filtered.label.type %>%
  dplyr::filter(granularity != '10_meter') %>%
  mutate(is.anon.route = condition.id > 121) %>%
  mutate(worker.type = factor(get.worker.type(n.workers, worker.thresh, is.turker, is.anon.route),
                              levels = c('anon', 'reg', 'turk1', 'turk3', 'turk5', 'turk0'))) %>%
  mutate(worker1 = fix.anon.names(worker1)) %>%
  mutate(raw.accuracy = (true.pos + true.neg) / (true.pos + true.neg + false.pos + false.neg)) %>%
  mutate(granularity = factor(granularity,
                              levels = c('street', '5_meter'),
                              labels = c('street', '5_meter'))) %>%
  dplyr::filter(label.type != 'NoSidewalk' | (granularity == 'street' & binary == TRUE))

all.users.with.raw.accuracy <-
  data.with.raw.accuracy %>%
  dplyr::filter(worker.type %in% c('turk1', 'anon', 'reg')) %>%
  mutate(worker.type = 'all')

turk.with.raw.accuracy <-
  data.with.raw.accuracy %>%
  dplyr::filter(n.workers %in% c(3, 5),
                worker.thresh == 1) %>%
  mutate(worker.type = case_when(n.workers == 3 ~ 'turk3.all',
                                 n.workers == 5 ~ 'turk5.all'))

expanded.data.with.raw.accuracy <-
  data.with.raw.accuracy %>%
  mutate(worker.type = case_when(worker.type == 'turk3' ~ 'turk3.maj.vote',
                                 worker.type == 'turk5' ~ 'turk5.maj.vote',
                                 TRUE                   ~ as.character(worker.type))) %>%
  bind_rows(all.users.with.raw.accuracy, turk.with.raw.accuracy)

validation.study.good.users <-
  data.with.raw.accuracy %>%
  dplyr::filter(n.workers == 1, binary == FALSE, included.severity == 'all',
                granularity == 'street', label.type == 'All') %>%
  mutate(label.count = true.pos + false.pos,
         labels.per.100m = if_else(is.anon.route,
         label.count / 6.096, label.count / 12.192)) %>%
  dplyr::filter(labels.per.100m >= 3.75) %>%
  distinct(worker1) %>%
  pull()
```


### Ground truth label counts

```{r turk.high.level.results, echo=FALSE, include=FALSE}
unique.turkers <-
  data.with.raw.accuracy %>%
  dplyr::filter(worker.type == 'turk5',
                binary == FALSE,
                included.severity == 'all',
                granularity == 'street',
                label.type == 'CurbRamp') %>%
  dplyr::select(worker1, worker2, worker3, worker4, worker5) %>%
  gather(worker.index, turker.id) %>%
  dplyr::select(turker.id)

n.turkers <- nrow(unique.turkers)
n.reg <-
  data.with.raw.accuracy %>%
  dplyr::filter(!is.turker, !is.anon.route) %>%
  dplyr::select(worker1) %>%
  n_distinct()
n.anon <-
  data.with.raw.accuracy %>%
  dplyr::filter(!is.turker, is.anon.route) %>%
  dplyr::select(worker1) %>%
  n_distinct()

aggregate.accuracy.data <-
  expanded.data.with.raw.accuracy %>%
  dplyr::filter(worker.type %in% c('anon', 'reg', 'turk1'),
                binary == TRUE,
                included.severity == 'all',
                label.type %in% c('All', 'Problem')) %>%
  select_at(dplyr::vars(label.type, granularity, recall, precision, f.measure)) %>%
  group_by(label.type, granularity) %>%
  summarize_at(dplyr::vars(recall, precision, f.measure), mean, na.rm = TRUE) %>%
  ungroup()

# We filter down to the data we care about, then in the group_by we get down to 2 entries per group;
# one entry for the All type and one entry for the Problem type. We want to combine those into one
# row with columns all.recall, all.precision, prob.recall, prob.precision, etc. So while they are
# grouped, we create each of those new columns. We fill in the all.* columns in correctly in the
# "All" label type rows, putting NaN in the "Problem" rows. Vice versa for the prob.* columns. Then
# to actually combine these two rows we take the sum (where NA is treated as 0 in the sum). But b/c
# NA is treated as 0, this doesn't work when the value that was _supposed_ to be there is NA also!
# So we use an if_else to set to NA if both are NA, and take the some o/w.
turk.study.summary.stats <-
  expanded.data.with.raw.accuracy %>%
  dplyr::filter(worker.type != 'turk0',
                binary == TRUE,
                included.severity == 'all',
                label.type %in% c('All', 'Problem')) %>%
  group_by(condition.id, worker1, is.anon.route, worker.type, granularity) %>%
  mutate(all.recall = if_else(label.type == 'All', recall, NaN),
         all.precision = if_else(label.type == 'All', precision, NaN),
         all.f.measure = if_else(label.type == 'All', f.measure, NaN),
         prob.recall = if_else(label.type == 'Problem', recall, NaN),
         prob.precision = if_else(label.type == 'Problem', precision, NaN),
         prob.f.measure = if_else(label.type == 'Problem', f.measure, NaN)) %>%
  summarize_at(dplyr::vars(all.recall:prob.f.measure),
               function(x) { if_else(length(na.omit(x)) > 0, sum(x, na.rm = TRUE), NaN) }) %>%
  ungroup()

turk.study.summary.stats.with.labels <-
  turk.study.summary.stats %>%
  dplyr::filter(worker.type %in% c('anon', 'reg', 'turk1')) %>%
  left_join(labels.turk.study, by = c('worker1' = 'user_id', 'condition.id' = 'condition_id')) %>%
  group_by_at(dplyr::vars(worker1:prob.f.measure)) %>%
  dplyr::summarise(n.labels = n_distinct(label_id, na.rm = TRUE)) %>%
  left_join(times.turk.study, by = c('worker1' = 'user_id')) %>%
  left_join(user.search.times %>% dplyr::filter(label.type == 'All'),
            by = c('worker1' = 'user.id')) %>%
  mutate(n.missions = if_else(is.anon.route, 2, 3),
         distance.feet = if_else(is.anon.route, 2000, 4000),
         distance.meters = distance.feet / 3.28084,
         labels.per.100m = 100 * n.labels / distance.meters,
         meters.per.min = distance.meters / minutes_audited,
         hours.audited = minutes_audited / 60)
```

Below is a table showing number of ground truth labels by user group and by label type.

```{r turk.showing.stats.gt, echo=FALSE}
# GT label counts by label type and route.
gt.count.by.route.and.lab.type <-
  data.with.raw.accuracy %>%
  dplyr::filter(worker.type %in% c('anon', 'reg'),
                binary == FALSE,
                included.severity == 'all',
                granularity == 'street') %>%
  group_by(condition.id, label.type) %>%
  summarize(gt.labels = sum(true.pos, false.neg)) %>%
  ungroup()

gt.count.by.label.type <-
  gt.count.by.route.and.lab.type %>%
  group_by(label.type) %>%
  summarise_at(dplyr::vars(gt.labels), sum)

gt.count.data <-
  data.with.raw.accuracy %>%
  dplyr::filter(worker.type %in% c('anon', 'reg'),
                binary == FALSE,
                included.severity == 'all',
                granularity == 'street',
                label.type != 'AllProb') %>%
  group_by(worker.type, label.type) %>%
  summarize(gt.labels = sum(true.pos, false.neg)) %>%
  ungroup() %>%
  mutate(worker.type = as.character(worker.type))

gt.count.totals <-
  gt.count.data %>%
  group_by(label.type) %>%
  summarize(gt.labels = sum(gt.labels)) %>%
  mutate(worker.type = 'total')

n.gt <- gt.count.totals %>% dplyr::filter(label.type == 'All') %>% pull(gt.labels)

gt.count.percentages <-
  gt.count.totals %>%
  mutate(gt.labels = paste0(base::format(100 * gt.labels / n.gt, digits = 2), '%'),
         worker.type = '% of total')
  
table.levels <- c('anon', 'reg', 'total', '% of total')
kable(bind_rows(gt.count.data %>% mutate(gt.labels = as.character(gt.labels)),
                gt.count.totals %>% mutate(gt.labels = as.character(gt.labels)),
                gt.count.percentages) %>%
        mutate(worker.type = factor(worker.type, levels = table.levels, labels = table.levels)) %>%
        spread(label.type, gt.labels),
  format = 'markdown',
  digits = 3,
  align = 'l'
  )
```

A total of `r n.turkers` turkers, `r n.reg` registered users, and `r n.anon` anonymous users were part of this study.


### Aggregate accuracy

Below are two tables (street level, then 5 meter level) showing mean accuracy across all users when aggregating over all label types, and for problem vs no problem. We see that the All/Problem accuracies are comparable at the street level, but accuracy is much higher for curb ramps than problems at the 5 meter level (which makes Problem have a much lower accuracy than All).

NOTE: In these two tables, the data is binary (not ordinal), we are only considering single users auditing (i.e., no multi-user clustering or majority vote), and we only consider the first turker to audit each route.

Mean accuracy across all users - street level:

```{r turk.showing.stats.2, echo=FALSE}
kable(
  aggregate.accuracy.data %>%
    dplyr::filter(granularity == 'street') %>%
    dplyr::select(-granularity),
  format = 'markdown',
  digits = 3,
  align = 'l'
  )
```

Mean accuracy across all users - 5 meter level:

```{r turk.showing.stats.3, echo=FALSE}
kable(
  aggregate.accuracy.data %>%
    dplyr::filter(granularity == '5_meter') %>%
    dplyr::select(-granularity),
  digits = 3,
  align = 'l'
  )
```

### Raw accuracy

Mean/median/sd raw accuracy by user group and label type - street level:

```{r turk.showing.stats.raw.accuracy.1, echo=FALSE}
turk.showing.raw.accuracy.by.label.type <-
  data.with.raw.accuracy %>%
  dplyr::filter(binary == TRUE,
                n.workers == 1,
                included.severity == 'all',
                label.type %in% LABEL_TYPES_TO_ANALYZE) %>%
  arrange(worker.type, label.type) %>%
  mutate(user.group.label.type = paste0(worker.type, ', ', label.type),
         # Making a factor and passing levels param to prevent auto-ordering levels alphabetically
         user.group.label.type = factor(user.group.label.type,
                                        levels = unique(user.group.label.type)))

kable(
  turk.showing.raw.accuracy.by.label.type %>%
    dplyr::filter(granularity == 'street') %>%
    summarise_vars('user.group.label.type', c('raw.accuracy')),
  digits = 3,
  align = 'l'
  )
```

Mean/median/sd raw accuracy by user group and label type - 5 meter level:

```{r turk.showing.stats.raw.accuracy.2, echo=FALSE}
kable(
  turk.showing.raw.accuracy.by.label.type %>%
    dplyr::filter(granularity == '5_meter') %>%
    summarise_vars('user.group.label.type', c('raw.accuracy')),
  digits = 3,
  align = 'l'
  )
```

### Voting: Improved recall when at least one turker marks

```{r turk.voting.method.setup, echo=FALSE}
voting.method.data <-
  data.with.raw.accuracy %>%
  dplyr::filter(label.type %in% c('All', 'Problem'),
                n.workers == 5 | worker.type == 'turk1',
                binary == TRUE,
                granularity == 'street',
                label.type == 'Problem',
                included.severity == 'all',
                worker.thresh %in% c(1,3)) %>%
  mutate(voting.method = factor(if_else(worker.thresh == 1,
                                        'at.least.one',
                                        'majority.vote'))) %>%
  group_by(worker.type, voting.method, label.type, granularity) %>%
  dplyr::summarize_at(dplyr::vars(recall, precision), mean, na.rm = TRUE) %>%
  ungroup()

# Get individual turker recall to compare to
turk.recall <-
  voting.method.data %>%
  dplyr::filter(worker.type == 'turk1') %>%
  dplyr::select(recall) %>%
  as.numeric() %>%
  base::format(digits = 2)
turk.precision <-
  voting.method.data %>%
  dplyr::filter(worker.type == 'turk1') %>%
  dplyr::select(precision) %>%
  as.numeric() %>%
  base::format(digits = 2)
```


Since dealing with false positives is pretty easy (relative to walking through GSV), the most important thing for us is to maximize recall. So how does recall look if we consider a label placed by at least one turker as a potential attribute (i.e., we use the "at least one" voting method)?

For reference, individual turkers tended to have the best performance among our user groups, and their recall for problem vs no problem was `r turk.recall` and their precision was `r turk.precision`.

NOTE: In this section we are looking at _problem vs no problem_, the data are binary (not ordinal), the data are at the street level (not 5 meter level), and we are looking at 5 clustered turkers with the "at least one" voting method.

*Takeaways*:

* The mean recall improves significantly when going from majority vote to the "at least one" voting method, accompanied by a much smaller decrease in precision. Since recall is much more important to us, this is the voting method we should likely use going forward.

```{r turk.voting.method.analysis, echo=FALSE}
kable(
  voting.method.data %>%
    dplyr::filter(worker.type != 'turk1') %>%
    dplyr::select(voting.method, recall, precision),
  digits = 3,
  align = 'l'
)
```


### Descriptive stats for users

Next we have some descriptive statistics of users, by user group. These are average (mean/median) stats.

NOTE: In this table, we are only considering single users auditing (i.e., no multi-user clustering or majority vote), and we only consider the first turker to audit each route.

```{r turk.showing.stats.6, echo=FALSE}
kable(
  turk.study.summary.stats.with.labels %>%
    dplyr::filter(granularity == 'street') %>%  # granularity doesn't affect these stats
    rename(m.p.min = meters.per.min, sec.p.label = seconds.to.label) %>%
    summarise_vars('worker.type',
                   var.names = c('labels.per.100m', 'm.p.min', 'minutes_audited', 'sec.p.label'),
                   include.n = TRUE),
  digits = 3,
  align = 'l'
  )
```

Below, we have a table of aggregate (sum) stats by user group.

NOTE: In this table, we are only considering single users auditing (i.e., no multi-user clustering or majority vote), and we only consider the first turker to audit each route.

```{r turk.showing.stats.7, echo=FALSE}
kable(
  turk.study.summary.stats.with.labels %>%
    dplyr::filter(granularity == 'street') %>%  # granularity doesn't affect these stats
    mutate(distance.miles = distance.feet / 5280,
           distance.km = distance.feet / 3280.84) %>%
    summarise_vars('worker.type',
                   c('n.missions', 'distance.miles', 'distance.km', 'n.labels', 'hours.audited'),
                   funcs = sum,
                   include.n = TRUE),
  digits = 3,
  align = 'l'
  )
```


### Giant table with accuracy

```{r turk.big.table, echo=FALSE}
kable(
  expanded.data.with.raw.accuracy %>%
    dplyr::filter(included.severity == 'all',
                  binary == TRUE,
                  worker.type != 'turk0',
                  label.type %in% LABEL_TYPES_TO_ANALYZE) %>%
    rename(prec = precision, rec = recall, raw.acc = raw.accuracy) %>%
    arrange(label.type, granularity, worker.type) %>%
    mutate(label.type.granularity.user.group = paste0(label.type, ', ', granularity, ', ', worker.type),
           label.type.granularity.user.group = factor(label.type.granularity.user.group,
                                                      levels = unique(label.type.granularity.user.group))) %>%
    summarise_vars('label.type.granularity.user.group',
                   var.names = c('rec', 'prec', 'raw.acc'),
                   funcs = funs(md = median, mn = mean, sd = sd, se = se)),
  digits = 3,
  align = 'l'
)
```


### Effects of filtering

_Non filtered_ large table like above, for individual users (i.e., anon, reg, and turk1) only.

```{r turk.big.table.unfiltered, echo=FALSE}
kable(
  data.with.raw.accuracy %>%
    dplyr::filter(included.severity == 'all',
                  binary == TRUE,
                  worker.type %in% c('anon', 'reg', 'turk1'),
                  label.type %in% LABEL_TYPES_TO_ANALYZE) %>%
    rename(prec = precision, rec = recall, raw.acc = raw.accuracy) %>%
    arrange(label.type, granularity, worker.type) %>%
    mutate(label.type.granularity.user.group = paste0(label.type, ', ', granularity, ', ', worker.type),
           label.type.granularity.user.group = factor(label.type.granularity.user.group,
                                                      levels = unique(label.type.granularity.user.group))) %>%
    summarise_vars('label.type.granularity.user.group',
                   var.names = c('rec', 'prec', 'raw.acc'),
                   funcs = funs(md = median, mn = mean, sd = sd, se = se)),
  digits = 3,
  align = 'l'
)
```

_Filtered_ large table like above, for individual users (i.e., anon, reg, and turk1) only. Filtered in terms of GUAs.

```{r turk.big.table.filtered, echo=FALSE}
kable(
  data.with.raw.accuracy %>%
    dplyr::filter(included.severity == 'all',
                  binary == TRUE,
                  worker.type %in% c('anon', 'reg', 'turk1'),
                  worker1 %in% validation.study.good.users,
                  label.type %in% LABEL_TYPES_TO_ANALYZE) %>%
    rename(prec = precision, rec = recall, raw.acc = raw.accuracy) %>%
    arrange(label.type, granularity, worker.type) %>%
    mutate(label.type.granularity.user.group = paste0(label.type, ', ', granularity, ', ', worker.type),
           label.type.granularity.user.group = factor(label.type.granularity.user.group,
                                                      levels = unique(label.type.granularity.user.group))) %>%
    summarise_vars('label.type.granularity.user.group',
                   var.names = c('rec', 'prec', 'raw.acc'),
                   funcs = funs(md = median, mn = mean, sd = sd, se = se)),
  digits = 3,
  align = 'l'
)
```


### IRR

NOTE: In these tables, the data is binary (not ordinal), and is at the street level (not 5 meter level).

Our average (mean) IRR (kripp alpha) over the 7 rounds, by label type, is in the first table below, followed by mean kripp alpha across label types for each round, followed by the IRR in each round by label type:

```{r turk.irr.stats, echo=FALSE}
kable(
  clean.agreement.data %>%
    group_by(label.type) %>%
    dplyr::summarize(mean.kripp.alpha = mean(kripp.alpha),
                     mean.agreement = mean(agreement)),
  digits = 3,
  align = 'l'
  )
kable(
  clean.agreement.data %>%
    group_by(round.code) %>%
    dplyr::summarise(mean.kripp.alpha = mean(kripp.alpha),
                     mean.agreement = mean(agreement)),
  digits = 3,
  align = 'l'
)
kable(
  clean.agreement.data %>%
    arrange(label.type, round.code) %>%
    mutate(label.type.round = paste0(label.type, ', ', round.code),
           label.type.round = factor(label.type.round, levels = unique(label.type.round))) %>%
    dplyr::select(label.type.round, kripp.alpha, agreement),
  digits = 3,
  align = 'l'
  )
```



## Possible Stories

### Granularity: Street-level vs 5 meter-level

Below we compare street vs 5 meter level recall and precision by label type.

NOTE: In this section, the data is binary (not ordinal), we are only considering single users auditing (i.e., no multi-user clustering or majority vote), and we only consider the first turker to audit each route.

Below is a table showing label type accuracy at the two granularity levels, followed by a graph that gives a visual representation of the mean and standard error.

```{r turk.granularity.analysis, echo=FALSE, fig.width=7.5, fig.height=5}
granularity.analysis.data <-
  data.with.raw.accuracy %>%
  dplyr::filter(included.severity == 'all',
                binary == TRUE,
                n.workers == 1,
                label.type %in% LABEL_TYPES_TO_ANALYZE) %>%
  gather(accuracy.type, accuracy.value, recall, precision, raw.accuracy,
         na.rm = TRUE, factor_key = TRUE)

granularity.analysis.summary <-
  granularity.analysis.data %>%
  group_by(label.type, accuracy.type, granularity) %>%
  dplyr::summarize(sd = sd(accuracy.value, na.rm = TRUE),
                   se = sd(accuracy.value, na.rm = TRUE) / sqrt(n()),
                   mean.accuracy.value = mean(accuracy.value, na.rm = TRUE),
                   median.accuracy.value = median(accuracy.value, na.rm = TRUE),
                   accuracy.value = mean(accuracy.value, na.rm = TRUE),
                   error.bar.min = max(0, accuracy.value - se),
                   error.bar.max = min(1, accuracy.value + se)) %>%
  ungroup()

kable(
  granularity.analysis.summary %>%
    mutate(mean.accuracy = mean.accuracy.value,
           median.accuracy = median.accuracy.value) %>%
    dplyr::select(accuracy.type, label.type, granularity, mean.accuracy, median.accuracy, sd, se) %>%
    arrange(accuracy.type),
  digits = 3,
  align = 'l'
)

# Make the set of labels that give the number of observations for each sub-graph.
granularity.analysis.labels <-
  granularity.analysis.data %>%
  dplyr::filter(granularity == 'street', accuracy.type == 'recall') %>%
  group_by(label.type) %>%
  dplyr::summarise(n.users = sum(!is.na(accuracy.value))) %>%
  inner_join(gt.count.by.label.type, by = 'label.type') %>%
  mutate(label = paste0(label.type, '\nN=', n.users, ';L=', gt.labels)) %>%
  pull(label)

ggplot(data = granularity.analysis.summary, aes(x = label.type, y = accuracy.value)) +
  geom_col(aes(group = granularity, fill = granularity), position = position_dodge()) +
  geom_errorbar(aes(ymin = error.bar.min, ymax = error.bar.max, group = granularity),
                width = 0.25, position = position_dodge(0.9)) +
  scale_x_discrete(labels = granularity.analysis.labels) +
  scale_y_continuous(breaks = ACCURACY_BREAKS, limits = c(0, 1.0), expand = c(0.01, 0)) +
  facet_grid(accuracy.type ~ .) +
  ggtitle('Street vs. 5 Meter level Granularity') +
  my.theme.discrete.x
```

And here is the same graph as above for only "good users", i.e., those with a labeling frequency over 3.75 labels per 100 meters.

```{r turk.granularity.analysis.good.users, echo=FALSE, fig.width=7.5, fig.height=5}
granularity.analysis.summary.good.users <-
  granularity.analysis.data %>%
  dplyr::filter(worker1 %in% validation.study.good.users) %>%
  group_by(label.type, accuracy.type, granularity) %>%
  dplyr::summarize(sd = sd(accuracy.value, na.rm = TRUE),
                   se = sd(accuracy.value, na.rm = TRUE) / sqrt(n()),
                   mean.accuracy.value = mean(accuracy.value, na.rm = TRUE),
                   median.accuracy.value = median(accuracy.value, na.rm = TRUE),
                   accuracy.value = mean(accuracy.value, na.rm = TRUE),
                   error.bar.min = max(0, accuracy.value - se),
                   error.bar.max = min(1, accuracy.value + se)) %>%
  ungroup()

granularity.analysis.labels.good.users <-
  granularity.analysis.summary.good.users %>%
  dplyr::filter(granularity == 'street', accuracy.type == 'recall') %>%
  group_by(label.type) %>%
  dplyr::summarise(n.users = sum(!is.na(accuracy.value))) %>%
  inner_join(gt.count.by.label.type, by = 'label.type') %>%
  mutate(label = paste0(label.type, '\nN=', n.users, ';L=', gt.labels)) %>%
  pull(label)

ggplot(data = granularity.analysis.summary.good.users, aes(x = label.type, y = accuracy.value)) +
  geom_col(aes(group = granularity, fill = granularity), position = position_dodge()) +
  geom_errorbar(aes(ymin = error.bar.min, ymax = error.bar.max, group = granularity),
                width = 0.25, position = position_dodge(0.9)) +
  scale_x_discrete(labels = granularity.analysis.labels.good.users) +
  scale_y_continuous(breaks = ACCURACY_BREAKS, limits = c(0, 1.0), expand = c(0.01, 0)) +
  facet_grid(accuracy.type ~ .) +
  ggtitle('Street vs. 5 Meter level Granularity') +
  my.theme.discrete.x
```

*Takeaways*:

* Analyzing at the 5 meter level shows higher raw accuracy and specificity, both because of the large number of true negatives that we get from splitting into 5 meter segments; there are very few street segments with no labels at all.

* Analyzing at the street level shows higher recall, implying that there were relatively fewer false negatives at the street level. This may mean that users aren't finding _every_ issue, but they are more likely to find _at least one_ issue of that type when there are multiple that occur on the same street.

* Analyzing at the street level shows higher precision, implying that there were relatively fewer false positives at the street level. I suspect that this is due to fundamental misunderstandings about how to label (implying both that labeling is complex and difficult and that our onboarding is insufficient) which are persistent/consistent and frequent (think: labeling driveways as curb ramps, labeling storm drains as missing curb ramps, and labeling fire hydrants or street signs that are not in the way as obstacles). In those cases where the mistake is made frequently (multiple times per street), relatively fewer false positives makes sense when moving to street level analysis.

* Analyzing at the street level shows higher f-measure. This clearly comes from the higher recall and precision.

* CurbRamp pretty much outperforms all other label types across the board, regardless of accuracy type of 5 meter vs. street level. This is likely because curb ramps are the easiest label type to understand and find in GSV (both because they are large and easy to see, and because you know where to expect them -- at intersections).

* The SurfaceProblem label type seems to have the highest precision and lowest recall among the different types of issues (I'm excluding CurbRamp here). I guess that, relative to the other types of issues, there are just fewer cases of mistaking something of a surface problem and more cases of not finding a surface problem that was visible in GSV (so maybe surface problems require increased diligence from users, and the other issues require better treatment in onboarding).

* The Problem type seems to perform better than the surface problem and obstacle label types (except for surface problem precision, mentioned in the previous bullet).

* NoCurbRamp seems to have high recall and low precision. This fits my intuition; since users know to expect curb ramps at intersections, if they arrive at an intersection and a curb ramp is not there, they know to place a NoCurbRamp label. However, if there was no sidewalk at all, then we did not add the missing curb ramp labels to the ground truth dataset, and this is not something that we covered during onboarding. I suspect that this, paired with users marking storm drains as missing curb ramps, were the main reasons for the low recall. Both could be addressed through proper training.


### Accuracy by user group

NOTE: In these two tables, the data is binary (not ordinal) and these are mean/median accuracies aggregated across all label types (all.\*) and for the problem vs no problem type (prob.\*).

#### Summary stats

Mean/median/sd accuracy by user group - street level:

```{r turk.showing.stats.4, echo=FALSE}
turk.showing.stats.4.and.5 <-
  turk.study.summary.stats %>%
    group_by(worker.type, granularity) %>%
    dplyr::summarize(
      all.rec.mn = mean(all.recall, na.rm = TRUE),
      all.rec.md = median(all.recall, na.rm = TRUE),
      all.rec.sd = sd(all.recall, na.rm = TRUE),
      all.rec.se = se(all.recall),
      all.prec.mn = mean(all.precision, na.rm = TRUE),
      all.prec.md = median(all.precision, na.rm = TRUE),
      all.prec.sd = sd(all.precision, na.rm = TRUE),
      all.prec.se = se(all.precision),
      all.f.mn = mean(all.f.measure, na.rm = TRUE),
      all.f.md = median(all.f.measure, na.rm = TRUE),
      all.f.sd = sd(all.f.measure, na.rm = TRUE),
      all.f.se = se(all.f.measure),
      prob.rec.mn = mean(prob.recall, na.rm = TRUE),
      prob.rec.md = median(prob.recall, na.rm = TRUE),
      prob.rec.sd = sd(prob.recall, na.rm = TRUE),
      prob.rec.se = se(prob.recall),
      prob.prec.mn = mean(prob.precision, na.rm = TRUE),
      prob.prec.md = median(prob.precision, na.rm = TRUE),
      prob.prec.sd = sd(prob.precision, na.rm = TRUE),
      prob.prec.se = se(prob.precision),
      prob.f.mn = mean(prob.f.measure, na.rm = TRUE),
      prob.f.md = median(prob.f.measure, na.rm = TRUE),
      prob.f.sd = sd(prob.f.measure, na.rm = TRUE),
      prob.f.se = se(prob.f.measure)
    ) %>%
    rename(user.type = worker.type)

kable(
  turk.showing.stats.4.and.5 %>%
    dplyr::filter(granularity == 'street') %>%
    dplyr::select(-granularity),
  digits = 3,
  align = 'l'
  )
```

Mean/median/sd accuracy by user group - 5 meter level:

```{r turk.showing.stats.5, echo=FALSE}
kable(
  turk.showing.stats.4.and.5 %>%
    dplyr::filter(granularity == '5_meter') %>%
    dplyr::select(-granularity),
  digits = 3,
  align = 'l'
  )
```


#### Statistical significance

NOTE: This is at the street level (not 5 meter level) and only looks at the aggregation of all label types ("All").

We created binomial mixed effects models to determine the relationship between user group and recall/precision. We had user group as the fixed effect and route id as the random effect. We modeled recall/precision/raw accuracy as binomial and used a logistic link function. You can check out some notes on mixed effects models at the end of this document: [here](#rationale-and-description-of-mixed-effect-models).

```{r turk.user.group.accuracy.glmm, echo=FALSE}
# TODO include the fact that the single turker is included in turk3, etc as a random effect
user.group.test.data <-
  expanded.data.with.raw.accuracy %>%
  dplyr::filter(worker.type != 'turk0',
                granularity == 'street',
                binary == TRUE,
                included.severity == 'all',
                label.type %in% c('All', 'Problem')) %>%
  mutate(label.type = factor(label.type),
         condition.id = factor(condition.id),
         worker.type = factor(worker.type))

user.group.test.data.single.recall <-
  user.group.test.data %>%
  dplyr::filter(worker.type %in% c('anon', 'reg', 'turk1'), label.type == 'All') %>%
  dplyr::select(condition.id, worker.type, true.pos, false.neg, recall) %>%
  na.omit() %>%
  droplevels()
user.group.test.data.single.prec <-
  user.group.test.data %>%
  dplyr::filter(worker.type %in% c('anon', 'reg', 'turk1'), label.type == 'All') %>%
  dplyr::select(condition.id, worker.type, true.pos, false.pos, precision) %>%
  na.omit() %>%
  droplevels()
user.group.test.data.single.raw <-
  user.group.test.data %>%
  dplyr::filter(worker.type %in% c('anon', 'reg', 'turk1'), label.type == 'All') %>%
  dplyr::select(condition.id, worker.type, true.pos, false.pos, true.neg, false.neg, raw.accuracy) %>%
  na.omit() %>%
  droplevels()

user.group.single.recall.model <- glmer(recall ~ worker.type + (1 | condition.id),
                           data = user.group.test.data.single.recall,
                           family = binomial(link = 'logit'),
                           weights = true.pos + false.neg,
                           control = glmerControl(optimizer = "bobyqa",
                                                  optCtrl = list(maxfun = 100000)))
lrt.user.group.single.recall <- drop1(user.group.single.recall.model, test = 'Chisq')

user.group.single.prec.model <- glmer(precision ~ worker.type + (1 | condition.id),
                         data = user.group.test.data.single.prec,
                         family = binomial(link = 'logit'),
                         weights = true.pos + false.pos,
                         control = glmerControl(optimizer = "bobyqa",
                                                optCtrl = list(maxfun = 100000)))
lrt.user.group.single.prec <- drop1(user.group.single.prec.model, test = 'Chisq')

user.group.single.raw.model <- glmer(raw.accuracy ~ worker.type + (1 | condition.id),
                         data = user.group.test.data.single.raw,
                         family = binomial(link = 'logit'),
                         weights = true.pos + false.pos + true.neg + false.neg,
                         control = glmerControl(optimizer = "bobyqa",
                                                optCtrl = list(maxfun = 100000)))
lrt.user.group.single.raw <- drop1(user.group.single.raw.model, test = 'Chisq')

user.group.test.data.multi.recall <-
  user.group.test.data %>%
  dplyr::filter(worker.type %in% c('turk1', 'turk3.maj.vote', 'turk3.all', 'turk5.maj.vote', 'turk5.all'),
                label.type == 'All') %>%
  dplyr::select(condition.id, worker.type, true.pos, false.neg, recall) %>%
  na.omit() %>%
  droplevels()
user.group.test.data.multi.prec <-
  user.group.test.data %>%
  dplyr::filter(worker.type %in% c('turk1', 'turk3.maj.vote', 'turk3.all', 'turk5.maj.vote', 'turk5.all'),
                label.type == 'All') %>%
  dplyr::select(condition.id, worker.type, true.pos, false.pos, precision) %>%
  na.omit() %>%
  droplevels()
user.group.test.data.multi.raw <-
  user.group.test.data %>%
  dplyr::filter(worker.type %in% c('turk1', 'turk3.maj.vote', 'turk3.all', 'turk5.maj.vote', 'turk5.all'),
                label.type == 'All') %>%
  dplyr::select(condition.id, worker.type, true.pos, false.pos, true.neg, false.neg, raw.accuracy) %>%
  na.omit() %>%
  droplevels()

user.group.multi.recall.model <- glmer(recall ~ worker.type + (1 | condition.id),
                           data = user.group.test.data.multi.recall,
                           family = binomial(link = 'logit'),
                           weights = true.pos + false.neg,
                           control = glmerControl(optimizer = "bobyqa",
                                                  optCtrl = list(maxfun = 100000)))
lrt.user.group.multi.recall <- drop1(user.group.multi.recall.model, test = 'Chisq')

user.group.multi.prec.model <- glmer(precision ~ worker.type + (1 | condition.id),
                         data = user.group.test.data.multi.prec,
                         family = binomial(link = 'logit'),
                         weights = true.pos + false.pos,
                         control = glmerControl(optimizer = "bobyqa",
                                                optCtrl = list(maxfun = 100000)))
lrt.user.group.multi.prec <- drop1(user.group.multi.prec.model, test = 'Chisq')

user.group.multi.raw.model <- glmer(raw.accuracy ~ worker.type + (1 | condition.id),
                         data = user.group.test.data.multi.raw,
                         family = binomial(link = 'logit'),
                         weights = true.pos + false.pos + true.neg + false.neg,
                         control = glmerControl(optimizer = "bobyqa",
                                                optCtrl = list(maxfun = 100000)))
lrt.user.group.multi.raw <- drop1(user.group.multi.raw.model, test = 'Chisq')
```

##### Comparing anon reg and turk1

Using likelihood ratio tests (LRTs), we found the contribution of the fixed effect (worker type) to have a statistically significant association with recall (`r print.lrt.results(lrt.user.group.single.recall, user.group.single.recall.model, 'worker.type')`) and precision (`r print.lrt.results(lrt.user.group.single.prec, user.group.single.prec.model, 'worker.type')`), but we did _not_ find a statistically significant difference for raw accuracy (`r print.lrt.results(lrt.user.group.single.raw, user.group.single.raw.model, 'worker.type')`).

To test that the orderings of the user groups are statistically significant (e.g., that turk1 recall is significantly higher than registered user recall, etc), we do post-hoc Tukey's HSD tests. This essentially gives us a pairwise test between each user group, which lets us determine what parts of the ordering are significant. The results of which are shown in the tables below.

NOTE: `*` means less than 0.05, `**` means less than 0.01, and `***` means less than 0.001

Recall: `r print.lrt.results(lrt.user.group.single.recall, user.group.single.recall.model, 'worker.type')`.

```{r turk.user.group.single.recall.tukey, echo=FALSE}
# Extracts the info we want from the Tukey HSD, and formats it nicely for a table.
tukey.user.group.single.recall <- base::summary(glht(user.group.single.recall.model,
                                                  linfct = mcp(worker.type = 'Tukey')),
                                             test = adjusted('holm'))
tukey.user.group.single.recall.clean <- format.tukey.table(tukey.user.group.single.recall,
                                                  'recall',
                                                  'worker.type',
                                                  user.group.test.data.single.recall,
                                                  descending = TRUE)
kable(tukey.user.group.single.recall.clean, digits = 3, align = 'l')
```

Precision: `r print.lrt.results(lrt.user.group.single.prec, user.group.single.prec.model, 'worker.type')`

```{r turk.user.group.single.prec.tukey, echo=FALSE}
tukey.user.group.single.prec <- base::summary(glht(user.group.single.prec.model,
                                                linfct = mcp(worker.type = 'Tukey')),
                                           test = adjusted('holm'))
tukey.user.group.single.prec.clean <- format.tukey.table(tukey.user.group.single.prec,
                                            'precision',
                                            'worker.type',
                                            user.group.test.data.single.prec,
                                            descending = TRUE)
kable(tukey.user.group.single.prec.clean, digits = 3, align = 'l')
```

##### Comparing different turker groups

Using likelihood ratio tests (LRTs), we found the contribution of the fixed effect (worker type) to have a statistically significant association with recall (`r print.lrt.results(lrt.user.group.multi.recall, user.group.multi.recall.model, 'worker.type')`), precision (`r print.lrt.results(lrt.user.group.multi.prec, user.group.multi.prec.model, 'worker.type')`), and raw accuracy (`r print.lrt.results(lrt.user.group.multi.raw, user.group.multi.raw.model, 'worker.type')`).

To test that the orderings of the user groups are statistically significant (e.g., that turk5 majority vote precision is significantly higher than turk3 majority vote precision, etc), we do post-hoc Tukey's HSD tests. This essentially gives us a pairwise test between each user group, which lets us determine what parts of the ordering are significant. The results of which are shown in the tables below.

NOTE: `*` means less than 0.05, `**` means less than 0.01, and `***` means less than 0.001

Recall: `r print.lrt.results(lrt.user.group.multi.recall, user.group.multi.recall.model, 'worker.type')`.

```{r turk.user.group.multi.recall.tukey, echo=FALSE}
# Extracts the info we want from the Tukey HSD, and formats it nicely for a table.
tukey.user.group.multi.recall <- base::summary(glht(user.group.multi.recall.model,
                                                  linfct = mcp(worker.type = 'Tukey')),
                                             test = adjusted('holm'))
tukey.user.group.multi.recall.clean <- format.tukey.table(tukey.user.group.multi.recall,
                                                  'recall',
                                                  'worker.type',
                                                  user.group.test.data.multi.recall,
                                                  descending = TRUE)
kable(tukey.user.group.multi.recall.clean, digits = 3, align = 'l')
```

Precision: `r print.lrt.results(lrt.user.group.multi.prec, user.group.multi.prec.model, 'worker.type')`

```{r turk.user.group.multi.prec.tukey, echo=FALSE}
tukey.user.group.multi.prec <- base::summary(glht(user.group.multi.prec.model,
                                                linfct = mcp(worker.type = 'Tukey')),
                                           test = adjusted('holm'))
tukey.user.group.multi.prec.clean <- format.tukey.table(tukey.user.group.multi.prec,
                                            'precision',
                                            'worker.type',
                                            user.group.test.data.multi.prec,
                                            descending = TRUE)
kable(tukey.user.group.multi.prec.clean, digits = 3, align = 'l')
```

Raw accuracy: `r print.lrt.results(lrt.user.group.multi.raw, user.group.multi.raw.model, 'worker.type')`

```{r turk.user.group.multi.raw.tukey, echo=FALSE}
tukey.user.group.multi.raw <- base::summary(glht(user.group.multi.raw.model,
                                                linfct = mcp(worker.type = 'Tukey')),
                                           test = adjusted('holm'))
tukey.user.group.multi.raw.clean <- format.tukey.table(tukey.user.group.multi.raw,
                                            'raw.accuracy',
                                            'worker.type',
                                            user.group.test.data.multi.raw,
                                            descending = TRUE)
kable(tukey.user.group.multi.raw.clean, digits = 3, align = 'l')
```


#### Old statistical significance section

NOTE: This is at the street level (not 5 meter level).

We created binomial mixed effects models to determine the relationship between user group and recall/precision. We had user group as the fixed effect and route id as the random effect. We modeled recall/precision as binomial and used a logistic link function. You can check out some notes on mixed effects models at the end of this document: [here](#rationale-and-description-of-mixed-effect-models).

```{r turk.user.group.accuracy.glmm.old, echo=FALSE}
# TODO include the fact that the single turker is included in turk3, etc as a random effect
user.group.test.data.old <-
  data.with.raw.accuracy %>%
  dplyr::filter(worker.type != 'turk0',
                n.workers == 1
                || (n.workers == 3 && worker.thresh = 2)
                || (n.workers == 5 && worker.thresh == 3),
                granularity == 'street',
                binary == TRUE,
                included.severity == 'all',
                label.type %in% c('All', 'Problem')) %>%
  mutate(label.type = factor(label.type),
         condition.id = factor(condition.id),
         worker.type = factor(worker.type))

user.group.test.data.recall.all <-
  user.group.test.data.old %>%
  dplyr::filter(label.type == 'All') %>%
  dplyr::select(condition.id, worker.type, true.pos, false.neg, recall) %>%
  na.omit() %>%
  droplevels()
user.group.test.data.prec.all <-
  user.group.test.data.old %>%
  dplyr::filter(label.type == 'All') %>%
  dplyr::select(condition.id, worker.type, true.pos, false.pos, precision) %>%
  na.omit() %>%
  droplevels()
user.group.test.data.recall.prob <-
  user.group.test.data.old %>%
  dplyr::filter(label.type == 'Problem') %>%
  dplyr::select(condition.id, worker.type, true.pos, false.neg, recall) %>%
  na.omit() %>%
  droplevels()
user.group.test.data.prec.prob <-
  user.group.test.data.old %>%
  dplyr::filter(label.type == 'Problem') %>%
  dplyr::select(condition.id, worker.type, true.pos, false.pos, precision) %>%
  na.omit() %>%
  droplevels()

user.group.recall.all.model <- glmer(recall ~ worker.type + (1 | condition.id),
                           data = user.group.test.data.recall.all,
                           family = binomial(link = 'logit'),
                           weights = true.pos + false.neg,
                           control = glmerControl(optimizer = "bobyqa",
                                                  optCtrl = list(maxfun = 100000)))
lrt.user.group.recall.all <- drop1(user.group.recall.all.model, test = 'Chisq')

user.group.prec.all.model <- glmer(precision ~ worker.type + (1 | condition.id),
                         data = user.group.test.data.prec.all,
                         family = binomial(link = 'logit'),
                         weights = true.pos + false.pos,
                         control = glmerControl(optimizer = "bobyqa",
                                                optCtrl = list(maxfun = 100000)))
lrt.user.group.prec.all <- drop1(user.group.prec.all.model, test = 'Chisq')

user.group.recall.prob.model <- glmer(recall ~ worker.type + (1 | condition.id),
                           data = user.group.test.data.recall.prob,
                           family = binomial(link = 'logit'),
                           weights = true.pos + false.neg,
                           control = glmerControl(optimizer = "bobyqa",
                                                  optCtrl = list(maxfun = 100000)))
lrt.user.group.recall.prob <- drop1(user.group.recall.prob.model, test = 'Chisq')

user.group.prec.prob.model <- glmer(precision ~ worker.type + (1 | condition.id),
                         data = user.group.test.data.prec.prob,
                         family = binomial(link = 'logit'),
                         weights = true.pos + false.pos,
                         control = glmerControl(optimizer = "bobyqa",
                                                optCtrl = list(maxfun = 100000)))
lrt.user.group.prec.prob <- drop1(user.group.prec.prob.model, test = 'Chisq')
```

Using likelihood ratio tests (LRTs), we found the contribution of the fixed effect (worker type) to have a statistically significant association with both recall and precision for both the Problem type and all label types aggregated (results shown below).

To test that the orderings of the user groups are statistically significant (e.g., that turk1 recall is significantly lower than registered user recall for the Problem type, etc), we do post-hoc Tukey's HSD tests. This essentially gives us a pairwise test between each user group, which lets us determine what parts of the ordering are significant. The results of which are shown in the tables below.

NOTE: `*` means less than 0.05, `**` means less than 0.01, and `***` means less than 0.001

Recall, all label types: `r print.lrt.results(lrt.user.group.recall.all, user.group.recall.all.model, 'worker.type')`.

```{r turk.user.group.recall.all.tukey.old, echo=FALSE}
# Extracts the info we want from the Tukey HSD, and formats it nicely for a table.
tukey.user.group.recall.all <- base::summary(glht(user.group.recall.all.model,
                                                  linfct = mcp(worker.type = 'Tukey')),
                                             test = adjusted('holm'))
tukey.user.group.recall.all.clean <- format.tukey.table(tukey.user.group.recall.all,
                                                  'recall',
                                                  'worker.type',
                                                  user.group.test.data.recall.all,
                                                  descending = TRUE)
kable(tukey.user.group.recall.all.clean, digits = 3, align = 'l')
```

Precision, all label types: `r print.lrt.results(lrt.user.group.prec.all, user.group.prec.all.model, 'worker.type')`

```{r turk.user.group.prec.all.tukey.old, echo=FALSE}
tukey.user.group.prec.all <- base::summary(glht(user.group.prec.all.model,
                                                linfct = mcp(worker.type = 'Tukey')),
                                           test = adjusted('holm'))
tukey.user.group.prec.all.clean <- format.tukey.table(tukey.user.group.prec.all,
                                            'precision',
                                            'worker.type',
                                            user.group.test.data.prec.all,
                                            descending = TRUE)
kable(tukey.user.group.prec.all.clean, digits = 3, align = 'l')
```

Recall, Problem type: `r print.lrt.results(lrt.user.group.recall.prob, user.group.recall.prob.model, 'worker.type')`

```{r turk.user.group.recall.prob.tukey.old, echo=FALSE}
tukey.user.group.recall.prob <- base::summary(glht(user.group.recall.prob.model,
                                                   linfct = mcp(worker.type = 'Tukey')),
                                              test = adjusted('holm'))
tukey.user.group.recall.prob.clean <- format.tukey.table(tukey.user.group.recall.prob,
                                                  'recall',
                                                  'worker.type',
                                                  user.group.test.data.recall.prob,
                                                  descending = TRUE)
kable(tukey.user.group.recall.prob.clean, digits = 3, align = 'l')
```

Precision, Problem type: `r print.lrt.results(lrt.user.group.prec.prob, user.group.prec.prob.model, 'worker.type')`

NOTE: Although anon user have a higher average problem type precision than turk5, the model actually says that turk5 has higher precision (though it is not statistically significant). This is because there is just higher precision across the board on the anon user routes; the mixed effects model takes this into account! More on this below.

```{r turk.user.group.prec.prob.tukey.old, echo=FALSE}
tukey.user.group.prec.prob <- base::summary(glht(user.group.prec.prob.model,
                                                 linfct = mcp(worker.type = 'Tukey')),
                                            test = adjusted('holm'))
tukey.user.group.prec.prob.clean <- format.tukey.table(tukey.user.group.prec.prob,
                                            'precision',
                                            'worker.type',
                                            user.group.test.data.prec.prob,
                                            ordering = c('turk5', 'anon', 'turk3', 'turk1', 'reg'),
                                            descending = TRUE)
kable(tukey.user.group.prec.prob.clean, digits = 3, align = 'l')

# Why no significant diff between anon user and others for problem precision..? Because anon users
# seem to have gotten the routes where everyone had more precision:
problem.prec.anon.routes.table <-
  kable(
    user.group.test.data.old %>%
      dplyr::filter(label.type == 'Problem', is.anon.route == TRUE) %>%
      group_by(worker.type) %>%
      dplyr::summarise(problem.precision.on.anon.routes = mean(precision, na.rm = TRUE)),
    digits = 3,
    align = 'l'
  )
```

One interesting thing I am seeing is anon users have a much higher average precision for the Problem type than other user groups, but the difference is not statistically significant. It turns out that on routes audited by anonymous users, turkers _also_ had much higher Problem type precision than for registered user routes. This can be seen in the following table:
`r problem.prec.anon.routes.table`


### Accuracy by label type

NOTE: In the two tables below, the data are binary (not ordinal), we are only considering single users auditing (i.e., no multi-user clustering or majority vote), and we only consider the first turker to audit each route.

#### Summary stats

Mean/median/sd accuracy by label type - street level:

```{r turk.showing.stats.by.label.type.1, echo=FALSE}
turk.showing.stats.by.label.type <-
  data.with.raw.accuracy %>%
  dplyr::filter(binary == TRUE,
                worker.type %in% c('anon', 'reg', 'turk1'),
                included.severity == 'all',
                label.type != 'AllProb')

kable(
  turk.showing.stats.by.label.type %>%
    dplyr::filter(granularity == 'street') %>%
    rename(prec = precision, f = f.measure) %>%
    summarise_vars('label.type', c('recall', 'prec', 'f')),
  digits = 3,
  align = 'l'
  )
```

Mean/median/sd accuracy by label type - 5 meter level:

```{r turk.showing.stats.by.label.type.2, echo=FALSE}
kable(
  turk.showing.stats.by.label.type %>%
    dplyr::filter(granularity == '5_meter') %>%
    rename(prec = precision, f = f.measure) %>%
    summarise_vars('label.type', c('recall', 'prec', 'f')),
  digits = 3,
  align = 'l'
  )
```


#### Statistical significance

NOTE: This is at the street level (not 5 meter level).

We created binomial mixed effects models to determine the relationship between label type and recall/precision. We had label type as the fixed effect and user id nested in route id as random effects. We modeled recall/precision as binomial and used a logistic link function. You can check out some notes on mixed effects models at the end of this document: [here](#rationale-and-description-of-mixed-effect-models).

```{r turk.label.type.accuracy.glmm, echo=FALSE}
type.test.data <-
  turk.showing.stats.by.label.type %>%
  dplyr::filter(granularity == 'street',
                label.type %in% NON_AGGREGATE_TYPES) %>%
  mutate(label.type = factor(label.type),
         user.id = factor(worker1),
         condition.id = factor(condition.id))

type.test.data.recall <-
  type.test.data %>%
  dplyr::select(condition.id, user.id, label.type, true.pos, false.neg, recall) %>%
  na.omit() %>%
  droplevels()
type.test.data.prec <-
  type.test.data %>%
  dplyr::select(condition.id, user.id, label.type, true.pos, false.pos, precision) %>%
  na.omit() %>%
  droplevels()
type.test.data.raw <-
  type.test.data %>%
  dplyr::select(condition.id, user.id, label.type, true.pos, false.pos, true.neg, false.neg, raw.accuracy) %>%
  na.omit() %>%
  droplevels()

type.recall.model <- glmer(recall ~ label.type + (1 | condition.id / user.id),
                           data = type.test.data.recall,
                           family = binomial(link = 'logit'),
                           weights = true.pos + false.neg,
                           control = glmerControl(optimizer = "bobyqa",
                                                  optCtrl = list(maxfun = 100000)))
lrt.type.recall <- drop1(type.recall.model, test = 'Chisq')
tukey.type.recall <- base::summary(glht(type.recall.model, linfct = mcp(label.type = 'Tukey')),
                                   test = adjusted('holm'))

type.prec.model <- glmer(precision ~ label.type + (1 | condition.id / user.id),
                         data = type.test.data.prec,
                         family = binomial(link = 'logit'),
                         weights = true.pos + false.pos,
                         control = glmerControl(optimizer = "bobyqa",
                                                optCtrl = list(maxfun = 100000)))
lrt.type.prec <- drop1(type.prec.model, test = 'Chisq')
tukey.type.prec <- base::summary(glht(type.prec.model, linfct = mcp(label.type = 'Tukey')),
                                 test = adjusted('holm'))

type.raw.model <- glmer(raw.accuracy ~ label.type + (1 | condition.id / user.id),
                        data = type.test.data.raw,
                        family = binomial(link = 'logit'),
                        weights = true.pos + false.pos + true.neg + false.neg,
                        control = glmerControl(optimizer = "bobyqa",
                                               optCtrl = list(maxfun = 100000)))
lrt.type.raw <- drop1(type.raw.model, test = 'Chisq')
tukey.type.raw <- base::summary(glht(type.raw.model, linfct = mcp(label.type = 'Tukey')),
                                test = adjusted('holm'))
```

Using likelihood ratio tests (LRTs), we found the contribution of the fixed effect (label type) to have a statistically significant association with recall (`r print.lrt.results(lrt.type.recall, type.recall.model, 'label.type')`), precision (`r print.lrt.results(lrt.type.prec, type.prec.model, 'label.type')`), and raw accuracy (`r print.lrt.results(lrt.type.raw, type.raw.model, 'label.type')`.

To test that the ordering of the label types are statistically significant (e.g., that NoCurbRamp recall is significantly lower than CurbRamp recall, etc), we do post-hoc Tukey's HSD tests. This essentially gives us a pairwise test between each label type, which lets us determine what parts of the ordering are significant. The results of which are shown in a tables below (first recall, then precision).

NOTE: `*` means less than 0.05, `**` means less than 0.01, and `***` means less than 0.001

Recall: `r print.lrt.results(lrt.type.recall, type.recall.model, 'label.type')`
```{r turk.label.type.accuracy.tukey.recall, echo=FALSE}
# Extracts the info we want from the Tukey HSD, and formats it nicely for a table.
tukey.type.recall.clean <- format.tukey.table(tukey.type.recall,
                                              'recall',
                                              'label.type',
                                              type.test.data.recall,
                                              descending = TRUE)
kable(tukey.type.recall.clean, digits = 3, align = 'l')
```

Precision: `r print.lrt.results(lrt.type.prec, type.prec.model, 'label.type')`
```{r turk.label.type.accuracy.tukey.prec, echo=FALSE}
tukey.type.prec.clean <- format.tukey.table(tukey.type.prec,
                                            'precision',
                                            'label.type',
                                            type.test.data.prec,
                                            descending = TRUE)

kable(tukey.type.prec.clean, digits = 3, align = 'l')
```

Raw accuracy: `r print.lrt.results(lrt.type.raw, type.raw.model, 'label.type')`
```{r turk.label.type.accuracy.tukey.raw, echo=FALSE}
tukey.type.raw.clean <- format.tukey.table(tukey.type.raw,
                                           'raw.accuracy',
                                           'label.type',
                                           type.test.data.raw,
                                           descending = TRUE)

kable(tukey.type.raw.clean, digits = 3, align = 'l')
```


### Visual search time: Time to label by type

NOTE: In this section, the data are binary (not ordinal), and is at the street level (not 5 meter level), we are only considering single users auditing (i.e., no multi-user clustering or majority vote), and we only consider the first turker to audit each route.

NOTE: We believe that a user's "typical" visual search time is more accurately represented by their median visual search time than mean due to the long right tail of the distribution of search times.

Below is a table that shows the average time to place a label by label type along with the average recall and precision. The results match my intuition for the most part: CurbRamp has the shortest labeling time, SurfaceProblem has the longest labeling time, and NoCurbRamp and Obstacle are somewhere in between. However, I do find it a bit surprising that NoCurbRamp took longer to label than Obstacle.

Time to place a label is defined as follows:

* For the first label a user places on a specific panorama, the time that elapsed between stepping into the panorama and placing the label.
* For subsequent labels on the same panorama, the time that elapsed between placing the previous label and placing the current label.

```{r turk.label.difficulty.analysis, echo=FALSE}
accuracy.by.label.type <-
  turk.showing.stats.by.label.type %>%
  dplyr::filter(granularity == 'street') %>%
  group_by(label.type) %>%
  dplyr::summarize(mean_recall = mean(recall, na.rm = TRUE),
                   mean_precision = mean(precision, na.rm = TRUE)) %>%
  mutate(label.type = as.character(label.type))

distinct.workers <- data.with.raw.accuracy %>% distinct(worker1) %>% pull()

ave.labeling.time.by.type.and.user <-
  user.search.times %>%
  dplyr::filter(user.id %in% distinct.workers,
                label.type %not-in% c('All', 'Problem')) %>%
  droplevels() %>%
  mutate(log.seconds.to.label = log(seconds.to.label))

ave.labeling.time.by.type <-
  ave.labeling.time.by.type.and.user %>%
  group_by(label.type) %>%
  dplyr::summarise(median.s.to.label = median(seconds.to.label), # across users, by label type
                   mean.sec.to.label = mean(seconds.to.label),
                   sd.s.to.label = sd(seconds.to.label)) %>%
  mutate(label.type = as.character(label.type)) %>%
  inner_join(accuracy.by.label.type, by = 'label.type') %>%
  arrange(mean.sec.to.label)

kable(
  ave.labeling.time.by.type,
  digits = 2,
  align = 'l'
)

n.ramp.users <- sum(ave.labeling.time.by.type.and.user$label.type == 'CurbRamp')
n.surface.prob.users <- sum(ave.labeling.time.by.type.and.user$label.type == 'NoCurbRamp')
```

We created a linear mixed effects models to determine the relationship between label type and visual search time. We had label type as the fixed effect and user id nested in route id as random effects. We modeled the log of the visual search time as linear and used a logistic link function. You can check out some notes on mixed effects models at the end of this document: [here](#rationale-and-description-of-mixed-effect-models).

```{r turk.visual.search.time.lme.assumption.1, echo=FALSE}
# Very helpful site that brought me from thinking in terms of repeated measures ANOVA to mixed model
# https://m-clark.github.io/docs/mixedModels/anovamixed.html
# Here is what the analysis would have looked like for rANOVA (you can see the similar F-scores):
# base::summary(aov(seconds.to.label ~ label.type + Error(user.id),
#                   data = ave.labeling.time.by.type.and.user))

pre.test.data <- turk.showing.stats.by.label.type %>%
  dplyr::filter(granularity == 'street',
                label.type %in% NON_AGGREGATE_TYPES) %>%
  mutate(label.type = factor(label.type), # drop unused levels
         condition.id = factor(condition.id)) %>%
  dplyr::select(condition.id, worker1, recall, precision, label.type) %>%
  inner_join(ave.labeling.time.by.type.and.user,
             by = c('worker1' = 'user.id', 'label.type' = 'label.type')) %>%
  rename(user.id = worker1) %>%
  mutate(user.id = factor(user.id))

labeling.time.test.data <-
  pre.test.data %>%
  dplyr::select(user.id, condition.id, label.type, seconds.to.label, log.seconds.to.label) %>%
  na.omit() %>%
  droplevels()

# After checking up on these different methods, using ?p.adjust you find the following:
# "There seems no reason to use the unmodified Bonferroni correction because it is dominated by
# Holm's method, which is also valid under arbitrary assumptions."
labeling.time.model <- lme(fixed = seconds.to.label ~ label.type,
                           random = ~ 1 | condition.id / user.id,
                           data = labeling.time.test.data,
                           method = 'ML')

labeling.time.shapiro <- shapiro.test(residuals(labeling.time.model))
```


Quick aside to discuss why we are using log-transformed visual search time: Our two assumptions are normality and heteroscedasticity of the residuals of the fit. Using the Shapiro-Wilk test, we reject the null (`r print.p.value(labeling.time.shapiro$p.value)`), meaning we have not proven that the residuals are normally distributed. The histogram on the left of the residuals looks relatively normal, but there are some outliers, and it has a long-ish right tail. More importantly, we really do not seem to meet the heteroscedasticity assumption, given how the variance seems much larger for longer labeling times in the qq plot on the right.

```{r turk.visual.search.time.lme.assumption.2, echo=FALSE, fig.width=3.5, fig.height=3.5}
ggplot(aes(x = residuals.labeling.time.model.),
       data = data.frame(residuals(labeling.time.model))) +
  geom_histogram(binwidth = 2) +
  theme_bw()
plot(labeling.time.model, resid(., type = "p") ~ fitted(.), abline = 0)


log.labeling.time.model <- update(labeling.time.model, fixed = log.seconds.to.label ~ label.type)

log.labeling.time.shapiro <- shapiro.test(residuals(log.labeling.time.model))
```

To try and deal with not meeting the normality or heteroscedasticity assumptions, we perform a log transform on our outcome variable (visual search time). After the transformation, we still fail the Shapiro-Wilk test (`r print.p.value(log.labeling.time.shapiro$p.value)`), but the histogram on the left looks clearly normal, and the residuals on the right seem to have nearly constant variance. Thus, we meet the assumptions necessary to use this model after the transformation.

```{r turk.visual.search.time.lme.test.1, echo=FALSE, fig.width=3.5, fig.height=3.5}
ggplot(aes(x = residuals.log.labeling.time.model.),
       data = data.frame(residuals(log.labeling.time.model))) +
  geom_histogram(binwidth = 0.15) +
  theme_bw()
plot(log.labeling.time.model, resid(., type = "p") ~ fitted(.), abline = 0)


lrt.labeling.time <- drop1(log.labeling.time.model, test = 'Chisq')
```

Using a likelihood ratio test, we find the contribution by the fixed effect (label type) to be statistically significant (`r print.lrt.results(lrt.labeling.time, log.labeling.time.model, 'label.type')`), i.e., the association between label type and labeling time is statistically significant. To test that the ordering of the label types are statistically significant (e.g., that CurbRamp labeling time is significantly lower than Obstacle labeling time, etc), we do a post-hoc Tukey's HSD test. This essentially gives us a pairwise test between each label type, which lets us determine what parts of the ordering are significant. The results of which are shown in a table below.

NOTE: `*` means less than 0.05, `**` means less than 0.01, and `***` means less than 0.001

```{r turk.visual.search.time.lme.test.2, echo=FALSE}
tukey.log.labeling.time <- base::summary(glht(log.labeling.time.model,
                                              linfct = mcp(label.type = 'Tukey')),
                                         test = adjusted('holm'))

# Extracts the info we want from the Tukey HSD, and formats it nicely for a table.
tukey.log.labeling.time.clean <- format.tukey.table(tukey.log.labeling.time,
                                                    'seconds.to.label',
                                                    'label.type',
                                                    labeling.time.test.data,
                                                    descending = FALSE)

kable(tukey.log.labeling.time.clean, digits = 3, align = 'l')
```


### Zone type: Land use effect on accuracy

#### Definitions

First, we have our definitions of the different zone types with shortened names that we use in parens (these can also be found in doc/zone_type_descriptions.txt):

* Downtown: High-density commercial and residential development
* Mixed-Use: Moderate-density residential and non-residential buildings (e.g., retail, art use)
* Neighborhood Mixed-Use (Nbhd Mixed-Use): Low-density mixed-use development with an emphasis on residential
* Production, distribution, and repair (Industrial): Moderate-density commercial and production, distribution, and repair (with heavy machinery)
* Residential: Predominantly residential with detached houses on medium-to-large lots
* Residential Apartment (Residential Apt): Medium-to-high density residential including apartments and some rowhouses
* Residential Flat: Low-to-moderate density residential rowhouses
* Special Purpose: Includes Fort McNair Naval Facility (and nearby high-density residential), some moderate-density residential, and some undeveloped land.
* Unzoned: Predominantly wooded areas (e.g., large parks, golf courses, and cemeteries) and a military base.

NOTE: For the Special Purpose zone, there are no public roads in the Naval Facility and very few roads in the undeveloped land. Thus, the majority of the _roads_ with the Special Purpose zone tag are in high-density residential areas, particularly apartments.

However, we are going to look at two different groupings of these zone types, because a lot of these zones have very little data. The first grouping is done semantically, and the second is done based on density of development.

* Semantic grouping
    * Commercial = Downtown + Industrial
    * Mixed-Use = Mixed-Use + Neighborhood Mixed-Use
    * Residential Apt = Residential Apt + Special Purpose
    * Residential House = Residential + Residential Flat
    * Unzoned = Unzoned
* Density grouping
    * Low to Moderate Density = Neighborhood Mixed-Use + Residential + Residential Flat + Unzoned
    * Medium to High Density = Downtown + Mixed-Use + Industrial + Residential Apt + Special Purpose

```{r turk.zone.type.setup, echo=FALSE}
all.dc.zoning.data <- data_frame(
  volunteer.type = c('All DC Streets'),
  zone.type = c('Downtown Zone',
                'Mixed-Use Zone',
                'Neighborhood Mixed-Use Zone',
                'Production, Distribution, and Repair Zone',
                'Residential Apartment Zone',
                'Residential Flat Zone',
                'Residential Zone',
                'Special Purpose Zone',
                'Unzoned'),
  percent.of.routes = c(5.74, 11.73, 1.02, 2.47, 14.82, 15.01, 42.89, 1.30, 5.03)
)

recode.semantic <- function(zone.type) {
  fct_recode(zone.type,
             'Commercial'        = 'Downtown Zone',
             'Mixed-Use'         = 'Mixed-Use Zone',
             'Mixed-Use'         = 'Neighborhood Mixed-Use Zone',
             'Commercial'        = 'Production, Distribution, and Repair Zone',
             'Residential Apt'   = 'Residential Apartment Zone',
             'Residential House' = 'Residential Flat Zone',
             'Residential House' = 'Residential Zone',
             'Residential Apt'   = 'Special Purpose Zone',
             'Unzoned'           = 'Unzoned')
}
recode.density <- function(zone.type) {
  fct_recode(zone.type,
             'Medium-High Density'  = 'Downtown Zone',
             'Medium-High Density'  = 'Mixed-Use Zone',
             'Low-Moderate Density' = 'Neighborhood Mixed-Use Zone',
             'Medium-High Density'  = 'Production, Distribution, and Repair Zone',
             'Medium-High Density'  = 'Residential Apartment Zone',
             'Low-Moderate Density' = 'Residential Flat Zone',
             'Low-Moderate Density' = 'Residential Zone',
             'Medium-High Density'  = 'Special Purpose Zone',
             'Low-Moderate Density' = 'Unzoned')
}

# Get predominant zone type for each condition.
get.zone.for.conditions <- function(data, recode.func) {
  data %>%
    dplyr::select(condition.id, is.anon.route) %>%
    distinct() %>%
    inner_join(zoning.data, by = 'condition.id') %>%
    mutate(zone.type = recode.func(zone_type)) %>%
    group_by(condition.id, is.anon.route) %>%
    count(zone.type) %>%
    slice(which.max(n)) %>%
    ungroup()
}
get.zone.distro <- function(data, recode.func) {
  data %>%
    mutate(volunteer.type = factor(is.anon.route,
                                   levels = c(FALSE, TRUE),
                                   labels = c(paste('Reg User Routes:', n.reg, 'total'),
                                              paste('Anon User Routes:', n.anon, 'total')))) %>%
    group_by(volunteer.type, zone.type) %>%
    dplyr::summarise(n.routes = n()) %>%
    ungroup() %>%
    complete(zone.type, fill = list(n.routes = 0)) %>%
    mutate(percent.of.routes = 100 * n.routes / sum(n.routes)) %>%
    dplyr::select(volunteer.type, zone.type, percent.of.routes) %>%
    rbind(all.dc.zoning.data %>% mutate(zone.type = recode.func(zone.type))) %>%
    group_by(volunteer.type, zone.type) %>%
    dplyr::summarise(percent.of.routes = sum(percent.of.routes)) %>%
    ungroup()
}

get.gt.count.distro <- function(data, recode.func) {
  data %>%
    mutate(meters = if_else(is.anon.route, 609.6, 1219.2)) %>%
    inner_join(dplyr::filter(gt.count.by.route.and.lab.type, label.type %in% NON_AGGREGATE_TYPES),
               by = 'condition.id') %>%
    group_by(label.type, zone.type) %>%
    summarise_at(dplyr::vars(gt.labels, meters), sum) %>%
    ungroup() %>%
    mutate(gt.labels.per.100m = 100 * gt.labels / meters) %>%
    dplyr::select(label.type, zone.type, gt.labels, gt.labels.per.100m)
}

plot.gt.dist.for.zone.type <- function(data, n.data, l.data) {
  xlabs <-
    data %>%
    distinct(zone.type) %>%
    mutate(lab = paste0(zone.type, '\nN=', n.data[zone.type], '; L=', l.data[zone.type])) %>%
    pull(lab)

  ggplot(data = data,
         aes(x = zone.type, y = gt.labels.per.100m, fill = label.type)) +
    geom_col(position = 'dodge') +
    my.theme.discrete.x +
    scale_fill_manual(values = c('#66CC66', 'red', 'blue', 'orange')) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
    scale_x_discrete(labels = xlabs) +
    ylab('GT Labels per 100 Meters') +
    scale_y_continuous(expand = c(0.01, 0)) # removes weird white space at bottom of plot
}
```

Here are the distribution of the streets across DC, registered user routes, and anonymous user routes for zone types and quadrant.

```{r turk.zone.type.bar.graphs.setup, echo=FALSE}
quadrant.distro.data <-
  read.csv('../data/dc_quadrant_distribution.csv', stringsAsFactors = FALSE) %>%
  rename(' Anonymous Users' = anon_user_quadrant_distrib..,
         ' Registered Users' = registered_user_quadrant_dist..,
         ' DC' = all_street_quadrant_distribution..) %>%
  gather(key = 'user_group', value = 'distribution',
         ' Anonymous Users', ' Registered Users', ' DC') %>%
  mutate(user_group = factor(user_group,
                             levels = c(' DC', ' Registered Users', ' Anonymous Users')),
         DC_quadrant = factor(DC_quadrant),
         distribution = as.numeric(sub('%', '', distribution)) / 100)

zone.distro.data <-
  read.csv('../data/zone_type_distribution.csv', stringsAsFactors = FALSE) %>%
  rename(' Anonymous Users' = anon_user_zone_distrib..,
         ' Registered Users' = registered_user_zone_dist..,
         ' DC' = all_street_zone_distribution..) %>%
  gather(key = 'user_group',
         value = 'distribution', ' Anonymous Users', ' Registered Users', ' DC') %>%
  mutate(user_group = factor(user_group,
                             levels = c(' DC', ' Registered Users', ' Anonymous Users')),
         DC_zone_type = factor(DC_zone_type),
         distribution = as.numeric(sub('%', '', distribution)) / 100)
```

```{r turk.zone.type.bar.graph.1, echo=FALSE, fig.width=5, fig.height=3}
ggplot(data = quadrant.distro.data, aes(x = DC_quadrant, y = distribution, fill = user_group)) +
  geom_col(position = 'dodge', colour = '#616161') +
  my.theme.discrete.x +
  theme(legend.title = element_blank()) +
  xlab('DC Quadrant') +
  ylab('Share of Streets (%)') +
  scale_y_continuous(expand = c(0, 0), labels = scales::percent, limits = c(0, 0.75)) +
  scale_fill_brewer(type = "qual", palette = 'Blues', direction = 2, aesthetics = "fill")
```
```{r turk.zone.type.bar.graph.2, echo=FALSE, fig.width=6, fig.height=5}
ggplot(data = zone.distro.data, aes(x = DC_zone_type, y = distribution, fill = user_group)) +
  geom_col(position = 'dodge', colour = '#616161') +
  my.theme.discrete.x +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1),
        legend.title = element_blank()) +
  xlab('Zone Type') +
  ylab('Share of Streets (%)') +
  scale_y_continuous(expand = c(0, 0), labels = scales::percent, limits = c(0, 0.75)) +
  scale_fill_brewer(type = "qual", palette = 'Blues', direction = 2, aesthetics = "fill")
```

#### Semantic grouping

As a reminder, the grouping is defined as follows:

* Commercial = Downtown + Industrial
* Mixed-Use = Mixed-Use + Neighborhood Mixed-Use
* Residential Apt = Residential Apt + Special Purpose
* Residential House = Residential + Residential Flat
* Unzoned = Unzoned

##### Distribution

Here is the zone type distribution for the mturk study. We assigned each street in DC a zone type based on the zone in which one of its endpoints is located. We then assigned a zone type to each route as the plurality zone type among the streets on that route. The graph below compares the zone type distributions of the anon user routes, registered user routes, and all of DC for reference.

NOTE: For registered and anon user routes below, it is the percentage of _routes_ marked as that zone type. But for All DC Streets, it is the percentage of _streets_ in DC, since we don't have a set of "routes" that makes up DC.

```{r turk.zone.type.distribution.semantic, echo=FALSE, fig.width=4, fig.height=3}
conditions.w.semantic.zone <- get.zone.for.conditions(data.with.raw.accuracy, recode.semantic)
semantic.zone.distro.by.user.group <- get.zone.distro(conditions.w.semantic.zone, recode.semantic)

ggplot(data = semantic.zone.distro.by.user.group,
       aes(x = zone.type, y = percent.of.routes, fill = volunteer.type)) +
  geom_col(position = 'dodge') +
  my.theme.discrete.x +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
  scale_y_continuous(expand = c(0.01, 0)) # removes weird white space at bottom of plot

kable(
  conditions.w.semantic.zone %>%
    inner_join(route.regions, by = c('condition.id' = 'condition_id')) %>%
    group_by(zone.type) %>%
    dplyr::summarise(n.routes = n(),
                     n.neighborhoods = length(unique(region_id))),
  align = 'l'
)
```


Below, we look at the distribution of the label type densities in the ground truth, by zone type. Commercial seems to be the only zone that has a noticeably different distribution of label types. However, we have only two routes for that zone type so we can't draw anything of statistical significance from that.

```{r gt.label.dist.zone.plot.semantic, fig.height=4, fig.width=6, warning=FALSE, message=FALSE, echo=FALSE}
gt.count.semantic.zone <- get.gt.count.distro(conditions.w.semantic.zone, recode.semantic)

n.routes.by.zone.type.semantic <-
  conditions.w.semantic.zone %>%
  group_by(zone.type) %>%
  dplyr::summarise(N = n())
route.count.zone.lookup.semantic <- setNames(n.routes.by.zone.type.semantic$N,
                                             n.routes.by.zone.type.semantic$zone.type)

total.gt.by.zone.semantic <-
  gt.count.semantic.zone %>%
  group_by(zone.type) %>%
  dplyr::summarise(L = sum(gt.labels))
gt.label.count.zone.lookup.semantic <- setNames(total.gt.by.zone.semantic$L,
                                                total.gt.by.zone.semantic$zone.type)

plot.gt.dist.for.zone.type(gt.count.semantic.zone,
                           route.count.zone.lookup.semantic,
                           gt.label.count.zone.lookup.semantic)
```


##### Relationship with accuracy

The first graph shows all label types aggregated, the second shows the problem vs. no problem type.

NOTE: In this section, the data are binary (not ordinal), and is at the street level (not 5 meter level), we are only considering single users auditing (i.e., no multi-user clustering or majority vote), and we only consider the first turker to audit each route.

NOTE: The red dots on the graphs are means.

NOTE: N in the graphs below means number of routes that are predominantly of that zone type. However, there are 3 users who completed each route, so there are actually n * 3 data points.

The only noticeable difference I see right away is particularly high recall and low precision for Unzoned relative to the other zones. However, there were only 2 routes classified as Unzoned, so we don't have enough data to make any judgements on that. The 3 zone types with significant data (Mixed-Use, Residential Apt, and Residential House) all seem to have roughly equal accuracies.

```{r turk.zone.type.analysis.semantic, echo=FALSE, fig.width=4.5, fig.height=4}
zone.type.analysis.data.semantic <-
  data.with.raw.accuracy %>%
  inner_join(conditions.w.semantic.zone, by = 'condition.id') %>%
  dplyr::filter(binary == TRUE,
                granularity == 'street',
                included.severity == 'all',
                n.workers == 1,
                label.type %in% c('All', 'Problem')) %>%
  gather(accuracy.type, accuracy.value, recall, precision, na.rm = TRUE, factor_key = TRUE)

x.labels.with.n.semantic <-
  n.routes.by.zone.type.semantic %>%
  inner_join(total.gt.by.zone.semantic, by = 'zone.type') %>%
  mutate(lab = paste0(zone.type, '\nN=', N, '; L=', L)) %>%
  pull(lab)

# Create the trellis boxplots.
ggplot(data = zone.type.analysis.data.semantic %>% dplyr::filter(label.type == 'All'),
       aes(x = zone.type, y = accuracy.value)) +
  geom_boxplot() +
  stat_summary(fun.y = mean, geom = 'point', color = 'red', size = 1) +
  scale_x_discrete(labels = x.labels.with.n.semantic) +
  facet_grid(accuracy.type ~ .) +
  ggtitle('Zone Type Accuracy - All Label Types') +
  my.theme.discrete.x +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
  scale_y_continuous(expand = c(0.01, 0)) # removes weird white space at bottom of plot

ggplot(data = zone.type.analysis.data.semantic %>% dplyr::filter(label.type == 'Problem'),
       aes(x = zone.type, y = accuracy.value)) +
  geom_boxplot() +
  stat_summary(fun.y = mean, geom = 'point', color = 'red', size = 1) +
  scale_x_discrete(labels = x.labels.with.n.semantic) +
  facet_grid(accuracy.type ~ .) +
  ggtitle('Zone Type Accuracy - Problem v NoProblem') +
  my.theme.discrete.x +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
  scale_y_continuous(expand = c(0.01, 0)) # removes weird white space at bottom of plot
```

#### Density grouping

As a reminder, the grouping is defined as follows:

* Low to Moderate Density = Neighborhood Mixed-Use + Residential + Residential Flat + Unzoned
* Medium to High Density = Downtown + Mixed-Use + Industrial + Residential Apt + Special Purpose

##### Distribution

Here is the zone type distribution for the mturk study. We assigned each street in DC a zone type based on the zone in which one of its endpoints is located. We then assigned a zone type to each route as the plurality zone type among the streets on that route. The graph below compares the zone type distributions of the anon user routes, registered user routes, and all of DC for reference.

NOTE: For registered and anon user routes below, it is the percentage of _routes_ marked as that zone type. But for All DC Streets, it is the percentage of _streets_ in DC, since we don't have a set of "routes" that makes up DC.

```{r turk.zone.type.distribution.density, echo=FALSE, fig.width=4, fig.height=3}
conditions.w.density.zone <- get.zone.for.conditions(data.with.raw.accuracy, recode.density)
density.zone.distro.by.user.group <- get.zone.distro(conditions.w.density.zone, recode.density)

ggplot(data = density.zone.distro.by.user.group,
       aes(x = zone.type, y = percent.of.routes, fill = volunteer.type)) +
  geom_col(position = 'dodge') +
  my.theme.discrete.x +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
  scale_y_continuous(expand = c(0.01, 0)) # removes weird white space at bottom of plot

kable(
  conditions.w.density.zone %>%
    inner_join(route.regions, by = c('condition.id' = 'condition_id')) %>%
    group_by(zone.type) %>%
    dplyr::summarise(n.routes = n(),
                     n.neighborhoods = length(unique(region_id))),
  align = 'l'
)

# Setup needed for paragraph below.
n.residential.routes.by.density <-
  conditions.w.density.zone %>%
  inner_join(conditions.w.semantic.zone,
             by = 'condition.id',
             suffix = c('.density', '.semantic')) %>%
  dplyr::filter(zone.type.semantic %in% c('Residential House', 'Residential Apt')) %>%
  count(zone.type.density)
residential.route.count.lookup.density <- setNames(n.residential.routes.by.density$n,
                                                   n.residential.routes.by.density$zone.type.density)
n.routes.by.zone.type.density <-
  conditions.w.density.zone %>%
  group_by(zone.type) %>%
  dplyr::summarise(N = n())
route.count.zone.lookup.density <- setNames(n.routes.by.zone.type.density$N,
                                            n.routes.by.zone.type.density$zone.type)
percentage.residential.lookup <-
  setNames(100 * n.residential.routes.by.density$n / n.routes.by.zone.type.density$N,
           n.routes.by.zone.type.density$zone.type)
```

Since there are many residential routes in each group, the number of residential routes is of interest. Of the `r unname(route.count.zone.lookup.density['Low-Moderate Density'])` low-moderate density routes, `r unname(residential.route.count.lookup.density['Low-Moderate Density'])` were residential (`r base::format(unname(percentage.residential.lookup['Low-Moderate Density']), digits = 4)`%). And of the `r unname(route.count.zone.lookup.density['Medium-High Density'])` medium-high density routes, `r unname(residential.route.count.lookup.density['Medium-High Density'])` were residential (`r base::format(unname(percentage.residential.lookup['Medium-High Density']), digits = 4)`%).


Below, we look at the distribution of the label type densities in the ground truth, by zone type. The types of density appear to have similar distributions of label types.

```{r gt.label.dist.zone.plot.density, fig.height=4, fig.width=6, warning=FALSE, message=FALSE, echo=FALSE}
gt.count.density.zone <- get.gt.count.distro(conditions.w.density.zone, recode.density)

total.gt.by.zone.density <-
  gt.count.density.zone %>%
  group_by(zone.type) %>%
  dplyr::summarise(L = sum(gt.labels))
gt.label.count.zone.lookup.density <- setNames(total.gt.by.zone.density$L,
                                                total.gt.by.zone.density$zone.type)

plot.gt.dist.for.zone.type(gt.count.density.zone,
                           route.count.zone.lookup.density,
                           gt.label.count.zone.lookup.density)
```


##### Relationship with accuracy

We first show a table with the mean/median/sd for accuracy in the two zones. This is followed by a pair of graphs where the first graph shows all label types aggregated, the second shows the problem vs. no problem type.

NOTE: In this section, the data are binary (not ordinal), and is at the street level (not 5 meter level), we are only considering single users auditing (i.e., no multi-user clustering or majority vote), and we only consider the first turker to audit each route.

NOTE: The red dots on the graphs are means.

NOTE: N in the graphs below means number of routes that are predominantly of that zone type. However, there are 3 users who completed each route, so there are actually n * 3 data points.

There does not appear to be a significant difference in accuracy between the densities.

```{r turk.zone.type.analysis.density, echo=FALSE, fig.width=4, fig.height=5}
zone.type.analysis.data.density <-
  data.with.raw.accuracy %>%
  inner_join(conditions.w.density.zone, by = 'condition.id') %>%
  dplyr::filter(binary == TRUE,
                granularity == 'street',
                included.severity == 'all',
                n.workers == 1,
                label.type %in% c('All', 'Problem')) %>%
  gather(accuracy.type, accuracy.value, recall, precision, na.rm = TRUE, factor_key = TRUE)

# Table with mean/median/sd
kable(
  zone.type.analysis.data.density %>%
    group_by(accuracy.type, label.type, zone.type) %>%
    dplyr::summarise(mean.accuracy = mean(accuracy.value),
                     median.accuracy = median(accuracy.value),
                     sd = sd(accuracy.value)),
  digits = 3,
  align = 'l'
)

x.labels.with.n.density <-
  n.routes.by.zone.type.density %>%
  inner_join(total.gt.by.zone.density, by = 'zone.type') %>%
  mutate(lab = paste0(zone.type, '\nN=', N, '; L=', L)) %>%
  pull(lab)

# Create the trellis boxplots.
ggplot(data = zone.type.analysis.data.density %>% dplyr::filter(label.type == 'All'),
       aes(x = zone.type, y = accuracy.value)) +
  geom_boxplot() +
  stat_summary(fun.y = mean, geom = 'point', color = 'red', size = 1) +
  scale_x_discrete(labels = x.labels.with.n.density) +
  facet_grid(accuracy.type ~ .) +
  ggtitle('Zone Type Accuracy - All Label Types') +
  my.theme.discrete.x +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
  scale_y_continuous(expand = c(0.01, 0)) # removes weird white space at bottom of plot

ggplot(data = zone.type.analysis.data.density %>% dplyr::filter(label.type == 'Problem'),
       aes(x = zone.type, y = accuracy.value)) +
  geom_boxplot() +
  stat_summary(fun.y = mean, geom = 'point', color = 'red', size = 1) +
  scale_x_discrete(labels = x.labels.with.n.density) +
  facet_grid(accuracy.type ~ .) +
  ggtitle('Zone Type Accuracy - Problem v NoProblem') +
  my.theme.discrete.x +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
  scale_y_continuous(expand = c(0.01, 0)) # removes weird white space at bottom of plot
```



### User behavior: Does auditing speed, etc influence accuracy

Variables being investigated: labeling frequency, auditing speed, and visual search time association with recall and precision. I'm also taking a look at both the All and Problem (vs. NoProblem) label types; we had been planning to only look at the All type, but it was easy enough for me to add both, and we can see if there is anything interesting there.

NOTE: In this section, the data are binary (not ordinal), at the street level granularity (not 5 meter level) we are only considering single users auditing (i.e., no multi-user clustering or majority vote), and we only consider the first turker to audit each route.

```{r turk.user.behavior.setup, echo=FALSE}
# Combines user behavior and accuracy data from various sources. So we start by filtering for the
# accuracy data we are using (single user, binary, etc.), then we do joins to get label counts and
# auditing times, and visual search times.
behavior.test.data.unscaled <-
  data.with.raw.accuracy %>%
  dplyr::filter(worker.type %in% c('anon', 'reg', 'turk1'),
                binary == TRUE,
                included.severity == 'all',
                granularity == 'street',
                label.type %in% c('All', 'Problem')) %>%
  dplyr::select(condition.id, worker1, label.type, true.pos:recall, is.anon.route) %>%
  droplevels() %>%
  inner_join(labels.turk.study, by = c('worker1' = 'user_id', 'condition.id' = 'condition_id')) %>%
  group_by_at(dplyr::vars(condition.id:is.anon.route)) %>%
  dplyr::summarise(n.labels = n_distinct(label_id, na.rm = TRUE)) %>%
  ungroup() %>%
  inner_join(times.turk.study, by = c('worker1' = 'user_id')) %>%
  mutate(distance.feet = if_else(is.anon.route, 2000, 4000),  # intermediate value
         distance.meters = distance.feet / 3.28084,           # intermediate value
         labels.per.100m = 100 * n.labels / distance.meters,
         meters.per.min = distance.meters / minutes_audited,
         user.id = worker1) %>%
  inner_join(user.search.times %>%
               dplyr::filter(label.type %in% c('All', 'Problem')) %>%
               droplevels(),
             by = c('user.id', 'label.type')) %>%
  gather(key = accuracy.type, value = accuracy.value, recall, precision, factor_key = TRUE) %>%
  mutate(user.id = factor(worker1),
         route.id = factor(condition.id))

# Scale the variables used in the tests.
behavior.test.data <-
  behavior.test.data.unscaled %>%
  mutate(viz.search.time = scale(seconds.to.label),
         label.freq = scale(labels.per.100m),
         audit.speed = scale(meters.per.min))

# Subset the data for each individual model
behavior.test.data.recall.prob <-
  behavior.test.data %>%
  dplyr::filter(label.type == 'Problem',
                accuracy.type == 'recall') %>%
  rename(recall = accuracy.value) %>%
  dplyr::select(condition.id, user.id, viz.search.time:audit.speed, true.pos, false.neg, recall) %>%
  na.omit() %>%
  droplevels()
behavior.test.data.recall.all <-
  behavior.test.data %>%
  dplyr::filter(label.type == 'All',
                accuracy.type == 'recall') %>%
  rename(recall = accuracy.value) %>%
  dplyr::select(condition.id, user.id, viz.search.time:audit.speed, true.pos, false.neg, recall) %>%
  na.omit() %>%
  droplevels()
behavior.test.data.prec.prob <-
  behavior.test.data %>%
  dplyr::filter(label.type == 'Problem',
                accuracy.type == 'precision') %>%
  rename(precision = accuracy.value) %>%
  dplyr::select(condition.id, user.id, viz.search.time:audit.speed, true.pos, false.pos, precision) %>%
  na.omit() %>%
  droplevels()
behavior.test.data.prec.all <-
  behavior.test.data %>%
  dplyr::filter(label.type == 'All',
                accuracy.type == 'precision') %>%
  rename(precision = accuracy.value) %>%
  dplyr::select(condition.id, user.id, viz.search.time:audit.speed, true.pos, false.pos, precision) %>%
  na.omit() %>%
  droplevels()
```

First, let's take a look at the relationships between the variables.

```{r turk.user.behavior.graphs, echo=FALSE, fig.width=4.5, fig.height=4.5}
# Plot relationships between variables.
ggplot(behavior.test.data.unscaled, aes(x = labels.per.100m, y = accuracy.value)) +
  geom_point(na.rm = TRUE) +
  stat_smooth(method = 'loess', na.rm = TRUE) +
  theme_bw() +
  coord_cartesian(ylim = c(0, 1)) +
  xlab('Labeling Frequency (# labels / 100 meters)') +
  facet_grid(accuracy.type ~ label.type)
ggplot(behavior.test.data.unscaled, aes(x = meters.per.min, y = accuracy.value)) +
  geom_point(na.rm = TRUE) +
  stat_smooth(method = 'loess', na.rm = TRUE) +
  theme_bw() +
  coord_cartesian(ylim = c(0, 1)) +
  xlab('Auditing Speed (meters / minute)') +
  facet_grid(accuracy.type ~ label.type)
ggplot(behavior.test.data.unscaled, aes(x = seconds.to.label, y = accuracy.value)) +
  geom_point(na.rm = TRUE) +
  stat_smooth(method = 'loess', na.rm = TRUE) +
  theme_bw() +
  coord_cartesian(ylim = c(0, 1)) +
  xlab('Visual Search Time (seconds to label)') +
  facet_grid(accuracy.type ~ label.type)
```

From these graphs, the potential associations I was seeing (before running the tests) were a possible positive association between labeling frequency and recall (both Problem and All types) and a possible negative association between auditing speed and recall (both Problem and All types). In fact, three of those four are cases where we find statistically significant results.

To test for the associations between the user behaviors and accuracy, we created 4 binomial mixed effect models (one for accuracy type, precision and recall; and label type, All and Problem). We had the 3 user behaviors as individual fixed effects (labeling frequency, audit speed, and visual search time), which we scaled and centered so that estimates and standard errors between the predictors is easier. We used user id nested in route id as the random effects. We modeled recall and precision as binomial and used the standard logistic link function. We performed likelihood ratio tests (LRTs) to determine the significance of the predictors. You can check out some notes on mixed effects models at the end of this document: [here](#rationale-and-description-of-mixed-effect-models).

Below is a table showing the summaries of the models and results of the LRTs. The estimate and standard error columns come from the models (along with the association column, which denotes direction of relationship), and the p value and LRT stat come from the likelihood ratio tests.

```{r turk.user.behavior.tests, echo=FALSE}
# Create the different models, along with likelihood ratio tests and data frames with relevant
# stats extracted from the model and LRT.
# Found the following link helpful in looking for solutions to convergence errors:
# https://rstudio-pubs-static.s3.amazonaws.com/33653_57fc7b8e5d484c909b615d8633c01d51.html
behavior.recall.prob.model <-
  glmer(recall ~ label.freq + audit.speed + viz.search.time + (1 | condition.id / user.id),
        data = behavior.test.data.recall.prob,
        family = binomial(link = 'logit'),
        weights = true.pos + false.neg,
        control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 100000)))
lrt.behavior.recall.prob <- drop1(behavior.recall.prob.model, test = 'Chisq')
coeff.recall.prob <-
  data.frame(base::summary(behavior.recall.prob.model)$coefficients) %>%
  mutate(param = dimnames(base::summary(behavior.recall.prob.model)$coefficients)[[1]],
         label.type = 'Problem',
         accuracy.type = 'recall',
         LRT = lrt.behavior.recall.prob$LRT,
         df = lrt.behavior.recall.prob$Df,
         n = nobs(behavior.recall.prob.model),
         p = lrt.behavior.recall.prob$`Pr(Chi)`)

behavior.recall.all.model <-
  glmer(recall ~ label.freq + audit.speed + viz.search.time + (1 | condition.id / user.id),
        data = behavior.test.data.recall.all,
        family = binomial(link = 'logit'),
        weights = true.pos + false.neg,
        control = glmerControl(optimizer = "Nelder_Mead", optCtrl = list(maxfun = 100000)))
lrt.behavior.recall.all <- drop1(behavior.recall.all.model, test = 'Chisq')
coeff.recall.all <-
  data.frame(base::summary(behavior.recall.all.model)$coefficients) %>%
  mutate(param = dimnames(base::summary(behavior.recall.all.model)$coefficients)[[1]],
         label.type = 'All',
         accuracy.type = 'recall',
         LRT = lrt.behavior.recall.all$LRT,
         df = lrt.behavior.recall.all$Df,
         n = nobs(behavior.recall.all.model),
         p = lrt.behavior.recall.all$`Pr(Chi)`)

behavior.prec.prob.model <-
  glmer(precision ~  label.freq +  audit.speed + viz.search.time + (1 | condition.id / user.id),
        data = behavior.test.data.prec.prob,
        family = binomial(link = 'logit'),
        weights = true.pos + false.pos,
        control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 100000)))
lrt.behavior.prec.prob <- drop1(behavior.prec.prob.model, test = 'Chisq')
coeff.precision.prob <-
  data.frame(base::summary(behavior.prec.prob.model)$coefficients) %>%
  mutate(param = dimnames(base::summary(behavior.prec.prob.model)$coefficients)[[1]],
         label.type = 'Problem',
         accuracy.type = 'precision',
         LRT = lrt.behavior.prec.prob$LRT,
         df = lrt.behavior.prec.prob$Df,
         n = nobs(behavior.prec.prob.model),
         p = lrt.behavior.prec.prob$`Pr(Chi)`)

behavior.prec.all.model <-
  glmer(precision ~  label.freq +  audit.speed + viz.search.time + (1 | condition.id / user.id),
        data = behavior.test.data.prec.all,
        family = binomial(link = 'logit'),
        weights = true.pos + false.pos,
        control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 100000)))
lrt.behavior.prec.all <- drop1(behavior.prec.all.model, test = 'Chisq')
coeff.precision.all <-
  data.frame(base::summary(behavior.prec.all.model)$coefficients) %>%
  mutate(param = dimnames(base::summary(behavior.prec.all.model)$coefficients)[[1]],
         label.type = 'All',
         accuracy.type = 'precision',
         LRT = lrt.behavior.prec.all$LRT,
         df = lrt.behavior.prec.all$Df,
         n = nobs(behavior.prec.all.model),
         p = lrt.behavior.prec.all$`Pr(Chi)`)

# Create nice looking table with important stats from each of the models/tests above.
all.coeffs <-
  rbind(coeff.recall.all, coeff.recall.prob, coeff.precision.all, coeff.precision.prob) %>%
  dplyr::filter(param != '(Intercept)') %>%
  mutate(association = case_when(Estimate > 0 & p < 0.05 ~ '+',
                                 Estimate < 0 & p < 0.05 ~ '-',
                                 TRUE                    ~ 'NA'),
         p.value = p.num.to.p.str.with.sig.code(p)) %>%
  rename(estimate = Estimate, std.err = Std..Error) %>%
  dplyr::select(accuracy.type, label.type, param, association, estimate, std.err, p.value, LRT, df, n)

kable(all.coeffs, digits = 3, align = 'l')
```

The positive association between labeling frequency and recall is expected, as someone who placed more labels probably correctly found more attributes. Similarly the negative association between labeling frequency and All type precision is expected (more labels means more incorrect labels).

The negative association between auditing speed and All type recall is also expected, as auditing more quickly will probably end up with fewer correct attributes being found. Similarly, the negative association b/w auditing speed and Problem type precision might make sense (auditing more quickly, less likely to over-label).

The associations that we do not have immediate explanations for are the negative association b/w visual search time and All type recall and the negative association b/w visual search time and Problem type precision.


### User group: Reg vs anon vs turk1 vs turk3 vs turk5

TODO: Make some graphs.

*Takeaways*:


```{r turk.user.group.analysis, echo=FALSE, fig.width=8, fig.height=6}
```


### Low severity: Removing low severity effect on recall

NOTE: I did this analysis using both >=3 and >=4, and both produced significant results. The difference between low and high severity is larger for >=3 compared to >=4, and thus the p-value is smaller. However, we could use >=4 if the story is more compelling.

NOTE: In this section, the data are binary (not ordinal), we are only considering single users auditing (i.e., no multi-user clustering or majority vote), and we only consider the first turker to audit each route.

```{r turk.high.severity.setup.1, echo=FALSE}
low.severity.check.data.1 <-
  data.with.raw.accuracy %>%
  dplyr::filter(included.severity %in% c('all', '<=3', '>=4'),
                label.type == 'Problem',
                worker.type %in% c('reg', 'anon', 'turk1')) %>%
  droplevels %>%
  mutate(included.severity = factor(included.severity,
                                    labels = c('all', '<=3', '>=4'),
                                    levels = c('all', '<=3', '>=4')))

recall.by.severity.1 <-
  low.severity.check.data.1 %>%
  dplyr::filter(binary == TRUE) %>%
  group_by(included.severity) %>%
  summarize(n.users = sum(!is.na(recall[granularity == 'street'])),
            mn.rec.st = mean(recall[granularity == 'street'], na.rm = TRUE),
            md.rec.st = median(recall[granularity == 'street'], na.rm = TRUE),
            sd.rec.st = sd(recall[granularity == 'street'], na.rm = TRUE),
            mn.rec.5m = mean(recall[granularity == '5_meter'], na.rm = TRUE),
            md.rec.5m = median(recall[granularity == '5_meter'], na.rm = TRUE),
            sd.rec.5m = sd(recall[granularity == '5_meter'], na.rm = TRUE))

gt.counts.by.severity.1 <-
  low.severity.check.data.1 %>%
  dplyr::filter(worker.type == 'turk1',
                binary == FALSE,
                granularity == 'street') %>%
  group_by(included.severity) %>%
  summarize(gt.problem.labels = sum(true.pos, false.neg))

severity.test.data.1 <-
  low.severity.check.data.1 %>%
  dplyr::filter(binary == TRUE, included.severity != 'all') %>%
  mutate(user.id = factor(worker1),
         condition.id = factor(condition.id))

severity.test.data.1.5m <-
  severity.test.data.1 %>%
  dplyr::filter(granularity == '5_meter') %>%
  dplyr::select(user.id, condition.id, included.severity, true.pos, false.neg, recall) %>%
  na.omit() %>%
  droplevels()
severity.test.data.1.street <-
  severity.test.data.1 %>%
  dplyr::filter(granularity == 'street') %>%
  dplyr::select(user.id, condition.id, included.severity, true.pos, false.neg, recall) %>%
  na.omit() %>%
  droplevels()

severity.model.1.5m <- glmer(recall ~ included.severity + (1 | condition.id / user.id),
                             data = severity.test.data.1.5m,
                             family = binomial(link = 'logit'),
                             weights = true.pos + false.neg,
                             control = glmerControl(optimizer = "bobyqa",
                                                    optCtrl = list(maxfun = 100000)))
severity.model.1.street <- glmer(recall ~ included.severity + (1 | condition.id / user.id),
                                data = severity.test.data.1.street,
                                family = binomial(link = 'logit'),
                                weights = true.pos + false.neg,
                                control = glmerControl(optimizer = "bobyqa",
                                                       optCtrl = list(maxfun = 100000)))

lrt.severity.1.5m <- drop1(severity.model.1.5m, test = 'Chisq')
lrt.severity.1.street <- drop1(severity.model.1.street, test = 'Chisq')
```

Below is a table showing the average recall across all users for labels that had severity <=3 (in the ground truth) and labels that had severity >=4, along with the number of labels that fall into each of those categories.

We also created a binomial mixed effects model to determine the relationship between severity and recall. We had severity (high or low) as the fixed effect and user id nested in route id as random effects. We modeled recall as binomial and used a logistic link function. Using a likelihood ratio test (LRT), we found the contribution of the fixed effect (severity) to be statistically significant (street-level: `r print.lrt.results(lrt.severity.1.street, severity.model.1.street, 'included.severity')`; 5 meter-level: `r print.lrt.results(lrt.severity.1.5m, severity.model.1.5m, 'included.severity')`). You can check out some notes on mixed effects models at the end of this document: [here](#rationale-and-description-of-mixed-effect-models).

```{r turk.high.severity.analysis.1, echo=FALSE, fig.width=8, fig.height=6}
kable(
  inner_join(gt.counts.by.severity.1, recall.by.severity.1, by = 'included.severity') %>%
    rename(sev = included.severity, gt.prob.labs = gt.problem.labels),
  format = 'markdown',
  digits = 3,
  align = 'l'
  )
```


```{r turk.high.severity.setup.2, echo=FALSE}
low.severity.check.data.2 <-
  data.with.raw.accuracy %>%
  dplyr::filter(included.severity %in% c('all', '<=2', '>=3'),
                label.type == 'Problem',
                worker.type %in% c('reg', 'anon', 'turk1')) %>%
  droplevels %>%
  mutate(included.severity = factor(included.severity,
                                    labels = c('all', '<=2', '>=3'),
                                    levels = c('all', '<=2', '>=3')))

recall.by.severity.2 <-
  low.severity.check.data.2 %>%
  dplyr::filter(binary == TRUE) %>%
  group_by(included.severity) %>%
  summarize(n.users = sum(!is.na(recall[granularity == 'street'])),
            mn.rec.st = mean(recall[granularity == 'street'], na.rm = TRUE),
            md.rec.st = median(recall[granularity == 'street'], na.rm = TRUE),
            sd.rec.st = sd(recall[granularity == 'street'], na.rm = TRUE),
            mn.rec.5m = mean(recall[granularity == '5_meter'], na.rm = TRUE),
            md.rec.5m = median(recall[granularity == '5_meter'], na.rm = TRUE),
            sd.rec.5m = sd(recall[granularity == '5_meter'], na.rm = TRUE))

gt.counts.by.severity.2 <-
  low.severity.check.data.2 %>%
  dplyr::filter(worker.type == 'turk1',
                binary == FALSE,
                granularity == 'street') %>%
  group_by(included.severity) %>%
  summarize(gt.problem.labels = sum(true.pos, false.neg))

severity.test.data.2 <-
  low.severity.check.data.2 %>%
  dplyr::filter(binary == TRUE, included.severity != 'all') %>%
  mutate(user.id = factor(worker1),
         condition.id = factor(condition.id))

severity.test.data.2.5m <-
  severity.test.data.2 %>%
  dplyr::filter(granularity == '5_meter') %>%
  dplyr::select(user.id, condition.id, included.severity, true.pos, false.neg, recall) %>%
  na.omit() %>%
  droplevels()
severity.test.data.2.street <-
  severity.test.data.2 %>%
  dplyr::filter(granularity == 'street') %>%
  dplyr::select(user.id, condition.id, included.severity, true.pos, false.neg, recall) %>%
  na.omit() %>%
  droplevels()

severity.model.2.5m <- glmer(recall ~ included.severity + (1 | condition.id / user.id),
                                    data = severity.test.data.2.5m,
                                    family = binomial(link = 'logit'),
                                    weights = true.pos + false.neg,
                                    control = glmerControl(optimizer = "bobyqa",
                                                           optCtrl = list(maxfun = 100000)))
severity.model.2.street <- glmer(recall ~ included.severity + (1 | condition.id / user.id),
                                        data = severity.test.data.2.street,
                                        family = binomial(link = 'logit'),
                                        weights = true.pos + false.neg,
                                        control = glmerControl(optimizer = "bobyqa",
                                                               optCtrl = list(maxfun = 100000)))

lrt.severity.2.5m <- drop1(severity.model.2.5m, test = 'Chisq')
lrt.severity.2.street <- drop1(severity.model.2.street, test = 'Chisq')

# Possible diagnostic plots...
# qqnorm(residuals(severity.model))
# ggplot(aes(sample = residuals.severity.model.),
#        data = data.frame(residuals(severity.model))) +
#   geom_qq() +
#   geom_qq_line()
```

Below is a table showing the average recall across all users for labels that had severity <=2 (in the ground truth) and labels that had severity >=3, along with the number of labels that fall into each of those categories.

We also created binomial mixed effects models to determine the relationship between severity and recall. We had severity (high or low) as the fixed effect and user id nested in route id as random effects. We modeled recall as binomial and used a logistic link function. Using a likelihood ratio test (LRT), we found the contribution of the fixed effect (severity) to be statistically significant (street-level: `r print.lrt.results(lrt.severity.2.street, severity.model.2.street, 'included.severity')`; 5 meter-level: `r print.lrt.results(lrt.severity.2.5m, severity.model.2.5m, 'included.severity')`). You can check out some notes on mixed effects models at the end of this document: [here](#rationale-and-description-of-mixed-effect-models).

```{r turk.high.severity.analysis.2, echo=FALSE, fig.width=8, fig.height=6}
kable(
  inner_join(gt.counts.by.severity.2, recall.by.severity.2, by = 'included.severity') %>%
    rename(sev = included.severity, gt.prob.labs = gt.problem.labels),
  format = 'markdown',
  digits = 3,
  align = 'l'
  )
```


### Binary vs ordinal issues per segment

For simplicity, the first graph looks at the 5 meter level, and the second looks at street level. All user groups are also combined (the groups being: registered volunteers, anonymous volunteers, and individual turkers).

NOTE: In this section, the data is at the street level (not 5 meter level), we are only considering single users auditing (i.e., no multi-user clustering or majority vote), and we only consider the first turker to audit each route.

NOTE: The red dots on the graphs are means.

*Takeaways*:

* 5 meter level (first graph): Considering multiple issues per segment results in _very slightly_ lower accuracy for pretty much every type of label and type of accuracy (except precision). I suspect that this comes mostly from our method of clustering, which makes it unlikely that users end up with multiple labels per 5 meter segment. We do not have this restriction in the ground truth, so those few cases where we have more than one label per 5 meter segment in the GT usually results in an additional false negative when moving to ordinal analysis. However, the difference here is very small, so our clustering method seems fine to me.

* Street level (second graph) recall: If we do this analysis at the street level, the decreases in accuracy are more pronounced. At this level, the clustering shouldn't have much effect. The decrease in recall suggests that users are finding _some_ of the problems, but not _all_ of them (meaning an increase in false negatives when we move to ordinal analysis).

* Street level (second graph) recall: I suspect that the reason for the decrease in precision when moving to ordinal analysis at the street level is the same reason as why 5 meter level has lower precision than street level (seen in the previous section). That is, users' misunderstandings of how to label certain common things (driveways as curb ramps, etc.); since these mistakes are common, they may happen many times on a single street edge, which means that you start racking up the false positives when you move to ordinal analysis.


```{r turk.issues.per.seg.analysis, echo=FALSE, fig.width=8, fig.height=4}
issues.per.seg.analysis.data <-
  data.with.raw.accuracy %>%
  dplyr::filter(included.severity == 'all',
                n.workers == 1,
                label.type %in% LABEL_TYPES_TO_ANALYZE) %>%
  mutate(issues.per.segment = factor(if_else(binary, 'binary', 'ordinal'))) %>%
  gather(accuracy.type, accuracy.value, recall, precision, na.rm = TRUE, factor_key = TRUE)

# Make the set of labels that give the number of observations for each sub-graph.
issues.per.seg.analysis.labels <-
  issues.per.seg.analysis.data %>%
  group_by(label.type, accuracy.type, granularity, issues.per.segment) %>%
  dplyr::summarize(n = sum(!is.na(accuracy.value))) %>%
  dplyr::summarize(n_label = paste0('n = (',
                                    first(n[issues.per.segment == 'binary']), ',',
                                    first(n[issues.per.segment == 'ordinal']),
                                    ')')) %>%
  ungroup()

# Create the trellis boxplots.
ggplot(data = issues.per.seg.analysis.data %>% dplyr::filter(granularity == '5_meter'),
       aes(x = issues.per.segment, y = accuracy.value)) +
  geom_boxplot() +
  stat_summary(fun.y = mean, geom = 'point', color = 'red', size = 1) +
  scale_y_continuous(breaks = ACCURACY_BREAKS, limits = c(0, 1.15)) +
  geom_hline(aes(yintercept = 1.0), linetype = 'dotted') +
  geom_text(data = issues.per.seg.analysis.labels %>% dplyr::filter(granularity == '5_meter'),
             mapping = aes(x = -Inf, y = 1.13, hjust = 'inward', label = n_label), size = 3.5) +
  facet_grid(accuracy.type ~ label.type) +
  ggtitle('5 Meter-Level Analysis of Issues per Segment') +
  my.theme.discrete.x

ggplot(data = issues.per.seg.analysis.data %>% dplyr::filter(granularity == 'street'),
       aes(x = issues.per.segment, y = accuracy.value)) +
  geom_boxplot() +
  stat_summary(fun.y = mean, geom = 'point', color = 'red', size = 1) +
  scale_y_continuous(breaks = ACCURACY_BREAKS, limits = c(0, 1.15)) +
  geom_hline(aes(yintercept = 1.0), linetype = 'dotted') +
  geom_text(data = issues.per.seg.analysis.labels %>% dplyr::filter(granularity == 'street'),
             mapping = aes(x = -Inf, y = 1.13, hjust = 'inward', label = n_label), size = 3.5) +
  facet_grid(accuracy.type ~ label.type) +
  ggtitle('Street-Level Analysis of Issues per Segment') +
  my.theme.discrete.x
```

## Rationale and description of mixed effect models

For some background on linear mixed-effect models, the reference I found most helpful was this one: https://stats.idre.ucla.edu/other/mult-pkg/introduction-to-linear-mixed-models/.

#### Why we are using them

A common question we are asking throughout this document is if some categorical variable (label type, user group, etc) has an effect on accuracy (or some other numeric variable, like visual search time), and if so, which categories have higher/lower accuracies than the others. Note that these are two different questions: is there a statistically significant effect overall, and what parts of the ordering of the categories is statistically significant.

The default analysis we would do in this situation is to do an ANOVA to test the overall significance, followed by Tukey's HSD post-hoc analysis to see if ordering is significant. Although we do fail most of the assumptions of these tests, that is not the main reason that we look to mixed effect models. We fail in different ways in different situations, so describing the failing of the assumptions is something that I will omit here.

The main reason that we want to use mixed effect models is that it allows us to take into account differences between users, the differences between routes, and the fact that the individual only audited a single route. Which route we are looking at has a significant influence on the accuracy of the users who audited that route, but the route ID is not a variable that is interesting to look at in relation to accuracy (the _type_ of route is interesting to look at, but not the route ID). As such, we need a way to account for the route ID in our analyses.

A similar problem exists for individual users. When we are looking at how accuracy is different for different label types, each user has an accuracy number for each label type. But users have different "baseline" accuracy levels, similar to how routes have "baseline" accuracy levels. We would like to account for those baseline levels in our analyses, and only look at how label type, for example, causes a *change from the baseline*.

#### Example text

We created a \<linear|binomial|poisson - depending on outcome variable\> mixed effects models to determine the relationship between \<independent variable\> and \<outcome variable\>. We had \<independent variable\> as the fixed effect and user id nested in route id as random effects. We modeled \<outcome variable\> as \<linear|binomial|poisson\> and used a logistic link function.

Using a likelihood ratio test (LRT), we found the contribution of the fixed effect (\<independent variable\>) to have a statistically significant association with \<outcome variable\> (likelihood ratio = A, df = B, n = C, p < 0.001).

To test that the ordering of the \<categorical independent variable\> is statistically significant (e.g., that NoCurbRamp recall is significantly lower than CurbRamp recall, etc), we do post-hoc Tukey's HSD tests. This essentially gives us a pairwise test between each \<categorical independent variable\>, which lets us determine what parts of the ordering are significant. The results of which are shown in a table below.

#### Assumptions

We won't need to go into the assumptions in the paper (people don't generally go into the assumptions for ANOVA, and we are already doing something better than that). But for those who are wondering, here is some text on the assumptions.

Our two assumptions are that the residuals of the fit are normally distributed (normality) and have constant variance across the range of fitted values (heteroscedasticity). To check for normality, we first use the Shapiro-Wilk test. In this test, the null hypothesis is that the residuals are normally distributed; if we fail to reject the null, then we meet our assumption that the residuals are normally distributed. This test has a high type 1 error rate (often says that data are not normally distributed when they really are), but it is good to check the test because it is a quick and easy way to say the data are normal if the test succeeds. If we fail the test, we check a histogram of the residuals to see if they are normally distributed. For heteroscedasticity, we make a scatter plot with the standardized residuals on the y-axis, and fitted visual search times on the x-axis. If the variance in the residuals (y-axis) is constant across the fitted values (x-axis), this constitutes "constant variance", and so we would meet the heteroscedasticity assumption.

Often we can deal with not meeting these assumptions by transforming our data. In fact, I think in at one of our analyses we do a log-transform which helps with failing to meet the heteroscedasticity assumption.
